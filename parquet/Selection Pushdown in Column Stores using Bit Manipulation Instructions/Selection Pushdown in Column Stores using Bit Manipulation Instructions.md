# Selection Pushdown in Column Stores using Bit Manipulation Instructions

Modern analytical database systems predominantly rely on column-oriented storage, which offers superior compression efficiency due to the nature of the columnar layout. This compression, however, creates challenges in decoding speed during query processing. Previous research has explored predicate pushdown on encoded values to avoid decoding, but these techniques are restricted to specific encoding schemes and predicates, limiting their practical use. In this paper, we propose a generic predicate pushdown approach that supports arbitrary predicates by leveraging selection pushdown to reduce decoding costs. At the core of our approach is a fast select operator capable of directly extracting selected encoded values without decoding, by using *Bit Manipulation Instructions*, an instruction set extension to the X86 architecture. We empirically evaluate the proposed techniques in the context of Apache Parquet using both micro-benchmarks and the TPC-H benchmark, and show that our techniques improve the query performance of Parquet by up to one order of magnitude with representative scan queries. Further experimentation using Apache Spark demonstrates speed improvements of up to 5.5X even for end-to-end queries involving complex joins.

## 1 INTRODUCTION

Column-oriented data storage increasingly dominates the analytical database system landscape in this Big Data era. Unlike traditional row-oriented storage formats where tables are stored in a row-by-row manner, such formats employ a columnar storage layout in which values in the same column are stored contiguously. One example of such approach is Apache Parquet [[4](#_bookmark49)], an open-source column-oriented storage format. With the growing trend of moving analytical databases to the cloud, we are seeing increased consensus in the industry with respect to using Parquet as a standard format across major cloud vendors and analytical platforms [[1](#_bookmark46), [10](#_bookmark55), [16](#_bookmark61), [18](#_bookmark63), [38](#_bookmark83), [45](#_bookmark90)].

Since consecutive values in the columnar layout are similar to each other, column stores use a variety of aggressive encoding schemes to compress column values [[14](#_bookmark59)]. The most widely-used encoding scheme is *dictionary encoding*, where each distinct value in a column is mapped to a unique small code according to a dictionary built for the column. These small codes of column values are usually stored in a *bit-packed* manner, using as many bits as needed to represent each code. This encoding scheme, though effective in reducing storage cost, imposes challenges on query processing as these codes must be **decoded** before they can be further processed.

> - [ ] Fig. 1. Time breakdown of TPC-H Q6

Previous research [[41](#_bookmark86)] has focused on accelerating the decoding process by unpacking a group of bit-packed codes in parallel via SIMD vectorization. This approach, though obviously better than a naÃ¯ve implementation, is still not fast enough due to a *fundamental* limitation: the produced decoded values (e.g., 64-bit integers) are much larger than the input encoded values (typically just a few bits), limiting the degree of data parallelism that can be achieved in this operation. To demonstrate this observation, Figure [1](#_bookmark0) shows the time breakdown of TPC-H Q6 in Parquet (ignoring Parquet-Select for now), which adopts this method for decoding. We see that decoding dominates the overall query time even with this state-of-the-art decoding technique.

In an effort to alleviate this challenge, a line of research attempts to avoid decoding by leveraging the idea of **predicate pushdown** [[23](#_bookmark68), [25](#_bookmark70), [26](#_bookmark71), [29](#_bookmark74), [40](#_bookmark85)]. The basic idea of this rich line of work is to evaluate a converted predicate on the encoded values directly, essentially pushing down predicate evaluation to avoid the costly decoding. In spite of the high performance they achieve, these techniques rely on two key assumptions: 1) the encoding is order-preserving; 2) the predicates are simple enough (e.g., basic comparison predicates) such that they can be converted to equivalent ones in the encoded domain. Unfortunately, neither of these two assumptions may hold in practice, significantly limiting their applicability. For example, although Parquet makes extensive use of dictionary encoding, the dictionary encoding employed is not order-preserving, which eliminates the possibility of adopting these techniques in Parquet. Moreover, even with an order-preserving encoding, many complex predicates, such as string matching, user-defined functions, and cross-table predicates (e.g., [[32]](#_bookmark78)), cannot be supported.

In this paper, we propose a different predicate pushdown approach for column stores. Our broad approach supports *arbitrary* predicates even without an order-preserving encoding. The approach is based on a simple observation: a query on a table typically involves multiple predicates across multiple columns; when evaluating a predicate on a column, records failing to meet prior predicates can be bypassed directly. In column stores, this short-circuit optimization can be implemented using a **select operator**, which selects all values in a given column that satisfy all previous predicates. <u>The key insight of this paper is that we can push down the select operator to directly select encoded values, rather than decoded values, ensuring that only the selected values need to be decoded for full predicate evaluation</u>. We call this technique **selection pushdown**. With this approach, all relevant values are still decoded first and then evaluated against the original predicates, making this approach applicable for arbitrary predicates.

Despite the seemingly simple idea, a key technical challenge lies in designing a fast select operator capable of operating directly on encoded values, moving all selected values from a vector of encoded values packed in a processor word simultaneously. It turns out that this goal is unattainable, if not impossible, without using special hardware capabilities [[27](#_bookmark72)]. A key finding in this paper is the recognition that the **Bit Manipulation Instructions** (BMI), which were introduced by Intel in 2013 and are now available in nearly all Intel and AMD processors, are well-suited to address this challenge. To the best of our knowledge, this is the first paper that leverages BMI for database applications. It is important to note that applying BMI to this application is *nontrivial*, which likely explains why BMI has been largely ignored in database systems over the past decade.

Additionally, we find that the role of BMI goes well beyond the select operator. In fact, BMI plays a crucial role in *all* the key techniques proposed in this paper, as summarized below.

- **Select operator** (Section [3](#_bookmark6)). A select operator copies all selected values, indicated by a select bitmap, from a vector of bit-packed values to contiguous bits in the output. Our BMI-based implementation can process all encoded values packed in a 64-bit processor word using a total of only four instructions, regardless of how many encoded values are packed in a word (e.g., 64 1-bit values, 32 2-bit values, or even 21$\frac{1}{3}$3-bit values), by fully exploiting the parallelism available inside a processor word. This fast operator lays the foundation for our solution.

- **Selection pushdown framework** (Section [4](#_bookmark15)). To take full advantage of the fast select operator, we develop a selection pushdown framework for evaluating an arbitrary scan query on a table, which typically involves a sequence of filter and project operations on various columns. In this framework, both filter and project operations take as input a select bitmap generated by previous filter operations, and make use of the fast select operator to select encoded values upfront before decoding them. Additionally, each filter operation also needs to refine the select bitmap according to the evaluation of its predicate. To achieve this, specific transformations on the select bitmap are required, because the predicate is evaluated on selected values only and the results must be aligned to the original select bitmap. Interestingly, these transformations, though previously considered to be expensive, can now be performed efficiently using BMI (Section [4.3](#_bookmark18)).

- **Supporting complex structures** (Section [5](#_bookmark23)). Modern columnar storage, such as Parquet, offers support for complex structures, including nested and/or repeated structures. Parquet adopts the columnar representation from Googleâ€™s Dremel [[31](#_bookmark77)], using two small integers per value to encode structure information. Due to the presence of null or repeated values in each column within complex structures, column values of the same record may not be fully aligned across columns. Consequently, in our framework, an intermediate select bitmap generated by a filter operation on one column cannot be directly applied to another column. Instead, these bitmaps require sophisticated transformations based on the structural information represented by the small integers. Another key finding of this paper is the recognition that BMI is also well-suited to evaluate these encoded small values (Section [5.3](#_bookmark27)) and transform these bitmaps accordingly (Section [5.4](#_bookmark30)). Based on this finding, we develop BMI-based techniques for these necessary operators and extend our framework to have full support for complex structures in Parquet.

By combining all these techniques, we built a library called Parquet-Select that enables predicate pushdown in Parquet (Section [6](#_bookmark35)). Importantly, Parquet-Select makes no changes to the Parquet format and can, therefore, read any file conforming to the format specification. Figure [1](#_bookmark0) compares the time breakdown between Parquet and Parquet-Select. It is clear that by selecting encoded values upfront, the decoding cost in Parquet-Select is significantly reduced. In the meantime, the selection cost decreases as well, thanks to our fast selector operator and the selection pushdown framework. Our detailed evaluation (Section [7)](#_bookmark36) using both micro-benchmarks and the TPC-H benchmark shows that Parquet-Select always outperforms Parquet, improving the query speed by up to more than one order of magnitude for individual scan queries. The results with Spark further show a speedup of up to 5.5X, even for end-to-end queries involving complex joins, which opens up new potential for faster database processing for similar critical workloads.

Finally, we note that although we present our techniques in the context of Parquet, these techniques can be adapted to other on-disk or in-memory columnar storage formats, such as Apache ORC [[3],](#_bookmark48) Apache Arrow [[2],](#_bookmark47) or internal formats of analytical database systems.

## 2 BACKGROUND

### 2.1  Bit Manipulation Instruction Set (BMI)

The Bit Manipulation Instruction set (BMI) is an extension to the X86 architecture for Intel and AMD processors. As the name suggests, the goal of BMI is to accelerate common bitwise operations by using dedicated hardware instructions. Unlike SIMD instruction sets (e.g., AVX2 or AVX512), BMI instructions operate on 64-bit general-purpose registers. BMI contains a total of 14 instructions. 

Most bitwise operations supported by BMI have a sufficiently fast software implementation. For example, the BLSI instruction extracts the rightmost 1 from a 64-bit operand *ğ‘¥* , which can be implemented by using two arithmetic instructions: `ğ‘¥ & -ğ‘¥` [[27](#_bookmark72)]. Consequently, even before the advent of BMI, these bitwise operations were used in various scenarios including database applications (e.g., [[28](#_bookmark75), [29](#_bookmark74)]). With the advent of BMI, these software-implemented operations in existing software can be easily replaced by their BMI counterparts through compiler techniques or manual optimizations, without rethinking the algorithm design.

> ä½æ“ä½œæŒ‡ä»¤é›† (BMI) æ˜¯ Intel å’Œ AMD å¤„ç†å™¨ X86 æ¶æ„çš„æ‰©å±•ã€‚é¡¾åæ€ä¹‰ï¼ŒBMI çš„ç›®æ ‡æ˜¯é€šè¿‡ä½¿ç”¨ä¸“ç”¨ç¡¬ä»¶æŒ‡ä»¤æ¥åŠ é€Ÿå¸¸è§çš„ä½æ“ä½œã€‚ä¸ SIMD æŒ‡ä»¤é›†ï¼ˆä¾‹å¦‚ AVX2 æˆ– AVX512ï¼‰ä¸åŒï¼ŒBMI æŒ‡ä»¤æ“ä½œäº 64 ä½é€šç”¨å¯„å­˜å™¨ã€‚BMI æ€»å…±åŒ…å« 14 æ¡æŒ‡ä»¤ã€‚
>
> BMI æ”¯æŒçš„å¤§å¤šæ•°ä½æ“ä½œéƒ½æ‹¥æœ‰è¶³å¤Ÿå¿«çš„è½¯ä»¶å®ç°ã€‚ä¾‹å¦‚ï¼ŒBLSI æŒ‡ä»¤ä» 64 ä½æ“ä½œæ•° x ä¸­æå–æœ€å³è¾¹çš„ 1ï¼Œè¿™å¯ä»¥é€šè¿‡ä¸¤æ¡ç®—æœ¯æŒ‡ä»¤æ¥å®ç°ï¼š`x& âˆ’ x`[27]ã€‚å› æ­¤ï¼Œå³ä½¿åœ¨ BMI å‡ºç°ä¹‹å‰ï¼Œè¿™äº›ä½æ“ä½œå°±å·²ç»åº”ç”¨äºåŒ…æ‹¬æ•°æ®åº“åº”ç”¨ç¨‹åºåœ¨å†…çš„å„ç§åœºæ™¯ï¼ˆä¾‹å¦‚ [28, 29]ï¼‰ã€‚éšç€ BMI çš„å‡ºç°ï¼Œç°æœ‰è½¯ä»¶ä¸­è¿™äº›è½¯ä»¶å®ç°çš„æ“ä½œå¯ä»¥é€šè¿‡ç¼–è¯‘å™¨æŠ€æœ¯æˆ–æ‰‹åŠ¨ä¼˜åŒ–è½»æ¾åœ°è¢« BMI å¯¹åº”æ“ä½œå–ä»£ï¼Œè€Œæ— éœ€é‡æ–°è€ƒè™‘ç®—æ³•è®¾è®¡ã€‚

2.1.1.  **PEXT and PDEP**. Two BMI instructions, namely PEXT and PDEP, do not fall into the above mentioned category. The PEXT (parallel bit extract) instruction extracts the bits selected by a select mask operand from a source operand and copies them to the contiguous low-order bits in the destination, with the high-order bits set to 0s. The PDEP (parallel bit deposit) instruction does the opposite of PEXT: the contiguous low-order bits from the source operand are copied to the selected bits of destination, indicated by the select mask operand, while other bits in the destination are set to 0s. Figure [2](#_bookmark3) shows examples of PEXT and PDEP on 16-bit operands. Notice that we use the little-endian view throughout this paper, which means the first bit, value, or word is the rightmost one in all figures and the last one is the leftmost one.

![](http://darwin-controller-pro.oss-cn-hangzhou.aliyuncs.com/docs/1378848234281619456/%E3%80%90%E5%8E%9F%E6%96%87%E3%80%91Selection%20Pushdown%20in%20Column%20Stores%20using%20Bit%20Manipulation_2.jpg?Expires=1748871504&OSSAccessKeyId=LTAI5tBVMtznbk7xyCa56gof&Signature=RaAyu2anFgjmTSQLSg%2BmRpAtaP8%3D)

> - [ ] Fig. 2. Examples of PEXT and PDEP

Unlike other BMI instructions such as BLSI, it remains an open question on how to efficiently implement PEXT and PDEP without using dedicated hardware instructions. A naÃ¯ve implementation, which iterates over each bit and moves selected bits one at a time, is extremely inefficient. To demonstrate this, Table [1](#_bookmark4) compares the software and BMI implementations of BLSI, PEXT, and PDEP on both Intel and AMD processors. Unsurprisingly, BMI is *two orders of magnitude faster* than our software implementation on both Intel and AMD processors. By contrast, the software implementation of BLSI runs at significantly higher throughput and is even surprisingly faster than its BMI counterpart.

<a id="_bookmark4"></a>

| Throughput (ops/s) | Intel Xeon Gold 6140 |          |          |          |         | AMD EPYC 7413 |
| ------------------ | -------------------- | -------- | -------- | -------- | ------- | ------------- |
|                    | BLSI                 | PEXT     | PDEP     | BLSI     | PEXT    | PDEP          |
| Software           | 3100M                | 8.1M     | 8.7M     | 6214M    | 18.3M   | 18.5M         |
| BMI                | 1381M                | 1150M    | 1143M    | 1243M    | 1713M   | 1651M         |
| Speedup            | **0.46X**            | **142X** | **131X** | **0.2X** | **94X** | **89X**       |

Table 1. BMI vs. software implementation

Due to the prohibitively low performance of the software implementation of PEXT and PDEP, existing algorithms or systems tend to avoid these expensive bitwise operations in performancecritical tasks, making it rare to find opportunities to apply PEXT/PDEP directly in existing programs. Consequently, the advent of BMI requires us to fundamentally rethink the algorithm/system design with these powerful instructions. In this paper, we explore this opportunity for predicate pushdown in column stores.

PEXT/PDEP support has been widely available in server processors for years. Intel first introduced PEXT and PDEP with the Haswell processors in 2013. AMD added support for these instructions through microcode starting with the Zen microarchitecture in 2017, followed by a full hardware implementation in the Zen 3 microarchitecture in 2020. However, ARM-based server processors, such as AWS Graviton [[5](#_bookmark50)] and Nvidia Grace [[11](#_bookmark56)], have yet to offer PEXT/PDEP support. One of the objectives of this paper is to motivate ARM engineers to assess the possibility of adding similar instructions in upcoming ARM-based processors.

### 2.2 Apache Parquet <a id="_bookmark5"></a>

Apache Parquet [[4](#_bookmark49)] is an open-source columnar storage format, developed based on the storage techniques from Googleâ€™s Dremel [[31](#_bookmark77)]. It has arguably become the de-facto columnar storage format for data analytics in the Big Data ecosystem. We briefly describe the data model, columnar encoding, and storage format of Parquet.

**Data Model and Schema.** Parquet inherits the data model from Protocol Buffers [[12](#_bookmark57)], which is based on strongly-typed nested structure. In Parquet, each record consists of one or more fields,

each of which can be an *atomic* field or a *group* field. Group fields contain nested fields, which can recursively be either atomic or group fields. Each field is defined with two types: *data type*, which specifies the primitive data type such as int32 or byte array, and *repetition type*, which defines the number of occurrences of the field in its parent group field and can be labeled as one of the three types: *required* (1 time), *optional* (0 or 1 time), and *repeated* (*\>*1 times).

**Repetition and Definition Levels.** To represent complex structure in a columnar representation, Parquet stores two additional integer numbers, called *repetition level* and *definition level*, to encode

this structural information. We refer the readers to the original Dremel paper [[31](#_bookmark77)] for the definitions and the algorithm to reconstruct the original tree-structured records. For the purpose of this paper, repetition and definition levels are used to find: 1) null field values; and 2) the number of repeated values for each record.

**Encodings.** Field values, repetition and definition levels are compressed independently using common encoding schemes [[14](#_bookmark59)]. In particular, Parquet extensively uses a hybrid encoding scheme

that adaptively switches between run-length encoding (RLE) and bit-packing encoding: a long run of the same value is stored as a RLE run; other values are encoded in bit-packing runs. Thus, an encoded column typically contains interleaved RLE and bit-packed runs. Repetition and definition levels are directly encoded using this hybrid encoding. Field values, regardless of data types, are first mapped to codes using dictionary encoding, which are then encoded using this hybrid scheme. If the size of the dictionary reaches a certain threshold, Parquet falls back to use the plain encoding. The dictionary used in Parquet is not order-preserving, meaning that most predicates cannot be evaluated on dictionary codes directly.

**Storage Format.** In Parquet, data is first partitioned into blocks in row-major order, called

*row-groups*. Within each row-group, data is stored contiguously in column-major order, i.e., similar to the PAX layout [[15](#_bookmark60)]. Each root-to-leaf field path in the schema corresponds to a *column* in a row group, which includes three components: field values, repetition levels, and definition levels. The three components are stored independently in separate data pages. Unnecessary information is never physically stored in Parquet: null values are omitted from the field values; definition levels are not physically stored if the field is a required field; similarly, repetition levels are omitted for non-repeated fields.

<a id="_bookmark6"></a>

## 3 BIT-PARALLEL SELECT OPERATOR

We begin by describing our fast select operator, which lays the foundation for efficient selection pushdown in column stores.

### 3.1 Problem Statement

A **select operator** takes as input a byte array consisting of *ğ‘› ğ‘˜*-bit values and an *ğ‘›*-bit select bitmap. It extracts all selected values where their corresponding bits in the select bitmap are 1s, and copies them into the contiguous bits in an output byte array, just as if the bits of all unselected values had been removed from the input.

Figure [3](#_bookmark9) shows the input and the expected output when selecting 3 out of 8 example 4-bit values (ignoring the computation steps for now). As the 3rd, 7th, and 8th bits from the right in the bitmap are 1s, the output should contain v2, v6, and v7. Similarly, an example with 3-bit values is shown in Figure [4](#_bookmark12). Note that in this example, as the word size (32) is not a multiple of the bit width (3), some values such as v10 and v21 are placed across the word boundaries, which makes this problem even more challenging.

An obvious solution to this problem would be to scan over all bit-packed values, extracting and gathering selected bit-packed values one at a time, which runs in *ğ‘‚(n)* instructions. However, considering that each value is typically only a few bits long and much smaller than the processor word (e.g., 64 bits), this simple implementation does not fully utilize the width of a processor word, thus wasting the parallelism available in processors.

Hence, our goal is to design a *bit-parallel* select operator. Intuitively, this means that the algorithm is able to simultaneously process *all* values that are packed into a processor word, moving all selected values to appropriate positions in parallel. The formal definition of a bit-parallel algorithm is given in Definition [1.](#_bookmark7)

**Definition 1.** For a given word size **ğ‘¤** , an algorithm is a *bit-parallel algorithm* if it processes *ğ‘›ğ‘˜*-bit values in $ğ‘‚(\frac{nk}{w})$ instructions.

**é€‰æ‹©è¿ç®—ç¬¦**å°†ä¸€ä¸ªç”± n ä¸ª k ä½å€¼å’Œä¸€ä¸ª n ä½é€‰æ‹©ä½å›¾ç»„æˆçš„å­—èŠ‚æ•°ç»„ä½œä¸ºè¾“å…¥ã€‚å®ƒä¼šæå–æ‰€æœ‰åœ¨é€‰æ‹©ä½å›¾ä¸­å¯¹åº”ä½ä¸º 1 çš„é€‰å®šå€¼ï¼Œå¹¶å°†å®ƒä»¬å¤åˆ¶åˆ°è¾“å‡ºå­—èŠ‚æ•°ç»„çš„è¿ç»­ä½ä¸­ï¼Œå°±åƒæ‰€æœ‰æœªé€‰å®šå€¼çš„ä½éƒ½å·²ä»è¾“å…¥ä¸­åˆ é™¤ä¸€æ ·ã€‚

å›¾ 3 æ˜¾ç¤ºäº†ä» 8 ä¸ªç¤ºä¾‹ 4 ä½å€¼ä¸­é€‰æ‹© 3 ä¸ªæ—¶çš„è¾“å…¥å’Œé¢„æœŸè¾“å‡ºï¼ˆæš‚æ—¶å¿½ç•¥è®¡ç®—æ­¥éª¤ï¼‰ã€‚ç”±äºä½å›¾ä¸­ä»å³ä¾§æ•°ç¬¬ 3ã€7 å’Œ 8 ä½ä¸º 1ï¼Œå› æ­¤è¾“å‡ºåº”åŒ…å« v2ã€v6 å’Œ v7ã€‚ç±»ä¼¼åœ°ï¼Œå›¾ 4 æ˜¾ç¤ºäº†åŒ…å« 3 ä½å€¼çš„ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œåœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œç”±äºå­—é•¿ (32) ä¸æ˜¯ä½å®½ (3) çš„å€æ•°ï¼Œå› æ­¤æŸäº›å€¼ï¼ˆä¾‹å¦‚ v10 å’Œ v21ï¼‰ä¼šè¶…å‡ºå­—è¾¹ç•Œï¼Œè¿™ä½¿å¾—æ­¤é—®é¢˜æ›´å…·æŒ‘æˆ˜æ€§ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªæ˜¾è€Œæ˜“è§çš„æ–¹æ³•æ˜¯æ‰«ææ‰€æœ‰ä½æ‰“åŒ…å€¼ï¼Œä¸€æ¬¡æå–å¹¶æ”¶é›†ä¸€ä¸ªé€‰å®šçš„ä½æ‰“åŒ…å€¼ï¼Œè¿™éœ€è¦ O(n) æ¡æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°æ¯ä¸ªå€¼é€šå¸¸åªæœ‰å‡ ä½é•¿ï¼Œè¿œå°äºå¤„ç†å™¨å­—é•¿ï¼ˆä¾‹å¦‚ 64 ä½ï¼‰ï¼Œè¿™ç§ç®€å•çš„å®ç°å¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨å¤„ç†å™¨å­—å®½ï¼Œä»è€Œæµªè´¹äº†å¤„ç†å™¨å¯ç”¨çš„å¹¶è¡Œèƒ½åŠ›ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¾è®¡ä¸€ä¸ªä½å¹¶è¡Œé€‰æ‹©ç®—å­ã€‚ç›´è§‚åœ°è¯´ï¼Œè¿™æ„å‘³ç€è¯¥ç®—æ³•èƒ½å¤ŸåŒæ—¶å¤„ç†æ‰“åŒ…åˆ°å¤„ç†å™¨å­—ä¸­çš„æ‰€æœ‰å€¼ï¼Œå¹¶å°†æ‰€æœ‰é€‰å®šçš„å€¼å¹¶è¡Œç§»åŠ¨åˆ°é€‚å½“çš„ä½ç½®ã€‚ä½å¹¶è¡Œç®—æ³•çš„æ­£å¼å®šä¹‰å¦‚å®šä¹‰ 1 æ‰€ç¤ºã€‚

**å®šä¹‰ 1**. å¯¹äºç»™å®šçš„å­—é•¿ $w$ï¼Œå¦‚æœä¸€ä¸ªç®—æ³•åœ¨ $O(\frac{ nk} w)$ æ¡æŒ‡ä»¤ä¸­å¤„ç† n ä¸ª k ä½å€¼ï¼Œåˆ™è¯¥ç®—æ³•æ˜¯**ä½å¹¶è¡Œç®—æ³•**ã€‚

### 3.2 Simplified Algorithm

We first describe a simplified bit-parallel algorithm for the cases where the bit width *ğ‘˜* of values is a power of 2 such that no value is placed across word boundaries. We will extend this algorithm to support arbitrary bit widths in Section [3.3](#_bookmark13).

We initially use a special case of the problem to illustrate the basic idea behind the algorithm. Suppose that each value has only 1 bit (*ğ‘˜* = 1). In this special case, we want to extract all bits that correspond to 1s in the bitmap from the values. Interestingly, this is precisely what PEXT does by placing the values in the source operand and using the bitmap as the mask operand (Section [2.1.1](#_bookmark2)).

This observation can be generalized to handle wider values. For *ğ‘˜*-bit values, instead of using the select bitmap as the mask operand of PEXT directly, we need an *extended bitmap* that uses *ğ‘˜* bits to represent each bit in the original bitmap, enabling us to extract all *ğ‘˜* bits for every selected value. Conceptually, this extended bitmap can be generated by duplicating each bit in the select bitmap *ğ‘˜* times.

> æˆ‘ä»¬é¦–å…ˆæè¿°ä¸€ç§ç®€åŒ–çš„ä½å¹¶è¡Œç®—æ³•ï¼Œè¯¥ç®—æ³•é€‚ç”¨äºå€¼çš„ä½å®½ k ä¸º 2 çš„å¹‚ï¼Œä¸”æ‰€æœ‰å€¼éƒ½ä¸ä¼šè·¨è¶Šå­—è¾¹ç•Œçš„æƒ…å†µã€‚æˆ‘ä»¬å°†åœ¨ 3.3 èŠ‚ä¸­æ‰©å±•æ­¤ç®—æ³•ä»¥æ”¯æŒä»»æ„ä½å®½ã€‚
>
> æˆ‘ä»¬é¦–å…ˆä½¿ç”¨é—®é¢˜çš„ä¸€ä¸ªç‰¹ä¾‹æ¥è¯´æ˜è¯¥ç®—æ³•èƒŒåçš„åŸºæœ¬æ€æƒ³ã€‚å‡è®¾æ¯ä¸ªå€¼åªæœ‰ 1 ä½ï¼ˆk= 1ï¼‰ã€‚åœ¨è¿™ä¸ªç‰¹ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ä»å€¼ä¸­æå–ä½å›¾ä¸­æ‰€æœ‰ä¸ 1 å¯¹åº”çš„ä½ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™æ­£æ˜¯ PEXT æ‰€åšçš„ï¼Œå®ƒå°†å€¼æ”¾å…¥æºæ“ä½œæ•°ä¸­ï¼Œå¹¶ä½¿ç”¨ä½å›¾ä½œä¸ºæ©ç æ“ä½œæ•°ï¼ˆè§ 2.1.1 èŠ‚ï¼‰ã€‚
>
> è¿™ä¸€è§‚å¯Ÿç»“æœå¯ä»¥æ¨å¹¿åˆ°å¤„ç†æ›´å®½çš„å€¼ã€‚å¯¹äº k ä½å€¼ï¼Œæˆ‘ä»¬ä¸éœ€è¦ç›´æ¥ä½¿ç”¨é€‰æ‹©ä½å›¾ä½œä¸º PEXT çš„æ©ç æ“ä½œæ•°ï¼Œè€Œæ˜¯éœ€è¦ä¸€ä¸ª**æ‰©å±•ä½å›¾**ï¼Œå®ƒä½¿ç”¨ k ä½æ¥è¡¨ç¤ºåŸå§‹ä½å›¾ä¸­çš„æ¯ä¸ªä½ï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæå–æ¯ä¸ªé€‰å®šå€¼çš„æ‰€æœ‰ k ä½ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œè¿™ä¸ªæ‰©å±•ä½å›¾å¯ä»¥é€šè¿‡å°†é€‰æ‹©ä½å›¾ä¸­çš„æ¯ä¸ªä½å¤åˆ¶ k æ¬¡æ¥ç”Ÿæˆã€‚

Figure [3](#_bookmark9) shows the algorithm to select 3 4-bit values from 8 4-bit values. In the figure, we switch the background color to distinguish adjacent elements corresponding to different values. As seen in the figure, the algorithm runs in two steps. In the first step, it converts the input select bitmap **1**1**0**0**0**1**0**0 to an extended bitmap **1111**1111**0000**0000**0000**1111**0000**0000. In step 2, since all corresponding bits of the selected value have been set in the extended bitmap, we now can apply this extended bitmap and use PEXT to copy all selected bits to the output, essentially moving only the selected values v7, v6, and v2 to the output.

> å›¾ 3 å±•ç¤ºäº†ä» 8 ä¸ª 4 ä½å€¼ä¸­é€‰æ‹© 3 ä¸ª 4 ä½å€¼çš„ç®—æ³•ã€‚å›¾ä¸­ï¼Œæˆ‘ä»¬åˆ‡æ¢èƒŒæ™¯é¢œè‰²ä»¥åŒºåˆ†å¯¹åº”äºä¸åŒå€¼çš„ç›¸é‚»å…ƒç´ ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œè¯¥ç®—æ³•åˆ†ä¸¤æ­¥è¿è¡Œã€‚ç¬¬ä¸€æ­¥ï¼Œå®ƒå°†è¾“å…¥é€‰æ‹©ä½å›¾ **1**1**0**0**0**1**0**0 è½¬æ¢ä¸ºæ‰©å±•ä½å›¾ **1111**1111**0000**0000**0000**1111**0000**0000ã€‚åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œç”±äºæ‰€é€‰å€¼çš„æ‰€æœ‰å¯¹åº”ä½éƒ½å·²åœ¨æ‰©å±•ä½å›¾ä¸­è®¾ç½®ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åº”ç”¨æ­¤æ‰©å±•ä½å›¾å¹¶ä½¿ç”¨ PEXT å°†æ‰€æœ‰é€‰å®šä½å¤åˆ¶åˆ°è¾“å‡ºï¼Œæœ¬è´¨ä¸Šåªå°†é€‰å®šå€¼ v7ã€v6 å’Œ v2 ç§»åŠ¨åˆ°è¾“å‡ºã€‚

![image-20250602094131945](./image/03.png)Fig. 3. Bit-parallel selection on 8 4-bit values

With BMI, we design an elegant way to convert a select bitmap to the extended bitmap using only three instructions (two PDEP and one subtraction), regardless of the bit width of values. Figure [3](#_bookmark9) shows this computation on the example values in step 1. **The first PDEP instruction** moves each bit in the select bitmap to the rightmost position in the corresponding *ğ‘˜*-bit field in the extended bitmap, according to the mask 0^ğ‘˜âˆ’1^1...0^ğ‘˜âˆ’1^1 (we use exponentiation to denote the bit repetition, e.g., 1^4^0^2^ = 111100). **The second PDEP instruction** uses a modified mask (*ğ‘šğ‘ğ‘ ğ‘˜*  1), where the rightmost 1 is removed from *ğ‘šğ‘ğ‘ ğ‘˜*. As a result, each bit in the select bitmap is now moved to the rightmost position in the *next ğ‘˜*-bit field in the extended bitmap. Thus, in the result mask *â„ğ‘–ğ‘”â„*, each moved bit is actually outside its corresponding *ğ‘˜*-bit field, and can be thought of as a â€œborrowedâ€ bit from the next field. With the two result masks *ğ‘™ğ‘œğ‘¤* and *â„ğ‘–ğ‘”â„*, we now perform a subtraction between the two masks (*â„ğ‘–ğ‘”â„ ğ‘™ğ‘œğ‘¤* ) to produce an extended bitmap. This last step relies on the propagating of the carries to set all bits between a pair of 1s to 1s, as illustrated below:

> åˆ©ç”¨ BMIï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä¼˜é›…çš„æ–¹æ³•ï¼Œåªéœ€ä¸‰æ¡æŒ‡ä»¤ï¼ˆä¸¤æ¡ PDEP æŒ‡ä»¤å’Œä¸€æ¡å‡æ³•æŒ‡ä»¤ï¼‰å³å¯å°†é€‰æ‹©ä½å›¾è½¬æ¢ä¸ºæ‰©å±•ä½å›¾ï¼Œè€Œæ— éœ€è€ƒè™‘å€¼çš„ä½å®½ã€‚å›¾ 3 å±•ç¤ºäº†æ­¥éª¤ 1 ä¸­ç¤ºä¾‹å€¼çš„è®¡ç®—è¿‡ç¨‹ã€‚ç¬¬ä¸€æ¡ PDEP æŒ‡ä»¤æ ¹æ®æ©ç  0^kâˆ’1^1...0^kâˆ’1^1ï¼ˆæˆ‘ä»¬ä½¿ç”¨å¹‚æ¥è¡¨ç¤ºä½é‡å¤ï¼Œä¾‹å¦‚ 1^4^0^2^= 111100ï¼‰ï¼Œå°†é€‰æ‹©ä½å›¾ä¸­çš„æ¯ä¸ªä½ç§»åŠ¨åˆ°æ‰©å±•ä½å›¾ä¸­ç›¸åº” k ä½å­—æ®µçš„æœ€å³ä¾§ä½ç½®ã€‚ç¬¬äºŒæ¡ PDEP æŒ‡ä»¤ä½¿ç”¨ä¿®æ”¹åçš„æ©ç ï¼ˆæ©ç  âˆ’ 1ï¼‰ï¼Œå…¶ä¸­æœ€å³ä¾§çš„ 1 è¢«ä»æ©ç ä¸­ç§»é™¤ã€‚<u>å› æ­¤ï¼Œé€‰æ‹©ä½å›¾ä¸­çš„æ¯ä¸ªä½ç°åœ¨éƒ½ç§»åŠ¨åˆ°æ‰©å±•ä½å›¾ä¸­ä¸‹ä¸€ä¸ª k ä½å­—æ®µçš„æœ€å³ä¾§ä½ç½®</u>ã€‚å› æ­¤ï¼Œåœ¨ç»“æœæ©ç é«˜ä½ä¸­ï¼Œæ¯ä¸ªç§»åŠ¨çš„ä½å®é™…ä¸Šéƒ½ä½äºå…¶å¯¹åº”çš„ k ä½å­—æ®µä¹‹å¤–ï¼Œå¯ä»¥å°†å…¶è§†ä¸ºä»ä¸‹ä¸€ä¸ªå­—æ®µâ€œå€Ÿç”¨â€çš„ä½ã€‚æœ‰äº†ä¸¤ä¸ªç»“æœæ©ç çš„ä½ä½å’Œé«˜ä½ï¼Œæˆ‘ä»¬ç°åœ¨å¯¹è¿™ä¸¤ä¸ªæ©ç ï¼ˆé«˜ä½ - ä½ä½ï¼‰è¿›è¡Œå‡æ³•è¿ç®—ï¼Œä»¥ç”Ÿæˆä¸€ä¸ªæ‰©å±•ä½å›¾ã€‚æœ€åä¸€æ­¥ä¾èµ–äºè¿›ä½çš„ä¼ æ’­ï¼Œå°†ä¸€å¯¹ 1 ä¹‹é—´çš„æ‰€æœ‰ä½è®¾ç½®ä¸º 1ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![image-20250602094408413](./image/k.png)

Notice that the 1-bit in high prevents carries from propagating to the next ğ‘˜-bit field. As a result, the calculations are safely performed inside each *ğ‘˜*-bit field and never interfere with each other. Thus, the subtraction acts as if it processes all *ğ‘˜*-bit fields in parallel.

The abovementioned algorithm is summarized in Algorithm [1](#_bookmark10) and Algorithm [2](#_bookmark11) (we show the extend operator as a separate operator, as it will be reused in Section [5.4.1](#_bookmark31)). In addition to the input values and bitmap, it takes a mask as input. For *ğ‘˜*-bit values where ğ‘˜ is a power of 2, we set *ğ‘šğ‘ğ‘ ğ‘˜*  = 0^ğ‘˜âˆ’1^1 0^ğ‘˜âˆ’1^1. If the input contains a large number of values packed into multiple processor words, we run Algorithm [1](#_bookmark10) on each word and concatenate the output through bit shifting.

> - [ ] **Algorithm 1** select (*ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ *, *ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘*, *ğ‘šğ‘ğ‘ ğ‘˜*)
> - [ ] **Algorithm 2** extend (*ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘*, *ğ‘šğ‘ğ‘ ğ‘˜*)

The length of the output for each word can be calculated by performing the `POPCNT` instruction (counting 1s in a processor word) on the input select bitmap.

### 3.3 General Algorithm

We next extend the simplified algorithm to support an arbitrary bit width *ğ‘˜*. Figure [4](#_bookmark12) shows an example of selecting 8 values from 32 3-bit values that are packed into 3 32-bit words. Since the bit width *ğ‘˜* = 3 is not a power of 2, there are values (v10 and v21) placed across word boundaries. The key challenge of the general algorithm lies in dealing with these partial values with minimal overhead.

> æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰©å±•ç®€åŒ–ç®—æ³•ä»¥æ”¯æŒä»»æ„ä½å®½ kã€‚å›¾ 4 å±•ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹ï¼Œè¯¥ç¤ºä¾‹ä» 32 ä¸ª 3 ä½å€¼ä¸­é€‰æ‹© 8 ä¸ªå€¼ï¼Œè¿™äº›å€¼è¢«æ‰“åŒ…æˆ 3 ä¸ª 32 ä½å­—ã€‚ç”±äºä½å®½ k=3 ä¸æ˜¯ 2 çš„å¹‚ï¼Œå› æ­¤å­˜åœ¨è·¨å­—è¾¹ç•Œçš„å€¼ï¼ˆv10 å’Œ v21ï¼‰ã€‚é€šç”¨ç®—æ³•çš„å…³é”®æŒ‘æˆ˜åœ¨äºä»¥æœ€å°çš„å¼€é”€å¤„ç†è¿™äº›éƒ¨åˆ†å€¼ã€‚

<a id="_bookmark12"></a>
![image-20250602084000038](./image/04.png)

Fig. 4. Bit-parallel selection on 32 3-bit values (v10 and v21 span over multiple words)

> æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿å¤„ç†å™¨å­—ä¸­åŒ…å«**éƒ¨åˆ†å€¼ï¼ˆpartial valuesï¼‰**ï¼Œåªè¦æ©ç æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªè¦æ±‚ï¼Œç®—æ³• [1](#_bookmark10) ä»ç„¶æœ‰æ•ˆã€‚é¦–å…ˆï¼Œæ©ç **éœ€æŒ‰å­—çš„å¸ƒå±€è¿›è¡Œç§»ä½å¯¹é½**ã€‚åœ¨å›¾ [4](#_bookmark12) ä¸­ï¼Œå­— 2 ä¸­çš„æ©ç éœ€**å·¦ç§» 2 ä½**ï¼Œå› ä¸ºå­— 2 ä¸­çš„éƒ¨åˆ†å€¼ v21 å ç”¨äº†å‰©ä½™çš„ 2 ä½ã€‚ç±»ä¼¼åœ°ï¼Œå­— 1 ä¸­çš„æ©ç å·¦ç§» 1 ä½ï¼Œä»¥å®¹çº³å­— 1 ä¸­ v10 çš„å‰©ä½™ 1 ä½ã€‚å…¶æ¬¡ï¼Œ**mask** ä¸­çš„æœ€ä½æœ‰æ•ˆä½å¿…é¡»ä¸º 1ï¼Œå³ä½¿å®ƒå¯¹åº”äºå€¼ä¸­é—´çš„ä¸€ä½ã€‚å¯¹äºæœ€å³ç«¯å…·æœ‰éƒ¨åˆ†å€¼çš„å­—ï¼Œæœ€å³è¾¹ä½ç½®çš„è¿™ä¸ªé¢å¤–çš„ 1 ç¡®ä¿å‡æ³•æŒ‡ä»¤èƒ½å¤Ÿä¸ºæ‰©å±•ä½å›¾ä¸­çš„éƒ¨åˆ†å€¼ç”Ÿæˆä¸€ä¸ª 1 åºåˆ—ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾ [4,](#_bookmark12) ä¸­ï¼Œå­— 1 ä¸­ **mask** çš„æœ€å³ä½è¢«è®¾ç½®ä¸º 1ï¼Œå°½ç®¡å®ƒå¯¹åº”çš„æ˜¯ v10 çš„ç¬¬ä¸‰ä½ã€‚è¿™ä¸ªé¢å¤–çš„ 1 ä½å¼•å¯¼**ç¬¬ä¸€æ¡ PDEP æŒ‡ä»¤**ï¼šå°†é€‰æ‹©ä½å›¾çš„æœ€å³æ¯”ç‰¹ä½ç§»è‡³ **low** æ©ç çš„æœ€æœ«ä½ï¼Œä»è€Œåœ¨æ‰©å±•ä½å›¾æœ€å³ç«¯ç”Ÿæˆæ‰€éœ€çš„ 1 ã€‚

Interestingly, we find that Algorithm [1](#_bookmark10) remains valid even for words containing partial values, as long as the masks meet the two requirements shown as follows. First, the mask needs to be shifted to be aligned with the layout of the word. In Figure [4,](#_bookmark12) the mask in word 2 is left shifted by 2 bits, as there are 2 remaining bits in the partial value v21 in word 2. Similarly, the mask in word 1 is left shifted by 1 bit to accommodate the 1 remaining bit of v10 in word 1. Second, the least significant bit in a *ğ‘šğ‘ğ‘ ğ‘˜* must be 1, even though it corresponds to a bit in the middle of a value. For a word with a partial value on the right end, this extra 1 at the rightmost position ensures that the subtraction instruction is able to generate a sequence of 1s for the partial value in the extended bitmap. For example, in Figure [4,](#_bookmark12) the rightmost bit of *ğ‘šğ‘ğ‘ ğ‘˜* in word 1 is set to 1 though it corresponds to the third bit of v10. This extra 1-bit guides the first PDEP instruction to move the rightmost bit from the select bitmap to the rightmost position in **low** , which then results in the expected 1-bit on the right end of the extended bitmap.

In general, Algorithm [3](#_bookmark14) shows the steps to generate masks for an arbitrary word size ğ‘¤ and bit width ğ‘˜. For a given ğ‘¤ and ğ‘˜, we put ğ‘¤ values in one group that span over ğ‘˜ processor words. It is clear that the words at the same position in these groups can use the same mask as they share the same layout of values. As a result, we only need to generate ğ‘˜ masks, one for each word in a group. These masks are always pre-created and reused repeatedly.

> æ€»ä½“è€Œè¨€ï¼Œç®—æ³• [3](#_bookmark14) å±•ç¤ºäº†ä¸ºä»»æ„å­—é•¿ ğ‘¤ å’Œä½å®½ ğ‘˜ ç”Ÿæˆæ©ç çš„æ­¥éª¤ã€‚å¯¹äºç»™å®šçš„ ğ‘¤ å’Œ ğ‘˜ï¼Œæˆ‘ä»¬å°†  k ä¸ª ğ‘¤ å­—é•¿çš„å€¼åˆ†ä¸ºä¸€ç»„ï¼Œå®ƒä»¬è·¨è¶Š k ä¸ªå¤„ç†å™¨å­—ã€‚æ˜¾ç„¶ï¼Œ**åŒç»„å†…ç›¸åŒä½ç½®çš„å¤„ç†å™¨å­—å¯ä½¿ç”¨ç›¸åŒæ©ç **ï¼Œå› ä¸ºå®ƒä»¬å€¼çš„å¸ƒå±€ä¸€è‡´ã€‚æ‰€ä»¥æˆ‘ä»¬åªéœ€ç”Ÿæˆ  k ä¸ªæ©ç ï¼ˆæ¯ç»„å†…æ¯ä¸ªå­—å¯¹åº”ä¸€ä¸ªï¼‰ã€‚è¿™äº›æ©ç æ˜¯é¢„å…ˆåˆ›å»ºï¼Œå¹¶å¯é‡å¤ä½¿ç”¨ã€‚

**ç®—æ³•3 generate_masks(w, k)** <a id="_bookmark14"></a> 
1: $masks \leftarrow \emptyset $  
2: **for** \( i := 0 \) **to** \( k \) **do**  
3: â€ƒ $offset \leftarrow k - (i \times w) \% k$  
4: â€ƒ  $masks.\text{add}\left( (0^{k-1}1 \ldots 0^{k-1}1 \ll offset) \lor 1 \right)$  
5: **return** masks 

With this approach, the general algorithm needs to run the same four instructions described in Algorithm [1](#_bookmark10) and [2](#_bookmark11) on each word, and surprisingly, it does not introduce any additional overhead compared to the simplified algorithm. It is also worth noting that the simplified algorithm is a specialization of the general algorithm. When the bit width *ğ‘˜* is a power of 2, the general algorithm will generate the same *ğ‘šğ‘ğ‘ ğ‘˜* for all words in a group and the mask generated by Algorithm [3](#_bookmark14) is identical to the one described in Section [3.2](#_bookmark8).

According to Definition [1,](#_bookmark7) the proposed algorithm is clearly a bit-parallel algorithm since it runs a constant number of instructions on each processor word, regardless of the bit width of values or the selectivity of the select bitmap.

> é‡‡ç”¨è¿™ç§æ–¹æ³•ï¼Œé€šç”¨ç®—æ³•éœ€è¦åœ¨æ¯ä¸ªå­—ä¸Šè¿è¡Œç®—æ³• [1](#_bookmark10) å’Œ [2](#_bookmark11) ä¸­æè¿°çš„å››æ¡æŒ‡ä»¤ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä¸ç®€åŒ–ç®—æ³•ç›¸æ¯”ï¼Œå®ƒå¹¶æ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„å¼€é”€ã€‚å¦å¤–å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç®€åŒ–ç®—æ³•æ˜¯é€šç”¨ç®—æ³•çš„ç‰¹ä¾‹ã€‚å½“ä½å®½ $ğ‘˜$ æ˜¯ 2 çš„å¹‚æ—¶ï¼Œé€šç”¨ç®—æ³•å°†ä¸ºåŒä¸€ç»„ä¸­çš„æ‰€æœ‰å­—ç”Ÿæˆç›¸åŒçš„ **ğ‘šğ‘ğ‘ ğ‘˜**ï¼Œå¹¶ä¸”ç®—æ³• [3](#_bookmark14) ç”Ÿæˆçš„æ©ç ä¸ [3.2](#_bookmark8) èŠ‚ä¸­æè¿°çš„æ©ç ç›¸åŒã€‚
>
> æ ¹æ®å®šä¹‰ [1](#_bookmark7)ï¼Œæ‰€æå‡ºçš„ç®—æ³•æ˜¾ç„¶æ˜¯ä¸€ç§ä½å¹¶è¡Œç®—æ³•ï¼Œå› ä¸ºå®ƒåœ¨æ¯ä¸ªå¤„ç†å™¨å­—ä¸Šè¿è¡Œæ’å®šæ•°é‡çš„æŒ‡ä»¤ï¼Œè€Œä¸å€¼çš„ä½å®½æˆ–æ‰€é€‰ä½å›¾çš„é€‰æ‹©æ€§æ— å…³ã€‚

## 4 SELECTION PUSHDOWN

Given the fast select operator described in Section [3,](#_bookmark6) we next present how to take *full* advantage of it in evaluating an arbitrary scan query. In this section, we will also introduce the second use case of BMI, which is critical in enabling selection pushdown in the framework.

> åŸºäºç¬¬ [3](#_bookmark6) èŠ‚ä¸­æè¿°çš„å¿«é€Ÿé€‰æ‹©è¿ç®—ç¬¦ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥å°†ä»‹ç»å¦‚ä½•åœ¨æ‰§è¡Œä»»æ„æ‰«ææŸ¥è¯¢æ—¶**å……åˆ†**åˆ©ç”¨å®ƒã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è¿˜å°†ä»‹ç» BMI çš„ç¬¬äºŒä¸ªç”¨ä¾‹ï¼Œå®ƒå¯¹äºåœ¨æ¡†æ¶ä¸­å®ç°é€‰æ‹©ä¸‹æ¨è‡³å…³é‡è¦ã€‚

### 4.1 Framework

Our framework aims to accelerate arbitrary scan queries by making the best use of the select operator. A scan query returns the values of the projection columns (i.e., in the SELECT clause) from the records that match the filters on a list of filter columns (i.e., in the WHERE clause). For the sake of simplicity, we first assume that the WHERE clause is a conjunction of filters, which is the most common case. We will relax this assumption in Section [4.5](#_bookmark22), extending the framework to allow conjunctions, disjunctions, negations, or an arbitrary boolean combination of them.

The framework is built upon a simple yet crucial observation: when performing a `filter` or `project` operation, records failing to meet prior predicates can be bypassed directly. While this observation is undeniably obvious, previous approaches have not leveraged it effectively. Indeed, in the case of filter operations, previous work tends to perform predicate evaluation on **all** values [[29](#_bookmark74), [34](#_bookmark80)], intentionally ignoring the fact that some values might have been filtered by prior filters. **This is primarily because the additional cost associated with the select operator often outweighs the potential savings in predicate evaluation**. However, given the fast select operator that operates on encoded values (Section [3](#_bookmark6)), it has become more favorable to select values upfront, even for filter operations. Consequently, our framework is designed to take full advantage of the BMI-based select operator in both project and filter operations. Interestingly, this design also introduces new technical challenges that can be addressed by using BMI (Section [4.3](#_bookmark18)).

In this framework, each filter operation produces a select bitmap as the output, which uses one bit per record to indicate if the corresponding record matches all filters that have been evaluated so far. The select bitmap can be fed into the next filter operation or the remaining project operations to accelerate the subsequent operations.

> æˆ‘ä»¬çš„æ¡†æ¶æ—¨åœ¨é€šè¿‡å……åˆ†åˆ©ç”¨ select è¿ç®—ç¬¦æ¥åŠ é€Ÿä»»æ„æ‰«ææŸ¥è¯¢ã€‚æ‰«ææŸ¥è¯¢ä¼šä»ä¸ä¸€ç³»åˆ—è¿‡æ»¤åˆ—ï¼ˆå³ WHERE å­å¥ä¸­çš„ filterï¼‰åŒ¹é…çš„è®°å½•ä¸­è¿”å›æŠ•å½±åˆ—ï¼ˆå³ SELECT å­å¥ä¸­çš„åˆ—ï¼‰çš„å€¼ã€‚ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬é¦–å…ˆå‡è®¾ WHERE å­å¥æ˜¯ç­›é€‰å™¨çš„**åˆå–**ï¼ˆå³é€šè¿‡ AND è¿æ¥ï¼‰ï¼Œè¿™æ˜¯æœ€å¸¸è§çš„æƒ…å†µã€‚æˆ‘ä»¬å°†åœ¨ [4.5](#_bookmark22) èŠ‚ä¸­æ”¾å®½æ­¤å‡è®¾ï¼Œå°†æ¡†æ¶æ‰©å±•åˆ°æ”¯æŒåˆå–ã€æå–ã€å¦å®šæˆ–å…¶ä»»æ„å¸ƒå°”ç»„åˆçš„è¿‡æ»¤æ¡ä»¶ã€‚
>
> è¯¥æ¡†æ¶åŸºäºä¸€ä¸ªç®€å•ä½†å…³é”®çš„è§‚å¯Ÿï¼šåœ¨æ‰§è¡Œ `filter`ï¼ˆè¿‡æ»¤ï¼‰æˆ– `project`ï¼ˆæŠ•å½±ï¼‰æ“ä½œæ—¶ï¼Œ**ä¸æ»¡è¶³å…ˆå‰è°“è¯**çš„è®°å½•å¯ä»¥ç›´æ¥è·³è¿‡ã€‚è™½ç„¶è¿™ä¸€è§‚å¯Ÿæ˜¾è€Œæ˜“è§ï¼Œä½†ä¹‹å‰çš„æ–¹æ³•å¹¶æœªæœ‰æ•ˆåœ°åˆ©ç”¨å®ƒã€‚äº‹å®ä¸Šï¼Œåœ¨è¿‡æ»¤æ“ä½œä¸­ï¼Œå…ˆå‰çš„ç ”ç©¶å€¾å‘äºå¯¹æ‰€æœ‰å€¼æ‰§è¡Œè°“è¯æ±‚å€¼ [[29](#_bookmark74), [34](#_bookmark80)]ï¼Œè€Œæœ‰æ„å¿½ç•¥äº†æŸäº›å€¼å¯èƒ½å·²è¢«å…ˆå‰çš„è¿‡æ»¤å™¨è¿‡æ»¤æ‰çš„äº‹å®ã€‚è¿™ä¸»è¦æ˜¯å› ä¸º**é€‰æ‹©è¿ç®—ç¬¦çš„é¢å¤–å¼€é”€**é€šå¸¸è¶…è¿‡äº†è°“è¯è®¡ç®—å¯èƒ½èŠ‚çœçš„æˆæœ¬ã€‚ç„¶è€Œï¼Œå€ŸåŠ©åŸºäºç¼–ç å€¼çš„é«˜æ•ˆé€‰æ‹©è¿ç®—ç¬¦ï¼ˆç¬¬ [3](#_bookmark6) èŠ‚ï¼‰ï¼Œ**æå‰è¿‡æ»¤å€¼**å˜å¾—æ›´å…·ä¼˜åŠ¿ï¼Œå³ä½¿å¯¹äºè¿‡æ»¤æ“ä½œä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ—¨åœ¨**å……åˆ†åˆ©ç”¨åŸºäº BMIï¼ˆä½å›¾ç´¢å¼•ï¼‰çš„é€‰æ‹©è¿ç®—ç¬¦**ï¼ŒåŒæ—¶æ”¯æŒæŠ•å½±å’Œè¿‡æ»¤æ“ä½œã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™ç§è®¾è®¡ä¹Ÿå¸¦æ¥äº†æ–°çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œä½†å¯é€šè¿‡ BMI æ¥è§£å†³ï¼ˆç¬¬ [4.3](#_bookmark18) èŠ‚ï¼‰ã€‚
>
> åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ¯ä¸ªè¿‡æ»¤æ“ä½œä¼šç”Ÿæˆä¸€ä¸ª**é€‰æ‹©ä½å›¾**ä½œä¸ºè¾“å‡ºï¼Œå…¶ä¸­æ¯æ¡è®°å½•ç”¨ **1 æ¯”ç‰¹ä½**è¡¨ç¤ºå…¶æ˜¯å¦æ»¡è¶³å½“å‰æ‰€æœ‰å·²è¯„ä¼°çš„è¿‡æ»¤æ¡ä»¶ã€‚è¯¥é€‰æ‹©ä½å›¾å¯ä¼ é€’è‡³ä¸‹ä¸€ä¸ªè¿‡æ»¤æ“ä½œæˆ–å‰©ä½™çš„æŠ•å½±æ“ä½œï¼Œä»è€ŒåŠ é€Ÿåç»­æµç¨‹ã€‚

**Running example.** To illustrate our methods, we use a running example throughout this section. The query is shown below:

> **è´¯ç©¿æœ¬èŠ‚çš„ç¤ºä¾‹**ã€‚ä¸ºäº†è¯´æ˜æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­**ä½¿ç”¨å¦‚ä¸‹ç¤ºä¾‹**ã€‚æŸ¥è¯¢å¦‚ä¸‹ï¼š

```sql
SELECT c FROM R WHERE a < 10 AND b < 4
```

Figure [5](#_bookmark16) shows the input and output of each operation to evaluate the example query. The query is converted as a filter operation on column *ğ‘*, followed by another filter operation on column *ğ‘*, and ended with a project operation on column *ğ‘*. The first filter must read all column values and, thus, has no input select bitmap. The produced bitmap, bitmap~ğ‘~, is then passed to the second filter operation that now can skip the values in records that fail to satisfy the first predicate. The second filter operation refines the select bitmap according to the predicate on column *ğ‘*, resulting in an updated bitmap, bitmap~ğ‘~ , with 4 bits set to 1. Finally, we pass bitmap~ğ‘~ to the project operation as an input to select values in column *ğ‘* from matching records.

> å›¾ [5](#_bookmark16) å±•ç¤ºäº†ç”¨äºè¯„ä¼°ç¤ºä¾‹æŸ¥è¯¢æ—¶æ¯ä¸ªæ“ä½œçš„è¾“å…¥å’Œè¾“å‡ºã€‚è¯¥æŸ¥è¯¢è¢«è½¬æ¢ä¸ºï¼šé¦–å…ˆå¯¹åˆ— **ğ‘** æ‰§è¡Œè¿‡æ»¤æ“ä½œï¼Œæ¥ç€å¯¹åˆ— **ğ‘** æ‰§è¡Œç¬¬äºŒä¸ªè¿‡æ»¤æ“ä½œï¼Œæœ€åå¯¹åˆ— **ğ‘** æ‰§è¡ŒæŠ•å½±æ“ä½œã€‚ç¬¬ä¸€ä¸ªè¿‡æ»¤æ“ä½œ**æ²¡æœ‰è¾“å…¥é€‰æ‹©ä½å›¾**ï¼Œå› æ­¤å¿…é¡»è¯»å–æ‰€æœ‰åˆ—å€¼ã€‚ç”Ÿæˆä½å›¾ bitmap~ğ‘~ ä¼ é€’ç»™ç¬¬äºŒä¸ªè¿‡æ»¤å™¨ï¼Œæ­¤æ—¶è¯¥æ“ä½œå¯è·³è¿‡**ä¸æ»¡è¶³ç¬¬ä¸€ä¸ªè°“è¯**çš„è®°å½•ä¸­çš„å€¼ã€‚ç¬¬äºŒä¸ªè¿‡æ»¤æ“ä½œæ ¹æ®åˆ— **ğ‘** ä¸Šçš„è°“è¯æ›´æ–°é€‰æ‹©ä½å›¾ï¼Œç”Ÿæˆæ›´æ–°åçš„ä½å›¾ bitmap~ğ‘~ ï¼Œå…¶ä¸­ 4 ä½è®¾ç½®ä¸º 1ã€‚æœ€åï¼Œæˆ‘ä»¬å°† bitmap~ğ‘~ ä½œä¸ºè¾“å…¥ä¼ é€’ç»™æŠ•å½±æ“ä½œï¼Œä»¥ä»åŒ¹é…çš„è®°å½•ä¸­é€‰æ‹©åˆ— **ğ‘** ä¸­çš„å€¼ã€‚

<a id="_bookmark16"></a>
|     ![Figure 1](./image/05.png)     |
| :------------------------------------: |
| Fig. 5. Operations in evaluating the example query |

### 4.2 Operation Implementation

In our framework, filter and project operations can be implemented by composing four basic operators, as shown below:

- **Select**. As the first step, we use the select operator described in Section [3](#_bookmark6) to remove irrelevant values from the target column. Pushing down the select operator results in a reduced number of values that need to be passed to the subsequent operators. This step can be skipped if the filter or project operation has no input select bitmap (e.g., the first filter in a query).

- **Unpack**. Next, we use the unpack operator to convert the encoded values to their native representation in primitive data types. We adopt the state-of-the-art SIMD-based implementation [[41](#_bookmark86)] for this operator. For project operations, we can now return the unpacked results and skip the remaining two operators/steps.

- **Evaluate**. For filter operations, we next evaluate all decoded values with the filter predicate and generate a bitmap to indicate whether each (selected) value satisfies the predicate. Since all column values have been unpacked and decoded, this operator allows arbitrary predicates. Furthermore, since all (selected) column values are now stored in primitive data types, this enables a more straightforward implementation of predicate evaluation using SIMD vectorization [[34â€“36]](#_bookmark81).

- **Transform.** The bitmap produced by an evaluate operator may not be directly used as a select bitmap for the next operation. This is because the bitmap has as many bits as the selected records, rather than all records. The transform operator is designed to convert such a bitmap into an appropriate select bitmap that can be used for the subsequent operation(s). In Section [4.3](#_bookmark18), we will describe an efficient way to implement this operator using BMI.

As an example, Table [2](#_bookmark17) shows the steps of the example filter and project operations. The first filter operation is implemented as an unpack operator followed by an evaluate operator. The select and transform operators are avoided because this is the first filter and has to read all values. In contrast, the second filter operation performs all four operators: it pre-selects the values based on bitmap*ğ‘*, which, however, requires an additional bitmap transformation at the end of this operation. The refined bitmap, bitmap*ğ‘* , is then used to accelerate the project operation on column c, which is implemented as a select operator followed by an unpack operator.

![image-20250602120623090](./image/table-2.png)
Table 2. Implementation of example filter and project operations

> åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œè¿‡æ»¤å’ŒæŠ•å½±æ“ä½œå¯ä»¥é€šè¿‡ç»„åˆå››ä¸ªåŸºæœ¬è¿ç®—ç¬¦æ¥å®ç°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
>
> - **é€‰æ‹©ï¼ˆSelectï¼‰**ï¼šç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ [3](#_bookmark6) èŠ‚ä¸­æè¿°çš„é€‰æ‹©è¿ç®—ç¬¦ï¼Œä»ç›®æ ‡åˆ—ä¸­åˆ é™¤ä¸ç›¸å…³çš„å€¼ã€‚**ä¸‹æ¨é€‰æ‹©è¿ç®—ç¬¦å¯å‡å°‘éœ€ä¼ é€’åˆ°åç»­æ“ä½œç¬¦çš„æ•°æ®é‡**ã€‚å¦‚æœè¿‡æ»¤æˆ–æŠ•å½±æ“ä½œæ²¡æœ‰è¾“å…¥é€‰æ‹©ä½å›¾ï¼ˆä¾‹å¦‚ï¼ŒæŸ¥è¯¢ä¸­çš„ç¬¬ä¸€ä¸ªè¿‡æ»¤å™¨ï¼‰ï¼Œåˆ™å¯ä»¥è·³è¿‡æ­¤æ­¥éª¤ã€‚
>
> - **è§£åŒ…ï¼ˆUnpackï¼‰**ï¼šæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨è§£åŒ…è¿ç®—ç¬¦å°†ç¼–ç å€¼è½¬æ¢ä¸ºåŸå§‹æ•°æ®ç±»å‹çš„åŸç”Ÿè¡¨ç¤ºå½¢å¼ã€‚å¯¹æ­¤è¿ç®—ç¬¦ï¼Œæˆ‘ä»¬é‡‡ç”¨æœ€å…ˆè¿›çš„åŸºäº SIMD çš„å®ç° [[41](#_bookmark86)]ã€‚å¯¹äºæŠ•å½±æ“ä½œï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥è¿”å›è§£åŒ…ç»“æœå¹¶è·³è¿‡å‰©ä¸‹çš„ä¸¤ä¸ªè¿ç®—ç¬¦/æ­¥éª¤ã€‚
>
> - **æ±‚å€¼ï¼ˆEvaluateï¼‰**ï¼šå¯¹äºè¿‡æ»¤æ“ä½œï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥ä½¿ç”¨è¿‡æ»¤è°“è¯è¯„ä¼°æ‰€æœ‰è§£ç å€¼ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªä½å›¾æ¥æŒ‡ç¤ºæ¯ä¸ªï¼ˆé€‰æ‹©çš„ï¼‰å€¼æ˜¯å¦æ»¡è¶³è°“è¯ã€‚ç”±äºæ‰€æœ‰åˆ—å€¼éƒ½å·²è§£åŒ…å’Œè§£ç ï¼Œå› æ­¤è¯¥è¿ç®—ç¬¦å…è®¸ä»»æ„è°“è¯ã€‚æ­¤å¤–ï¼Œç”±äºæ‰€æœ‰ï¼ˆé€‰æ‹©çš„ï¼‰åˆ—å€¼ç°åœ¨å­˜å‚¨ä¸ºåŸå§‹æ•°æ®ç±»å‹ï¼Œå¯é€šè¿‡ SIMD å‘é‡åŒ– [[34â€“36]](#_bookmark81) æ›´ç®€å•åœ°å®ç°è°“è¯è¯„ä¼°ã€‚
>
> - **è½¬æ¢ï¼ˆTransformï¼‰**ï¼šè¯„ä¼°è¿ç®—ç¬¦ç”Ÿæˆçš„ä½å›¾ä¸èƒ½ç›´æ¥ç”¨ä½œä¸‹ä¸€ä¸ªæ“ä½œçš„é€‰æ‹©ä½å›¾ã€‚<u>è¿™æ˜¯å› ä¸ºä½å›¾çš„ä½æ•°ä¸é€‰å®šçš„è®°å½•æ•°ç›¸åŒï¼Œè€Œä¸æ˜¯æ‰€æœ‰è®°å½•çš„ä½æ•°</u>ã€‚è½¬æ¢è¿ç®—ç¬¦æ—¨åœ¨å°†æ­¤ç±»ä½å›¾è½¬æ¢ä¸ºé€‚ç”¨äºåç»­æ“ä½œçš„é€‰æ‹©ä½å›¾ã€‚ç¬¬ [4.3](#_bookmark18) èŠ‚å°†æè¿°å¦‚ä½•é€šè¿‡ BMI é«˜æ•ˆå®ç°æ­¤è¿ç®—ç¬¦ã€‚
>
> ä¾‹å¦‚ï¼Œè¡¨ [2](#_bookmark17) å±•ç¤ºäº†ç¤ºä¾‹è¿‡æ»¤å’ŒæŠ•å½±æ“ä½œçš„æ­¥éª¤ã€‚ç¬¬ä¸€ä¸ªè¿‡æ»¤æ“ä½œå®ç°ä¸ºä¸€ä¸ªè§£åŒ…è¿ç®—ç¬¦ï¼Œåè·Ÿä¸€ä¸ªæ±‚å€¼è¿ç®—ç¬¦ã€‚ç”±äºè¿™æ˜¯ç¬¬ä¸€ä¸ªè¿‡æ»¤å™¨ï¼Œéœ€è¦è¯»å–æ‰€æœ‰å€¼ï¼Œå› æ­¤é¿å…ä½¿ç”¨é€‰æ‹©è¿ç®—ç¬¦å’Œè½¬æ¢è¿ç®—ç¬¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¬¬äºŒä¸ªè¿‡æ»¤æ“ä½œæ‰§è¡Œæ‰€æœ‰å››ä¸ªè¿ç®—ç¬¦ï¼šå®ƒæ ¹æ®ä½å›¾ **ğ‘** é¢„å…ˆé€‰æ‹©å€¼ï¼Œä½†è¿™éœ€è¦åœ¨æ­¤æ“ä½œç»“æŸæ—¶è¿›è¡Œé¢å¤–çš„ä½å›¾è½¬æ¢ã€‚ç„¶åï¼Œä½¿ç”¨æ”¹è¿›çš„ä½å›¾ bitmap~ğ‘~ æ¥åŠ é€Ÿå¯¹ **c** åˆ—çš„æŠ•å½±æ“ä½œï¼Œè¯¥æ“ä½œå®ç°ä¸ºä¸€ä¸ªé€‰æ‹©è¿ç®—ç¬¦ï¼Œåè·Ÿä¸€ä¸ªè§£åŒ…è¿ç®—ç¬¦ã€‚

#### Unpack å’Œ Decode

Unpack çš„æœ¬è´¨æ˜¯æ•°æ®ä»**å­˜å‚¨ä¼˜åŒ–å½¢æ€**åˆ°**è®¡ç®—ä¼˜åŒ–å½¢æ€**çš„è½¬æ¢ï¼Œå®ƒå¹³è¡¡äº†å­˜å‚¨æ•ˆç‡ä¸è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡ SIMD åŠ é€Ÿå’Œé€‰æ‹©ä¸‹æ¨ï¼Œè¯¥æ“ä½œæˆä¸ºç°ä»£åˆ†æå‹æ•°æ®åº“å®ç°é«˜æ€§èƒ½æŸ¥è¯¢çš„åŸºçŸ³æŠ€æœ¯ã€‚

åœ¨æ•°æ®åº“æ‰§è¡Œå¼•æ“çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œ**ï¼ˆè§£åŒ…ï¼‰å’Œ Decodeï¼ˆè§£ç ï¼‰æœ¬è´¨ç›¸åŒ**ï¼Œä½†å­˜åœ¨**ç»†ç²’åº¦æŠ€æœ¯å·®å¼‚**ã€‚ä»¥ä¸‹æ˜¯å…³é”®è§£æï¼š

##### ä¸€ã€**æœ¯è¯­å®šä¹‰å¯¹æ¯”**

| **æœ¯è¯­**   | **æŠ€æœ¯æœ¬è´¨**                                            | **æ“ä½œç›®æ ‡**                | **è¾“å…¥/è¾“å‡º**       |
| :--------- | :------------------------------------------------------ | :-------------------------- | :------------------ |
| **Unpack** | å°†**å‹ç¼©å­˜å‚¨çš„ç¼–ç å€¼**è½¬æ¢ä¸º**è¿ç»­å†…å­˜çš„åŸç”Ÿæ•°æ®**      | ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼ + æ•°æ®å¯¹é½ | ç¼–ç å€¼ â†’ åŸå§‹å€¼æ•°ç»„ |
| **Decode** | å°†**ç‰¹å®šç¼–ç æ ¼å¼**ï¼ˆå¦‚å­—å…¸/RLE/ä½å‹ç¼©ï¼‰**è§£é‡Šä¸ºè¯­ä¹‰å€¼** | è¿˜åŸæ•°æ®è¯­ä¹‰                | å‹ç¼©æ•°æ®å— â†’        |

##### äºŒã€**åœ¨æ¡†æ¶ä¸­çš„å®é™…å…³ç³»**

```mermaid
graph TB
    A[åˆ—å­˜å‚¨æ•°æ®] -->|å‹ç¼©å—| B(Selectç®—å­)
    B -->|ç­›é€‰åçš„ç¼–ç å€¼| C[Unpack]
    C --> D{å†…å­˜æ“ä½œ}
    D -->|ç‰©ç†è§£å‹| E[è¿ç»­å†…å­˜åŸå§‹æ•°ç»„]
    D -->|é€»è¾‘è§£é‡Š| F[Decodeï¼šè½¬æ¢ç¼–ç è§„åˆ™]
    E & F --> G[Evaluate]
```

1. **è€¦åˆæ€§**
   åœ¨å®ç°ä¸­ï¼ŒUnpack **å¿…ç„¶åŒ…å« Decode**
   - *ä¾‹*ï¼šå­—å…¸ç¼–ç åˆ—éœ€å…ˆæŸ¥å­—å…¸ï¼ˆDecodeï¼‰å†å¡«å……åˆ°è¿ç»­å†…å­˜ï¼ˆUnpackï¼‰
2. **å·®å¼‚ç„¦ç‚¹**
   - `Decode` ä¾§é‡ **æ•°æ®è¯­ä¹‰è¿˜åŸ**ï¼ˆç¼–ç å€¼ â†’ é€»è¾‘å€¼ï¼‰
     *å¦‚ï¼šå­—å…¸ID 42 â†’ å­—ç¬¦ä¸² "Beijing"*
   - `Unpack` ä¾§é‡ **å†…å­˜å¸ƒå±€è½¬æ¢**ï¼ˆåˆ†æ•£å­˜å‚¨ â†’ è¿ç»­æ•°ç»„ï¼‰
     *å¦‚ï¼šä½å‹ç¼©æ•°æ®[0b1101] â†’ æ•°ç»„[1,0,1,1]*

### 4.3 Bitmap Transform Operator

To demonstrate the need for the transform operator, we first walk through the filter operation on column b in the running example. Figure [6](#_bookmark19) breaks down the key steps of this operation. **As the first step**, it applies bitmap~ğ‘~, i.e., the bitmap generated by the filter on column a, and selects 8 values (v3, v9-11, v18, v24, v29, v31) that pass the first filter (see Figure [4](#_bookmark12) for detailed steps). **Next**, we unpack these 3-bit encoded values and evaluate all decoded values, producing a 8-bit bitmap called **filtered** (step 2). **However**, this bitmap indicates whether each **selected value**, rather than any value, in the column satisfies the predicate, and thus needs to be transformed in order to be used as a select bitmap for the next operation(s).

To transform the filtered bitmap, we need to deposit the bits in â€œfilteredâ€ to the bit positions corresponding to the selected values in the select bitmap bitmap~ğ‘~ (i.e., the 1s in bitmap~ğ‘~). In other words, we need to replace the *ğ‘–*-th 1 in the select bitmap with the *ğ‘–*-th bit in â€œfilteredâ€, while retaining all 0s in the select bitmap at their original bit positions. Interestingly, this is exactly what the PDEP instruction performs by using â€œfilteredâ€ as the source operand and the select bitmap as the mask operand (see Section [2.1](#_bookmark1) and Figure [2](#_bookmark3)). Continuing the example from Figure [6](#_bookmark19) (step 3), we replace the first (rightmost) 1 in bitmap~ğ‘~ by the first (rightmost) bit 0 from â€œfilteredâ€, indicating that the first selected value v3 does not pass the predicate on column b. The use of PDEP enables us to move all 8 bits from â€œfilteredâ€ to appropriate positions in the select bitmap in parallel. It is worth noting that without the hardware-implemented BMI instruction, this transformation (as well as the select operator) would be significantly more expensive as we showed in Table [1](#_bookmark4), which further highlights the crucial role of BMI in the design of the entire solution.

<a id="_bookmark19"></a>

|     ![Figure 1](./image/06.png)     |
| :------------------------------------: |
| Fig. 6. Selection pushdown on example column b |

> ä¸ºäº†æ¼”ç¤ºå˜æ¢è¿ç®—ç¬¦çš„å¿…è¦æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨è¿è¡Œç¤ºä¾‹ä¸­æ¼”ç¤ºå¯¹ b åˆ—çš„è¿‡æ»¤æ“ä½œã€‚å›¾ [6](#_bookmark19) åˆ†è§£äº†æ­¤æ“ä½œçš„å…³é”®æ­¥éª¤ã€‚**ç¬¬ä¸€æ­¥**åº”ç”¨ bitmap~ğ‘~ï¼ˆå³ç”± a åˆ—ä¸Šçš„è¿‡æ»¤å™¨ç”Ÿæˆçš„ä½å›¾ï¼‰ï¼Œå¹¶é€‰æ‹©é€šè¿‡ç¬¬ä¸€ä¸ªè¿‡æ»¤å™¨çš„ 8 ä¸ªå€¼ (v3ï¼Œv9-11ï¼Œv18ï¼Œv24ï¼Œv29ï¼Œv31)ï¼ˆè¯¦ç»†æ­¥éª¤å‚è§å›¾ [4](#_bookmark12)ï¼‰ã€‚**æ¥ä¸‹æ¥**ï¼Œæˆ‘ä»¬è§£åŒ…è¿™äº› 3 ä½ç¼–ç å€¼å¹¶è¯„ä¼°æ‰€æœ‰è§£ç å€¼ï¼Œç”Ÿæˆä¸€ä¸ªåä¸º **filtered** çš„ 8 ä½ä½å›¾ï¼ˆæ­¥éª¤ 2ï¼‰ã€‚**ä½†æ˜¯**ï¼Œæ­¤ä½å›¾æŒ‡ç¤ºçš„æ˜¯åˆ—ä¸­æ¯ä¸ª**é€‰å®šçš„å€¼**ï¼ˆè€Œä¸æ˜¯ä»»ä½•å€¼ï¼‰æ˜¯å¦æ»¡è¶³è°“è¯ï¼Œå› æ­¤éœ€è¦ç»è¿‡è½¬æ¢æ‰èƒ½ç”¨ä½œåç»­æ“ä½œçš„é€‰æ‹©ä½å›¾ã€‚
>
> è¦è½¬æ¢è¿™ä¸ª **filtered** ä½å›¾ï¼Œæˆ‘ä»¬éœ€è¦å°† **filtered** ä¸­çš„ä½å­˜å…¥é€‰æ‹©ä½å›¾ bitmap~ğ‘~ ä¸­é€‰å®šå€¼å¯¹åº”çš„ä½ä½ç½®ï¼ˆå³ bitmap~ğ‘~ ä¸­çš„ 1ï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬éœ€è¦å°†é€‰æ‹©ä½å›¾ä¸­çš„ç¬¬ *ğ‘–* ä¸ª 1 æ›¿æ¢ä¸º **filtered** ä¸­çš„ç¬¬ *ğ‘–* ä¸ªä½ï¼ŒåŒæ—¶å°†é€‰æ‹©ä½å›¾ä¸­çš„æ‰€æœ‰ 0 ä¿ç•™åœ¨å…¶åŸå§‹ä½ä½ç½®ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™æ­£æ˜¯ PDEP æŒ‡ä»¤é€šè¿‡ä½¿ç”¨ **filtered** ä½œä¸ºæºæ“ä½œæ•°ï¼Œå¹¶ä½¿ç”¨é€‰æ‹©ä½å›¾ä½œä¸ºæ©ç æ“ä½œæ•°æ‰€æ‰§è¡Œçš„æ“ä½œï¼ˆå‚è§ç¬¬ [2.1](#_bookmark1) å’Œå›¾ [2](#_bookmark3)ï¼‰ã€‚ç»§ç»­å›¾ [6](#_bookmark19)ï¼ˆæ­¥éª¤ 3ï¼‰ä¸­çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬ç”¨ **filtered** ä¸­ï¼ˆæœ€å³ä¾§ï¼‰çš„ç¬¬ä¸€ä¸ª 0 æ›¿æ¢ bitmap~ğ‘~ ä¸­ï¼ˆæœ€å³ä¾§ï¼‰çš„ç¬¬ä¸€ä¸ª 1ï¼Œè¿™è¡¨æ˜ç¬¬ä¸€ä¸ªé€‰å®šå€¼ v3 æœªé€šè¿‡ b åˆ—çš„è°“è¯ã€‚ä½¿ç”¨ PDEP ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°† **filtered** ä¸­çš„æ‰€æœ‰ 8 ä½å¹¶è¡Œç§»åŠ¨åˆ°é€‰æ‹©ä½å›¾ä¸­çš„é€‚å½“ä½ç½®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœæ²¡æœ‰ç¡¬ä»¶å®ç°çš„ BMI æŒ‡ä»¤ï¼Œæ­¤è½¬æ¢ï¼ˆä»¥åŠé€‰æ‹©è¿ç®—ç¬¦ï¼‰çš„å¼€é”€å°†æ˜¾è‘—å¢åŠ ï¼Œå¦‚è¡¨ [1](#_bookmark4) æ‰€ç¤ºï¼Œè¿™è¿›ä¸€æ­¥å‡¸æ˜¾äº† BMI åœ¨æ•´ä¸ªè§£å†³æ–¹æ¡ˆè®¾è®¡ä¸­çš„å…³é”®ä½œç”¨ã€‚

### 4.4 Filter Ordering <a id="_bookmark20"></a>

Thus far we have assumed that all filters are evaluated in the same order as specified in the query. We next address the query optimization problem of determining the order of filters. Unlike traditional filter ordering problems, this problem requires the consdieration of both filter selectivity and the bit width of columns, both of which affect scan performance (see Figure [12](#_bookmark39)). To solve this problem, we first develop a cost model and then propose a simple yet efficient greedy algorithm to determine the best order. For simplicity, we assume that filter predicates are independent of each other and the selectivity of each filter is pre-known, e.g., via selectivity estimation techniques such as [[37]](#_bookmark82).

4.4.1 **Cost Model**. Let **ğ‘˜** denote the bit width, **ğ‘¤** denote the processor word size, and **ğ‘ ** denote the selectivity where $ğ‘  \in [0, 1]$. We assume a sequence of **ğ‘›** filters, **ğ‘“~1~** ... **ğ‘“~ğ‘›~**. Our objective is to minimize the cost of running the filter sequence. The cost of any filter (except the first filter **ğ‘“~1~**) is the sum of the cost to run the select operator on all **ğ‘˜**-bit values and the cost to unpack and evaluate selected values and the cost to transform the bitmap. According to Definition [1](#_bookmark7), its runtime is positively correlated with the bit width. <u>Thus</u>, the cost of select for a filter **ğ‘“~i~** is $\propto \frac{k_i}{w} $. The unpack and evaluate operators run on some subset of values that have been filtered by all p*ğ‘¤*rior filters. The number of values that any filter will unpack and evaluate equals to the total number of values multiplied by the product of selectivity of all prior filters. <u>Thus</u>, the cost of unpack and evaluate for a filter **ğ‘“~i~** is $\propto  {\textstyle \prod_{j=1}^{i-1}}s_j$. The cost of transform can be ignored as it uses only one PDEP instruction. <u>Finally</u>, the cost of a filter **ğ‘“~i~** in a sequence (except the first filter **ğ‘“~1~**) is $\propto (\frac{k_i}{w} + {\textstyle \prod_{j=1}^{i-1}}s_j)$. The first filter **ğ‘“~1~** does not use select and transform operators, but it needs to unpack and evaluate all values. Putting all these pieces together, the cost of **ğ‘“~1~** ... **ğ‘“~n~** is:

<a id="_bookmark21"></a>
$$
\operatorname{cost}\left(f_{1}\right)+\sum_{i=2}^{n} \operatorname{cost}\left(f_{i}\right) \propto 1+\sum_{i=2}^{n}\left(\frac{k_{i}}{w}+\prod_{j=1}^{i-1} s_{j}\right) \propto \sum_{i=2}^{n} \frac{k_{i}}{w}+\sum_{i=2}^{n} \prod_{j=1}^{i-1} s_{j}\qquad\qquad (1)
$$

4.4.2 **Optimal Order**. For a set of *ğ‘›* filters, our goal is to find a sequence with the lowest cost as defined in Equation [1](#_bookmark21). We make two key observations from Equation [1](#_bookmark21) that can significantly prune the search space: 1) for sequences starting with the same filter, the term ${\textstyle \sum_{i=2}^{n}}\frac{k_i}{w}    $ in Equation [1](#_bookmark21) filters; 2) to minimize the second term ${\textstyle \sum_{i=2}^{n}}\frac{k_i}{w} {\textstyle \prod_{j=1}^{i-1}}s_j    $, we should sort all filters in ascending order of selectivity, assuming the first filter has been determined.

Based on these observations, it becomes evident that a simple greedy approach can find the *optimal* order: we begin by selecting an arbitrary filter as the first filter; the optimal order of the remaining filters can be found by sorting them in the ascending order of their selectivity, whose cost can be calculated by using Equation [1](#_bookmark21); we then compare all *ğ‘›* possible choices for the first filter and find the one with the lowest overall cost. This approach drastically reduces our search space from **ğ‘‚(ğ‘›!)** to **ğ‘‚(ğ‘›)** candidate sequences, and the obtained order is optimal under the aforementioned assumptions. Relaxing these assumptions is an interesting direction for future work.

### 4.5 Supporting Disjunctions and Negations

Finally, we extend our framework to allow conjunctions, disjunctions, negations, or an arbitrary boolean combination of them. For each disjunction in the WHERE clause, we always convert it to a combination of conjunctions and negations by applying De Morganâ€™s laws: $a \vee b = \neg (\neg a\wedge \neg b)$. To support negations, we add a boolean flag, namely **negate**, as an addition input parameter to the filter operation. If this flag is true, we need to flip the bitmap produced by the evaluate operator. All other operators within a filter operation remain unchanged. With this approach, our framework supports disjunctions and negations with negligible overhead.

> æœ€åï¼Œæˆ‘ä»¬æ‰©å±•äº†æ¡†æ¶ï¼Œä½¿å…¶æ”¯æŒåˆå–ï¼ˆANDï¼‰ã€æå–ï¼ˆORï¼‰ã€å¦å®šï¼ˆNOTï¼‰æˆ–å…¶ä»»æ„çš„å¸ƒå°”ç»„åˆã€‚å¯¹äº WHERE å­å¥ä¸­çš„æ¯ä¸ªæå–ï¼ˆORï¼‰æ¡ä»¶ï¼Œæˆ‘ä»¬æ€»æ˜¯åº”ç”¨ De Morgan å®šå¾‹ï¼š$a \vee b = \neg (\neg a\wedge \neg b)$ï¼Œå°†å…¶è½¬æ¢ä¸ºåˆå–ï¼ˆANDï¼‰å’Œå¦å®šï¼ˆNOTï¼‰çš„ç»„åˆã€‚ä¸ºäº†æ”¯æŒå¦å®šï¼ˆNOTï¼‰ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼Œåä¸º **negate**ï¼Œä½œä¸ºè¿‡æ»¤æ“ä½œçš„é™„åŠ è¾“å…¥å‚æ•°ã€‚å¦‚æœè¯¥æ ‡å¿—ä¸ºçœŸï¼Œå¦‚æœæ­¤æ ‡å¿—ä¸ºçœŸï¼Œåˆ™éœ€è¦ç¿»è½¬æ±‚å€¼è¿ç®—ç¬¦ç”Ÿæˆçš„ä½å›¾ã€‚è¿‡æ»¤æ“ä½œä¸­çš„æ‰€æœ‰å…¶ä»–è¿ç®—ç¬¦ä¿æŒä¸å˜ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿä»¥å¯å¿½ç•¥çš„å¼€é”€æ”¯æŒæå–ï¼ˆORï¼‰å’Œå¦å®šï¼ˆNOTï¼‰ã€‚

```mermaid
graph 
    A[a OR b] -->|å¾·æ‘©æ ¹å®šå¾‹| B["NOT(NOT a AND NOT b)"]
    B --> C["Filter a: è¾“å‡ºä½å›¾A"]
    C --> D["å–å: NOT A"]
    B --> E["Filter b: è¾“å‡ºä½å›¾B"]
    E --> F["å–å: NOT B"]
    D --> G["åˆå¹¶ä½å›¾: NOT A AND NOT B"]
    F --> G
    G --> H["å–å: NOT(NOT A AND NOT B)"]
```

## 5 SELECTION PUSHDOWN IN PARQUET

The techniques described in the previous sections are general techniques that can be applied to most column stores. In this section, we adapt and extend these techniques to enable selection pushdown in one specific but widely adopted storage format, Apache Parquet.

> å‰å‡ èŠ‚ä¸­æè¿°çš„æŠ€æœ¯æ˜¯é€šç”¨æŠ€æœ¯ï¼Œå¯ä»¥åº”ç”¨äºå¤§å¤šæ•°åˆ—å­˜å‚¨ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è°ƒæ•´å’Œæ‰©å±•è¿™äº›æŠ€æœ¯ï¼Œä»¥ä¾¿åœ¨ä¸€ç§ç‰¹å®šä½†å¹¿æ³›é‡‡ç”¨çš„å­˜å‚¨æ ¼å¼ Apache Parquet ä¸­å®ç°é€‰æ‹©ä¸‹æ¨ã€‚

### 5.1 Overview

As we described in Section [2.2](#_bookmark5), each column value in Parquet is a triple: **repetition level**, definition level, **field value**. Repetition and definition levels are metadata that is used to represent complex structure in a columnar manner. A select operation in Parquet takes as input a column that includes encoded repetition and definition levels as well as field values, and a bitmap indicating the records to be selected, and outputs the repetition/definition levels and the field values of all selected records, as if we use the standard reader to read a Parquet file that contains the matching records only.

The challenge arises from the way that Parquet encodes the structure information to represent optional, nested, or repeated fields (Section [2.2](#_bookmark5)). As Parquet never explicitly stores null values and all repeated values are stored contiguously in the same array, the number of levels or values in the column may not be the same as the number of records, meaning that the select operation we presented in Section [3](#_bookmark6) is not directly applicable to Parquet.

In this section, we present our techniques to efficiently transform the input select bitmap to the ones that can be applied to the field values and repetition/definition levels. This transformation requires the knowledge of the structure of data, which is represented by repetition and definition levels. In the interest of space, we omit the formal definitions of these concepts, but introduce two simple facts that we will use throughout this section: â‘  a column value is null if its definition level is not equal to the maximum definition level of the column; â‘¡ a column value belongs to the same record of the previous column value if its repetition level is not 0.

**Running example**. Figure [7](#_bookmark24) shows an example repeated column that contains 32 column values belonging to 24 records. Each column value has one definition level and one repetition level. So there are 32 definition/repetition levels. The mapping between levels and records can be built by looking at the repetition levels according to â‘¡ : the 1st and 2nd levels belong to the first record as the 2nd repetition level is non-0; the 2nd and 3rd records have only one value; the next three levels all belong to the 4th record, etc. Half of the 32 column values have a definition level that is not equal to 2 (the maximum definition level in this column), meaning that there are 16 null values (â‘ ). These null values are not explicitly stored in the field values. As a result, even though the column contains 32 column values, it has only 16 non-null field values stored in the value array. In Figure [7](#_bookmark24), we also include a 24-bit select bitmap. Each bit in the bitmap indicates whether each record, i.e., all column values belonging to the record, need to be selected. We connect each bit in the select bitmap to the corresponding repetition and definition levels, and the non-null field value in solid lines. Thus, a level or value needs to be included in the selected column if and only if it is connected to a 1 in the select bitmap.

<a id="_bookmark24"></a>

|     ![Figure 1](./image/07.png)     |
| :------------------------------------: |
| Fig. 7. Selecting an example repeated column in Parquet |

> æ­£å¦‚æˆ‘ä»¬åœ¨ [2.2](#_bookmark5) èŠ‚ä¸­æ‰€è¿°ï¼ŒParquet ä¸­çš„æ¯ä¸ªåˆ—å€¼éƒ½æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„ï¼š**é‡å¤çº§åˆ«**ï¼Œ**å®šä¹‰çº§åˆ«**ï¼Œ**å­—æ®µå€¼**ã€‚é‡å¤çº§åˆ«å’Œå®šä¹‰çº§åˆ«æ˜¯ç”¨äºä»¥åˆ—å¼æ–¹å¼è¡¨ç¤ºå¤æ‚ç»“æ„çš„å…ƒæ•°æ®ã€‚Parquet ä¸­çš„ select æ“ä½œä»¥ä¸€ä¸ªåˆ—ä½œä¸ºè¾“å…¥ï¼ŒåŒ…å«ç¼–ç åçš„é‡å¤å±‚çº§ã€å®šä¹‰å±‚çº§ä»¥åŠå­—æ®µå€¼ï¼ŒåŒæ—¶è¿˜éœ€è¦ä¸€ä¸ªæŒ‡ç¤ºå¾…é€‰è®°å½•çš„ä½å›¾ï¼›è¾“å‡ºæ‰€æœ‰è¢«é€‰ä¸­è®°å½•çš„é‡å¤/å®šä¹‰å±‚çº§å’Œå­—æ®µå€¼ï¼Œå°±åƒæˆ‘ä»¬ä½¿ç”¨æ ‡å‡† reader è¯»å–ä»…åŒ…å«åŒ¹é…è®°å½•çš„ Parquet æ–‡ä»¶ä¸€æ ·ã€‚
>
> æŒ‘æˆ˜æºäº Parquet å¯¹ç»“æ„ä¿¡æ¯è¿›è¡Œç¼–ç ä»¥è¡¨ç¤ºå¯é€‰ã€åµŒå¥—æˆ–é‡å¤å­—æ®µçš„æ–¹å¼ï¼ˆ[2.2](#_bookmark5) èŠ‚ï¼‰ã€‚ç”±äº Parquet ä»ä¸æ˜¾å¼å­˜å‚¨ç©ºå€¼ï¼Œå¹¶ä¸”æ‰€æœ‰é‡å¤å€¼éƒ½è¿ç»­å­˜å‚¨åœ¨åŒä¸€ä¸ªæ•°ç»„ä¸­ï¼Œå› æ­¤åˆ—ä¸­çš„çº§åˆ«æˆ–å€¼çš„æ•°é‡å¯èƒ½ä¸è®°å½•æ•°ä¸åŒï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åœ¨ [3](#_bookmark6) èŠ‚ä¸­ä»‹ç»çš„ select æ“ä½œä¸èƒ½ç›´æ¥åº”ç”¨äº Parquetã€‚
>
> åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€äº›æŠ€æœ¯ï¼Œç”¨äºé«˜æ•ˆåœ°å°†è¾“å…¥çš„é€‰æ‹©ä½å›¾è½¬æ¢ä¸ºå¯åº”ç”¨äºå­—æ®µå€¼å’Œé‡å¤/å®šä¹‰çº§åˆ«çš„ä½å›¾ã€‚æ­¤è½¬æ¢éœ€è¦äº†è§£æ•°æ®çš„ç»“æ„ä¿¡æ¯ï¼Œè¯¥ç»“æ„ç”±é‡å¤å’Œå®šä¹‰çº§åˆ«è¡¨ç¤ºã€‚é™äºç¯‡å¹…ï¼Œæˆ‘ä»¬çœç•¥äº†è¿™äº›æ¦‚å¿µçš„æ­£å¼å®šä¹‰ï¼Œä½†å¼•å…¥ä¸¤ä¸ªå°†åœ¨æœ¬èŠ‚é€šç¯‡ä½¿ç”¨çš„ç®€å•äº‹å®ï¼šâ‘  å¦‚æœåˆ—å€¼çš„å®šä¹‰çº§åˆ«ä¸ç­‰äºè¯¥åˆ—çš„æœ€å¤§å®šä¹‰çº§åˆ«ï¼Œåˆ™è¯¥åˆ—å€¼ä¸ºç©ºï¼›â‘¡ å¦‚æœåˆ—å€¼çš„é‡å¤çº§åˆ«ä¸ä¸º 0ï¼Œåˆ™è¯¥åˆ—å€¼ä¸å…¶å‰ä¸€ä¸ªåˆ—å€¼å±äºåŒä¸€æ¡è®°å½•ã€‚
>
> **æœ¬èŠ‚ç¤ºä¾‹**ã€‚å›¾ [7](#_bookmark24) å±•ç¤ºäº†ä¸€ä¸ªé‡å¤åˆ—çš„ç¤ºä¾‹ï¼Œå…¶ä¸­åŒ…å« 24 æ¡è®°å½•çš„ 32 ä¸ªåˆ—å€¼ã€‚æ¯ä¸ªåˆ—å€¼éƒ½æœ‰ä¸€ä¸ªå®šä¹‰çº§åˆ«å’Œä¸€ä¸ªé‡å¤çº§åˆ«ã€‚å› æ­¤ï¼Œå…±æœ‰ 32 ä¸ªå®šä¹‰/é‡å¤çº§åˆ«ã€‚å¯ä»¥é€šè¿‡æ ¹æ® â‘¡ æŸ¥çœ‹é‡å¤çº§åˆ«æ¥æ„å»ºçº§åˆ«å’Œè®°å½•ä¹‹é—´çš„æ˜ å°„ï¼šç¬¬ 1 å’Œç¬¬ 2 ä¸ªçº§åˆ«å±äºç¬¬ä¸€æ¡è®°å½•ï¼ˆå› ä¸ºç¬¬ 2 ä¸ªé‡å¤çº§åˆ«é 0 ï¼‰ï¼›ç¬¬äºŒå’Œç¬¬ä¸‰æ¡è®°å½•åªæœ‰ä¸€ä¸ªå€¼ï¼›æ¥ä¸‹æ¥çš„ä¸‰ä¸ªçº§åˆ«å‡å±äºç¬¬å››æ¡è®°å½•ï¼Œä¾æ­¤ç±»æ¨ã€‚32 ä¸ªåˆ—å€¼ä¸­æœ‰ä¸€åŠçš„å®šä¹‰çº§åˆ«ä¸ç­‰äº 2ï¼ˆæ­¤åˆ—ä¸­çš„æœ€å¤§å®šä¹‰çº§åˆ«ï¼‰ï¼Œè¿™æ„å‘³ç€æœ‰ 16 ä¸ªç©ºå€¼ (â‘ )ã€‚è¿™äº›ç©ºå€¼æœªæ˜¾å¼å­˜å‚¨åœ¨å­—æ®µå€¼ä¸­ã€‚å› æ­¤ï¼Œå³ä½¿è¯¥åˆ—åŒ…å« 32 ä¸ªåˆ—å€¼ï¼Œå…¶å€¼æ•°ç»„ä¸­ä¹Ÿåªå­˜å‚¨äº† 16 ä¸ªéç©ºå­—æ®µå€¼ã€‚åœ¨å›¾ [7](#_bookmark24) ä¸­ï¼Œæˆ‘ä»¬è¿˜åŒ…å«ä¸€ä¸ª 24 ä½çš„é€‰æ‹©ä½å›¾ã€‚ä½å›¾ä¸­çš„æ¯ä¸ªä½æŒ‡ç¤ºæ˜¯å¦éœ€è¦é€‰æ‹©æ¯æ¡è®°å½•ï¼ˆå³å±äºè¯¥è®°å½•çš„æ‰€æœ‰åˆ—å€¼ï¼‰ã€‚æˆ‘ä»¬å°†é€‰æ‹©ä½å›¾ä¸­çš„æ¯ä¸ªä½ä¸ç›¸åº”çš„é‡å¤å’Œå®šä¹‰çº§åˆ«ä»¥åŠéç©ºå­—æ®µå€¼ç”¨å®çº¿è¿æ¥èµ·æ¥ã€‚å› æ­¤ï¼Œå½“ä¸”ä»…å½“æŸä¸ªçº§åˆ«æˆ–å€¼ä¸é€‰æ‹©ä½å›¾ä¸­çš„ 1 è¿æ¥æ—¶ï¼Œæ‰éœ€è¦å°†å…¶åŒ…å«åœ¨é€‰å®šåˆ—ä¸­ã€‚

### 5.2 Workflow

Algorithm [4](#_bookmark25) shows the workflow to select repetition/definition levels and field values from a Parquet column based on a select bitmap. The basic idea of the algorithm is to transform the input select bitmap to two auxiliary select bitmaps, called **level bitmap** and **value bitmap**, that can be used to select the definition/repetition levels and values, respectively. The level bitmap is generated by copying each bit in the select bitmap as many times as the number of levels in the corresponding record. Then, the value bitmap can be created by removing the bits corresponding to null values from the level bitmap. Figure [8](#_bookmark26) illustrates the transformations from the select bitmap to the level and value bitmaps for the example column.

> ç®—æ³• [4](#_bookmark25) å±•ç¤ºäº†åŸºäºé€‰æ‹©ä½å›¾ä» Parquet åˆ—ä¸­é€‰æ‹©é‡å¤/å®šä¹‰çº§åˆ«å’Œå­—æ®µå€¼çš„å·¥ä½œæµç¨‹ã€‚è¯¥ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¾“å…¥çš„é€‰æ‹©ä½å›¾è½¬æ¢ä¸ºä¸¤ä¸ª**è¾…åŠ©é€‰æ‹©ä½å›¾**â€”â€”**å±‚çº§ä½å›¾**ï¼ˆlevel bitmapï¼‰å’Œ**å€¼ä½å›¾**ï¼ˆvalue bitmapï¼‰ï¼Œå®ƒä»¬åˆ†åˆ«ç”¨äºé€‰æ‹©å®šä¹‰/é‡å¤çº§åˆ«å’Œå­—æ®µå€¼ã€‚å±‚çº§ä½å›¾çš„ç”Ÿæˆæ–¹å¼æ˜¯ï¼šå°†é€‰æ‹©ä½å›¾ä¸­çš„æ¯ä¸€ä½å¤åˆ¶è‹¥å¹²æ¬¡ï¼Œ**å¤åˆ¶çš„æ¬¡æ•°ç­‰äºå¯¹åº”è®°å½•ä¸­çš„å±‚çº§æ•°é‡**ã€‚éšåï¼Œé€šè¿‡ä»å±‚çº§ä½å›¾ä¸­å‰”é™¤ä¸ç©ºå€¼ï¼ˆnull valuesï¼‰å¯¹åº”çš„ä½æ¥åˆ›å»ºå€¼ä½å›¾ã€‚å›¾ [8](#_bookmark26) å±•ç¤ºäº†**ç¤ºä¾‹åˆ—**ä»é€‰æ‹©ä½å›¾åˆ°å±‚çº§å’Œå€¼ä½å›¾çš„è½¬æ¢è¿‡ç¨‹ã€‚

<a id="_bookmark25"></a>

| **Algorithm 4** select-parquet (*ğ‘Ÿğ‘’ğ‘ğ‘ *, *ğ‘‘ğ‘’ğ‘“ğ‘ *, *ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ *, *ğ‘ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡* ) |
| :----------------------------------------------------------: |
|                    ![](./image/algo4.png)                    |

<a id="_bookmark26"></a>

|     ![](./image/08.png)     |
| :------------------------------------: |
| Fig. 8. Transformations on the select bitmap |

In the first part of the algorithm (Line 1-5), we produce the level bitmap for selecting the repetition and definition levels. This step can be skipped for the simple cases where the column has no repeated values: we can reuse the input select bitmap directly (Line 1) as the number of levels matches the number of records. To produce the level bitmap, we first produce a bitmap called **record bitmap** by finding the first levels of all records, i.e., all repetition levels that are 0s ( 2 ) (Line 4), and then extend the input select bitmap to the level bitmap using the produced record bitmap (Line 5). In Section [5.3](#_bookmark27), we will describe a bit-parallel operator for the former step, and present a way to reuse an existing operator (extend) for the latter step in Section [5.4.1](#_bookmark31). Now, with the produced level bitmap that has been aligned to the levels, we can reuse the select operator presented in Section [3](#_bookmark6) to select both repetition levels (Line 6) and definition levels (Line 12).

Similarly, to accommodate the fact that all null values are not physically stored in the field values, we also need to generate the value bitmap (Line 7-11). According to 1 , we can find all null values by comparing the definition levels to the maximum definition level of the column (Line 10). The result bitmap, called *valid bitmap*, is then used to compress the input select bitmap, by removing all bits that correspond to null values (Line 11). We describe this step in more details in Section [5.4.2](#_bookmark33).

Finally, we select the field values by using the value bitmap as the select bitmap (Line 13), and return all selected field values along with the repetition/definition levels. Note that for the arguably most common cases where the column is simply a required column, all bitmap transformations are not needed, and only the field values are selected. In this case, the Parquet select operator is reduced to the standard select operator that we presented in Section [3](#_bookmark6).

It is worth pointing out that, according to Definition [1](#_bookmark7), all operators used in Algorithm [4](#_bookmark25) are bit-parallel algorithms. Additionally, all operators rely on either the PDEP or PEXT instruction to achieve the full data parallelism available in processor words.

>  åœ¨ç®—æ³•çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆç¬¬ 1-5 è¡Œï¼‰ï¼Œæˆ‘ä»¬ç”Ÿæˆç”¨äºé€‰æ‹©é‡å¤å’Œå®šä¹‰çº§åˆ«çš„å±‚çº§ä½å›¾ã€‚å¯¹äºåˆ—ä¸­æ²¡æœ‰é‡å¤å€¼çš„ç®€å•æƒ…å†µï¼Œè¿™ä¸€æ­¥å¯ä»¥è·³è¿‡ï¼šç”±äº<u>**å±‚çº§æ•°**</u>ä¸è®°å½•æ•°åŒ¹é…ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥é‡ç”¨è¾“å…¥çš„é€‰æ‹©ä½å›¾ï¼ˆç¬¬ 1 è¡Œï¼‰ã€‚ä¸ºäº†ç”Ÿæˆå±‚çº§ä½å›¾ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡æŸ¥æ‰¾æ‰€æœ‰è®°å½•çš„ç¬¬ä¸€ä¸ªå±‚çº§ï¼ˆå³æ‰€æœ‰ä¸º 0 çš„é‡å¤çº§åˆ«ï¼ˆ2ï¼‰ï¼‰æ¥ç”Ÿæˆä¸€ä¸ªåä¸º**<u>è®°å½•ä½å›¾</u>**çš„ä½å›¾ï¼ˆç¬¬ 4 è¡Œï¼‰ï¼Œç„¶åä½¿ç”¨ç”Ÿæˆçš„è®°å½•ä½å›¾å°†è¾“å…¥é€‰æ‹©ä½å›¾æ‰©å±•ä¸ºå±‚çº§ä½å›¾ï¼ˆç¬¬ 5 è¡Œï¼‰ã€‚åœ¨ [5.3](#_bookmark27) èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æè¿°ç”¨äºå‰ä¸€æ­¥çš„ä½å¹¶è¡Œè¿ç®—ç¬¦ï¼Œå¹¶åœ¨ [5.4.1](#_bookmark31) èŠ‚ä¸­ä»‹ç»ä¸€ç§åœ¨åä¸€æ­¥ä¸­é‡ç”¨ç°æœ‰è¿ç®—ç¬¦ï¼ˆæ‰©å±•ï¼‰çš„æ–¹æ³•ã€‚ç°åœ¨ï¼Œé€šè¿‡ç”Ÿæˆçš„<u>å·²å¯¹é½åˆ°å„çº§åˆ«çš„å±‚çº§ä½å›¾</u>ï¼Œæˆ‘ä»¬å¯ä»¥é‡ç”¨ [3](#_bookmark6) èŠ‚ä¸­ä»‹ç»çš„é€‰æ‹©è¿ç®—ç¬¦æ¥é€‰æ‹©é‡å¤çº§åˆ«ï¼ˆç¬¬ 6 è¡Œï¼‰å’Œå®šä¹‰çº§åˆ«ï¼ˆç¬¬ 12 è¡Œï¼‰ã€‚
>
> åŒæ ·ï¼Œä¸ºäº†é€‚åº”æ‰€æœ‰ç©ºå€¼åœ¨å­—æ®µå€¼ä¸­å¹¶ä¸å®é™…å­˜å‚¨çš„äº‹å®ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ç”Ÿæˆå€¼ä½å›¾ï¼ˆç¬¬ 7-11 è¡Œï¼‰ã€‚æ ¹æ® 1ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†å®šä¹‰çº§åˆ«ä¸åˆ—çš„æœ€å¤§å®šä¹‰çº§åˆ«è¿›è¡Œæ¯”è¾ƒæ¥æ‰¾åˆ°æ‰€æœ‰ç©ºå€¼ï¼ˆç¬¬ 10 è¡Œï¼‰ã€‚ç„¶åï¼Œä½¿ç”¨ç»“æœä½å›¾ï¼ˆç§°ä¸ºâ€œæœ‰æ•ˆä½å›¾â€ï¼‰æ¥å‹ç¼©è¾“å…¥é€‰æ‹©ä½å›¾ï¼Œæ–¹æ³•æ˜¯åˆ é™¤æ‰€æœ‰ä¸ç©ºå€¼å¯¹åº”çš„ä½ï¼ˆç¬¬ 11 è¡Œï¼‰ã€‚æˆ‘ä»¬å°†åœ¨ [5.4.2](#_bookmark33) èŠ‚ä¸­æ›´è¯¦ç»†åœ°æè¿°æ­¤æ­¥éª¤ã€‚
>
> æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å€¼ä½å›¾ä½œä¸ºé€‰æ‹©ä½å›¾æ¥é€‰æ‹©å­—æ®µå€¼ï¼ˆç¬¬ 13 è¡Œï¼‰ï¼Œå¹¶è¿”å›æ‰€æœ‰é€‰å®šçš„å­—æ®µå€¼ä»¥åŠé‡å¤/å®šä¹‰çº§åˆ«ã€‚è¯·æ³¨æ„ï¼Œåœ¨æœ€å¸¸è§çš„æƒ…å†µä¸‹ï¼Œå¦‚æœè¯¥åˆ—ä»…ä»…æ˜¯å¿…éœ€åˆ—ï¼Œåˆ™æ— éœ€è¿›è¡Œæ‰€æœ‰ä½å›¾è½¬æ¢ï¼Œåªéœ€é€‰æ‹©å­—æ®µå€¼å³å¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒParquet é€‰æ‹©è¿ç®—ç¬¦å°†ç®€åŒ–ä¸ºæˆ‘ä»¬åœ¨ [3](#_bookmark6) èŠ‚ä¸­ä»‹ç»çš„æ ‡å‡†é€‰æ‹©è¿ç®—ç¬¦ã€‚
>
> å€¼å¾—æŒ‡å‡ºçš„æ˜¯ï¼Œæ ¹æ®å®šä¹‰ [1](#_bookmark7)ï¼Œç®—æ³• [4](#_bookmark25) ä¸­ä½¿ç”¨çš„æ‰€æœ‰è¿ç®—ç¬¦éƒ½æ˜¯**ä½å¹¶è¡Œç®—æ³•**ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰è¿ç®—ç¬¦éƒ½ä¾èµ–äº PDEP æˆ– PEXT æŒ‡ä»¤æ¥å®ç°å¤„ç†å™¨å­—ä¸­çš„**å®Œå…¨æ•°æ®å¹¶è¡Œæ€§**ã€‚
>

### 5.3 Predicate Pushdown on Levels

We describe a bit-parallel equal operator to compare a sequence of bit-packed values to a constant value and output an equality bitmap. It operates on encoded values directly and thus evaluates all values packed in a processor word in parallel. We use this operator to find: 1) all definition levels that are equal to the maximum definition level ( 1 ); and 2) all repetition levels that are 0s ( 2 ). In Parquet, repetition and definition levels are small integer values that are typically encoded with no more than a few bits. Consequently, applying this operator on levels is remarkably efficient due to the higher degree of data parallelism.

Algorithm [5](#_bookmark28) shows the steps to perform bit-parallel comparisons. For ğ‘˜-bit levels, the first step is to duplicate the literal value to all *ğ‘˜*-bit fields. In the next step, we use a formula adapted from [[27](#_bookmark72)] to compare all ğ‘˜-bit values simultaneously. The results are stored in the most significant bit of each ğ‘˜-bit field: 1 means that the two values are the same. These result bits are then extracted by using the PEXT instruction to generate a compact bitmap representation.

> æˆ‘ä»¬æè¿°äº†ä¸€ä¸ª**ä½å¹¶è¡Œç›¸ç­‰è¿ç®—ç¬¦**ï¼Œç”¨äºå°†ä¸€ç»„ä½å‹ç¼©å€¼ï¼ˆbit-packed valuesï¼‰ä¸ä¸€ä¸ªå¸¸é‡å€¼è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¾“å‡ºä¸€ä¸ª**ç›¸ç­‰ä½å›¾**ã€‚å®ƒç›´æ¥åœ¨ç¼–ç å€¼ä¸Šæ“ä½œï¼Œå› æ­¤å¯ä»¥å¹¶è¡Œè¯„ä¼°å¤„ç†å™¨å­—ä¸­æ‰“åŒ…çš„æ‰€æœ‰å€¼ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤è¿ç®—ç¬¦æ¥æŸ¥æ‰¾ï¼š1) æ‰€æœ‰ç­‰äºæœ€å¤§å®šä¹‰çº§åˆ«çš„å®šä¹‰çº§åˆ«ï¼ˆäº‹å®â‘ ï¼‰ï¼›ä»¥åŠ 2) æ‰€æœ‰ä¸º 0 çš„é‡å¤çº§åˆ«ï¼ˆäº‹å®â‘¡ï¼‰ã€‚åœ¨ Parquet ä¸­ï¼Œé‡å¤çº§åˆ«å’Œå®šä¹‰çº§åˆ«æ˜¯è¾ƒå°çš„æ•´æ•°å€¼ï¼Œé€šå¸¸ä»…ä½¿ç”¨å‡ ä½è¿›è¡Œç¼–ç ã€‚å› æ­¤ï¼Œç”±äºæ•°æ®å¹¶è¡Œåº¦è¾ƒé«˜ï¼Œå°†æ­¤è¿ç®—ç¬¦åº”ç”¨äºçº§åˆ«éå¸¸é«˜æ•ˆã€‚
>
> ç®—æ³• [5](#_bookmark28) å±•ç¤ºäº†æ‰§è¡Œä½å¹¶è¡Œæ¯”è¾ƒçš„æ­¥éª¤ã€‚å¯¹äº **ğ‘˜** ä½çº§åˆ«ï¼Œç¬¬ä¸€æ­¥æ˜¯å°†å­—é¢å€¼å¤åˆ¶åˆ°æ‰€æœ‰ **ğ‘˜** ä½å­—æ®µã€‚ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬ä½¿ç”¨æ”¹ç¼–è‡ª [[27](#_bookmark72)] çš„å…¬å¼åŒæ—¶æ¯”è¾ƒæ‰€æœ‰ **ğ‘˜** ä½å€¼ã€‚ç»“æœå­˜å‚¨åœ¨æ¯ä¸ª **ğ‘˜** ä½å­—æ®µçš„æœ€é«˜æœ‰æ•ˆä½ä¸­ï¼š1 è¡¨ç¤ºä¸¤ä¸ªå€¼ç›¸åŒã€‚ç„¶åä½¿ç”¨ PEXT æŒ‡ä»¤æå–è¿™äº›ç»“æœä½ï¼Œä»¥ç”Ÿæˆç´§å‡‘çš„ä½å›¾è¡¨ç¤ºã€‚

<a id="_bookmark28"></a>
![image-20250602161523250](./image/algo5.png)

Figure [9](#_bookmark29) shows an example of comparing the first 16 2-bit definition levels in the running example to the maximum definition level (2). The results are the first 16 bits of the valid bitmap `ğ‘ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘` , which indicates non-null values in the column.

> å›¾ [9](#_bookmark29) å±•ç¤ºäº†å°†æœ¬èŠ‚ç¤ºä¾‹ä¸­çš„å‰ 16 ä¸ª 2 ä½å®šä¹‰çº§åˆ«ä¸æœ€å¤§å®šä¹‰çº§åˆ« (2) è¿›è¡Œæ¯”è¾ƒçš„ç¤ºä¾‹ã€‚ç»“æœæ˜¯æœ‰æ•ˆä½å›¾ `ğ‘ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘` çš„å‰ 16 ä½ï¼Œè¡¨ç¤ºè¯¥åˆ—ä¸­çš„å€¼ä¸ä¸ºç©ºã€‚

<a id="_bookmark29"></a>
![image-20250602161835839](./image/09.png)
Fig. 9. Equality comparisons on 16 2-bit definition levels

### 5.4 Transforming the Select Bitmap

1.  *Select Bitmap to Level Bitmap.* The upper part of Figure [8](#_bookmark26) illustrates the transformation from the select bitmap to the level bitmap. For each bit in the select bitmap, we need to duplicate it as many times as the number of values in the corresponding record. Interestingly, this transformation

can be implemented by using the extend operator (Algorithm [2)](#_bookmark11) we introduced in Section [3.2.](#_bookmark8) In the select operator, for *ğ‘˜*-bit values, we use the extend operator to copy each bit in the bitmap to *ğ‘˜* bits by using a specific predefined bitmap 0*ğ‘˜* âˆ’11*...*0*ğ‘˜* âˆ’11 as the mask. In general, the extend operator duplicates the *ğ‘–*-th bit in the input *ğ‘˜* times, where *ğ‘˜* denotes the distance between the *ğ‘–*-th 1 and

![](media/c5d815c89f49499e6f05696ae04850e3.png)*ğ‘–* 1 -th 1 in the mask bitmap. By performing the equality comparisons between the repetition levels and 0s (Section [5.3),](#_bookmark27) we have generated the record bitmap where each 1-bit represents the first value in each record (according to 2 ). Thus, the distance between each pair of adjacent 1s represents the number of values in the corresponding record. Based on this finding, we now can extend the select bitmap by using the record bitmap as the mask of the extend operator, duplicating each bit *ğ‘˜* times where *ğ‘˜* is the number of values in the corresponding record. The result bitmap is the level bitmap. Figure [10](#_bookmark32) demonstrates the steps of this transformation for the running example. In this figure, we alternate background color to distinguish values or bits in different records.

**input** b*ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡* : 010000010001100000100001

b*ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘‘* : 10111111111100011110111110011101

low = PDEP(b*ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡* , b*ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘‘* ): 00100000100000011000000100000001

high = PDEP(b*ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡* , b*ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘‘* -1): 10000001000100010000001000000100

**output** b*ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™*  = high low: 01100000100011111000000100000011

Fig. 10. Transforming a select bitmap to a level bitmap

1.  *Level Bitmap to Value Bitmap.* As described earlier, Parquet does not explicitly store null values in the field values. Consequently, to produce a bitmap that can be used to select field values, we need to extract all bits from the level bitmap that corresponds to non-null values. This transformation is illustrated in the lower part of Figure [8.](#_bookmark26) As we have generated the valid bitmap by comparing the definition levels (Section [5.3),](#_bookmark27) this transformation can be simply implemented by applying PEXT on the level bitmap with the use of the valid bitmap as the mask (similar to Section [4.3).](#_bookmark18)

    Figure [11](#_bookmark34) demonstrates that a single PEXT instruction transforms the level bitmap *ğ‘ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™* to the value bitmap *ğ‘ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’* , removing all bits in *ğ‘ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™* that correspond to 0s in *ğ‘ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘* . The produced *ğ‘ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’* is then be used to select the non-null values from the example column.

**input** b*ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™* : 01100000100011111000000100000011

b*ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘* : 01100001000111110001100101110011

**output**  b*ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’*  = PEXT(b*ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™* , b*ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘* ): 1100111100100011

Fig. 11. Transforming a level bitmap to a value bitmap

## 6 PARQUET-SELECT

We built a library, called Parquet-Select, that enables predicate pushdown in Apache Parquet and is the full implementation of the techniques presented in this paper. Parquet-Select makes no changes to the Parquet format and can, therefore, read any file conforming to the format specification. It is designed to support *arbitrary* filters: each filter is a user-defined lambda function and can be used to implement even the most complex predicates such as complex string matching, UDFs, or cross-table predicates (e.g., [[32](#_bookmark78)]). For a given set of filters, Parquet-Select returns only the values in the matching records, as if we use the standard Parquet library to read a Parquet file that contains the matching records only.

Parquet-Select supports all data types available in Parquet, i.e., boolean, int32, int64, int96, float, double, byte array, and fixed-length byte array. It also inherits the data model from Parquet, supporting arbitrary nesting of required, optional, and repeated fields. However, in our current implementation, the fields referenced in filters must be required or optional fields. Supporting filters on repeated fields is part of our plan for future work.

The techniques we presented in previous sections focus on the dictionary/bit-packing encoding. RLE is another common encoding that is particularly favorable for encoding levels, which likely have long runs of identical values (e.g., 0s). Additionally, plain encoding is often used as the fallback option for the dictionary encoding. In Parquet-Select, we also implemented selection pushdown for RLE and plain encodings by leveraging bitwise operators. **These bitwise operators do not require the use of BMI and are consequently beyond the scope of this paper**. Therefore, we briefly outline the approach: for an RLE run(ğ‘£, ğ‘˜ ) where *ğ‘£* denotes the value and *ğ‘˜* denotes how many times the value appears, to select values from it based on a select bitmap, we need to populate the value *ğ‘£* as many times as the number of selected values in the run; this number can be calculated by using the POPCNT instruction to count the number of 1s in the *ğ‘˜* corresponding bits in the select bitmap. POPCNT is not part of BMI and is available on most CPU architectures including X86 and ARM.

We implemented Parquet-Select in C++, based on the open-source C++ version of the Arrow/Parquet library^1^ (version 8.0.0). For all operators that use BMI instructions, we implemented a dynamic dispatching mechanism to detect whether BMI support is available on the host processors at run time and fall back to an alternative implementation if BMI support is missing.

## 7 EVALUATION

We ran our experiments on a server with a 2.6GHz AMD EPYC 7413 processor, and 256GB of DDR4 memory. We used an NVMe SSD as the storage device, with a maximum I/O throughput of 3.5GB/s. The machine runs 64-bit Windows Server 2022 Datacenter operating system. We also ran our experiments on another server with Intel Xeon Gold 6140 CPUs. The results are similar to that with the AMD processor and are therefore omitted in this paper.

The configuration of the server is similar to a compute node running analytical data systems for data warehouse or data lake applications [[45](#_bookmark90)]. These systems typically cache data read from cloud object stores (e.g., Azure Storage and AWS S3) in local SSDs using techniques such as Databricks Cache [[30](#_bookmark76)], Snowflake Cache Layer [[21](#_bookmark66)], or Crystal cache [[22](#_bookmark67)]. Below, we assume that all data files are stored in local SSDs.

In the evaluation below, we compare Parquet-Select to the open-source C++ version of Arrow/Parquet^2^ (version 8.0.0). Parquet implements the SIMD-based unpack algorithm [[41](#_bookmark86)] for decoding. To compare with Parquet-Select, we also implemented the filter and selection operations on decoded column values using SIMD instructions and optimized these operations with our best efforts. Unless explicitly stated otherwise, all Parquet files used in our evaluation are generated with default values from Parquet for all configuration parameters, e.g., row group size = 64MB, data page size = 1MB, maximum dictionary size = 1MB, no page-level compression. We ran all experiments using a single thread.

> 1. The Parquet C++ library has been integrated into the Apache Arrow library.
> 2. <https://github.com/apache/arrow>

### 7.1 Evaluating Selection Performance

In the first experiment, we compare the performance of Parquet-Select and Parquet on a project operation with a given select bitmap, and evaluate how this comparison is affected by the characteristics of the operation, including the bit width of the column values and the selectivity of the select bitmap. To this end, we created a Parquet file with a single column containing 128 million 64-bit integer values. We ensure that there are only 2*ğ‘˜* distinct values in the column, where *ğ‘˜* is the bit width of the values encoded with dictionary encoding and is varying in this experiment. For Parquet, all encoded values are first unpacked to 64-bit integers and then selected into an output buffer based on the select bitmap, while Parquet-Select selects encoded values directly followed by unpacking the selected values. No evaluate or transform operators are involved in this experiment.

> - [ ]  Fig. 12. Parquet vs. Parquet-Select: selection operation

Figure [12](#_bookmark39) plots the speedup of Parquet-Select over Parquet by varying the selectivity and the bit width of column values. Parquet-Select outperforms Parquet under all circumstances, with up to 10X performance gains. As expected, Parquet-Select performs better as the selectivity increases. In the extreme case where we select all values (selectivity = 1/1), both methods have similar performance, as they have to unpack the same amount of values. The only difference is in the select operator: Parquet-Select is slightly faster than Parquet with smaller values because the selection on bit-packed values reads less amount of memory compared to the selection on decoded 64-bit values. As the selectivity increases, the speedup of Parquet-Select dramatically improves for the obvious reason that, in addition to the performance gain on the select operator, Parquet-Select also reduces the number of values that need to be unpacked. These results confirm that the unpack operator, though it has been highly optimized with SIMD vectorization [[41](#_bookmark86)], still acts as a performance bottleneck since it has a lower degree of data parallelism due to the size of decoded values (64 bits).

The figure also shows that Parquet-Select achieves higher performance gains as the bit width decreases. This is due to the fact that the select operator used in Parquet-Select can process all values packed in a processor word in parallel (see Section [3](#_bookmark6)) and thus achieves a higher degree of data parallelism with smaller values. It is therefore more efficient than either the unpack or select operators on 64-bit decoded values where the available data parallelism is limited by the size of decoded values (64 bits). This effect is more pronounced when the selectivity is relatively low and the unpack operator in Parquet-Select does not dominate the overall execution time.

### 7.2 Micro-Benchmark Evaluation

We next evaluate the two approaches with scan queries using a micro-benchmark. In this experiment, we generated a Parquet table R with 20 columns (a1 âˆ¼ a20) and 128 million rows. A scan query, shown below, is used to evaluate how the performance comparison is affected by the characteristics of a scan query, including the number of filters, the number of projections, the bit width of column values, and the data types. The selectivity of each filter is fixed at 25%, but the overall selectivity of the query is varied as we vary the number of filters. By default, we use the settings shown as follows: \#filters = 2 (overall selectivity = 6.25%), \#projections = 3, bit width = 5, data type = int64.

```sql
SELECT MAX(a10), MAX(a11), ... FROM R WHERE a1 < C1 AND a2 < C2 AND ...
```
Figure [13a](#_bookmark40) compares the performance of Parquet-Select and Parquet by varying the bit width from 1 to 16. Similar to the results in the previous experiment, Parquet-Select achieves higher speedup with smaller column values. Interestingly, we observe that the execution time saved by Parquet-Select nearly remains the same regardless of the bit width. Since the gap between the two lines is mainly due to the additional unpacking cost on unselected values in Parquet, this further confirms that the unpacking cost is largely independent of the bit width of the input values due to the fixed size of decoded values (64 bits).

> - [ ] Fig. 13. Parquet vs. Parquet-Select: scan micro-benchmark

As shown in Figure [13b](#_bookmark40) and [13c,](#_bookmark40) Parquet-Select is more effective when the scan query has more projection or filter columns. This is because, as we evaluate the filters successively, the selectivity on each column continues to increase, resulting in higher performance gains of Parquet-Select on projection columns or filter columns evaluated later in the query. As a result, Parquet-Select performs better when the query contains a long list of projection or filter columns. In addition to this effect, adding more filters also further reduces the overall selectivity, which leads to even higher speedup compared to adding projections (6.0X vs. 4.7X).

Figure [13d](#_bookmark40) shows the query time of the two approaches with various data types including int32, int64, int96, float, double, and byte array. The performance improvement is similar (3.0-3.6X) with all data types except for byte array. The relatively lower speedup (2.5X) with byte array is mainly because the more expensive predicates on byte array values particularly slow down the first filter, where Parquet-Select has to select and evaluate all values, thereby hindering the overall performance gain over Parquet. The costly data types, however, may also result in higher improvement if there are more filter or projection columns. To show this effect, Figure [13e](#_bookmark40) plots the performance with byte array values by varying the number of filters. Compared to the results with int64 values (Figure [13b),](#_bookmark40) Parquet-Select achieves lower speedup when the query has only 1 filter, but becomes more effective as the number of filters increases.

### 7.3 Case Study: TPC-H Benchmark Q6

To understand the performance of Parquet-Select on a more realistic workload, in the following experiments, we use the TPC-H benchmark [[13](#_bookmark58)] at the scale factor of 10. The total size of the dataset is approximately 10GB. The largest table Lineitem contains 60 million rows. Before a more complete evaluation with join queries, we first conduct a case study using a representative scan query â€“ Q6 in the TPC-H benchmark.

<a id="_bookmark41"></a>

|     ![Figure 1](./image/14.png)     |
| :------------------------------------: |
| Fig. 14. Parquet vs. Parquet-Select: TPC-H benchmark Q6 |


Q6 is a scan query that contains three filters on three different columns l_shipdate, l_discount, and l_quantity, and two projections on columns l_extendedprice and l_discount. The three filter columns are encoded with 12 bits, 4 bits, and 6 bits in Parquet, respectively. The selectivity of the three filters is 15.2%, 27.3%, and 46.0%, respectively. The overall selectivity of the query is 1.9%.

7.3.1 **Non-nullable Columns**. As per the specification of the TPC-H benchmark, all column values are not nullable. We first ran an experiment with this setting. Figure [14a](#_bookmark41) shows the results of Q6 with three I/O settings: the Parquet file is either preloaded into main memory or loaded from disks using asynchronous or synchronous I/O. As can be seen from the figure, Parquet-Select outperforms Parquet by a factor of 3 when the file has been preloaded into memory. Interestingly, with the use of asynchronous I/O, both Parquet-Select and Parquet maintain nearly the same query time even though data is now loaded from disks. **This is due to the fact that reading Parquet columns is CPU bound, making the asynchronous I/O operations fully overlap with the CPU-intensive operations such as select and unpack operators**. This can be further confirmed by the results with synchronous I/O: the time difference between Preloaded and Sync I/O indicates the I/O time, which is clearly less than the computation time for both approaches. For storage devices with limited I/O speed (e.g., magnetic disks or remote storage), the time difference between Parquet and Parquet-Select should remain unchanged, while the speedup may be reduced accordingly.

7.3.2 **Nullable Columns**. To evaluate the performance with nullable column values, we modified the Lineitem table by altering 12.5% of values in each column to nulls. Figure [14b](#_bookmark41) shows the query time of Q6 on this modified dataset. For Parquet, we observe a significant performance degradation due to the added null values, with the query time up from 0.5s to 2.2s. The reasons for this dramatic difference are twofold. First, in addition to column values, Parquet has to decode the definition levels and evaluate a predicate on these decoded levels to find all null values. Second, the select bitmap needs to be transformed in order to be used for selecting non-null values. In contrast, Parquet-Select performs the predicate evaluation on encoded levels directly (Section [5.3](#_bookmark27)) and leverages BMI instructions to transform the select bitmap in a bit-parallel manner (Section [5.4](#_bookmark30)). With these techniques, it adds only marginal cost in Parquet-Select to handle nullable columns, as shown by comparing the query time in Figure [14a](#_bookmark41) and [14b.](#_bookmark41) For the above reasons, Parquet-Select is 13.7X faster than Parquet with nullable values.

7.3.3 **Repeated Columns**. Next, we added two extra columns with repeated values to the Lineitem table. In these two repeated columns, each record contains a variable number of values, following a uniform distribution between 0 and 8 with an average of 4 values per record. We further modified Q6 to replace the two projected columns (l_discount and l_extendedprice) accessed in the aggregation with the two repeated columns. All filters and their selectivity in this modified query remain unchanged. Figure [14c](#_bookmark41) shows the query time of this modified Q6 with repeated columns. Unsurprisingly, Parquet-Select achieves even higher speedup (around 20X) over Parquet. Selecting a repeated column requires two additional steps: 1) reading and selecting repetition levels, which are encoded and stored in the same way as the column values; and 2) transforming the select bitmap according to the repetition levels. With the use of BMI, the techniques proposed in this work substantially improve both the read/select step (Section [3](#_bookmark6)) and the bitmap transformation step (Section [5.4.1](#_bookmark31)), resulting in over 20X reduction in the cost of processing repeated columns. In light of these results, we envision that Parquet-Select could potentially encourage the broader use of complex structured data in analytical data systems.

<a id="_bookmark42"></a>

|     ![Figure 1](./image/15.png)     |
| :------------------------------------: |
| Fig. 15. Impact of filter ordering on TPC-H Q6 |

7.3.4 **Filter Ordering**. In the next experiment, we study the impact of filter ordering on the performance of Parquet-Select. For this purpose, we manually changed the order of filter evaluation and tested all six different orders of the three filters in Q6. The results are shown in Figure [15](#_bookmark42). For this query, the best order \<l_shipdate, l_discount, l_quantity\> is up to 30% faster than other orders. As can be seen from the figure, the most critical choice is to evaluate the filter on l_shipdate first. One reason is that this filter is more selective (15.2%) than other filters and thus evaluating this filter earlier is beneficial for subsequent filters. Second, the column l_shipdate has the largest values (12 bits), making it less beneficial to evaluate it later in the plan even with a more selective bitmap. Our order selection algorithm precisely captures these effects (see Section [4.4](#_bookmark20)) and successively picks the best order for this query.

### 7.4 TPC-H Benchmark Evaluation

We next extend our evaluation with the TPC-H benchmark by using Apache Spark [[17](#_bookmark62), [44](#_bookmark89)] (version 3.3.0). We study two approaches: 1) Spark + Parquet, which is the standard and deeply integrated way to query Parquet files using Spark; and 2) Spark + Parquet-Select, which is a way to enable predicate pushdown in Spark by leveraging Parquet-Select. To connect Parquet-Select to Spark as a new data source, we built a connector by implementing the Spark Data Source V2 API. With this approach, all records in Parquet files are filtered using the pushdown predicates in the ParquetSelect library and only the matching ones are fed into Spark for the subsequent processing such as joins and aggregates. Although the Data Source V2 API also allows aggregate pushdown, we did not implement it as it is orthogonal to this work. The connector is implemented in Scala/Java and interacts with the C++ Parquet-Select library through JNI (Java Native Interface).

In addition to Q6, we select 9 join queries from the TPC-H benchmark. All these queries, except for Q19, have a conjunction predicate consisting of at least 1 filter on the Lineitem table. Q19 has a more complex predicate that is a disjunction of three predicates, each of which is a conjunction of filters. The selectivity on the Lineitem table of these queries is summarized in the table below:

> - [ ]  Fig. 16. Spark+Parquet vs. Spark+Parquet-Select: TPC-H
> - [ ] è¡¨æ ¼

Figure [16](#_bookmark43) shows the running time of the two solutions with the 10 TPC-H queries. Spark+ParquetSelect outperforms Spark+Parquet for all queries, with speedup ranging from 1.1X to 5.5X. The performance gains are highly correlated with the selectivity on the Lineitem table. In particular, it achieves higher improvement (5.0X, 3.1X, 2.0X, 3.7X, 5.5X) for queries that have highly selective predicates (Q6, Q12, Q14, Q15, Q19). This is because: 1) Parquet-Select is more effective with selective predicates; 2) the subsequent joins are performed on a smaller portion of the Lineitem table and thus have an insignificant impact on speedup achieved by Parquet-Select. For the rest of the queries, the join operations are more expensive and dominate the overall query time, limiting the performance gains of Parquet-Select (1.1X 1.3X). It is worth pointing out that these queries do have highly selective filters on the other side of the joins the dimension tables. This presents an opportunity to further push down the cross-table predicates, e.g., data-induced predicates [[32](#_bookmark78)], which is an interesting direction for future work.

We also compare the query time of Q6 in Spark to that of the standalone implementation shown in Figure [14a.](#_bookmark41) Compared to the standalone implementation where the data produced by either Parquet-Select or Parquet is processed in a hard-coded tight loop, Spark imposes a considerable overhead to process the produced data for the obvious reason that the data must be fed into Spark engine, meaning that it must be reformatted to the internal format and be deeply copied into a buffer managed by Spark. Parquet-Select mitigates this overhead by feeding only the selected values to Spark, resulting in higher speedup (5.0X) in Spark than in the standalone implementation (3.1X). This indicates that the performance gains of Parquet-Select in real-world use cases could be even higher than what we demonstrated in previous experiments.

### 7.5 Impact of Encoding and Compression

7.5.1 **RLE encoding**. Thus far, we have evaluated selection pushdown for bit-packed values. In the next experiment, we investigate the efficiency of selection pushdown for RLE-encoded data. Figure [17a](#_bookmark44) plots the speedup of Parquet-Select over Parquet by varying the selectivity of the select operation. Parquet-Select achieves up to 9X speedup over Parquet. <u>Even with all values selected, Parquet-Select remains 1.8X faster than Parquet, **as it decodes and selects values in a single pass**, **while Parquet requires two separate passes to complete these steps**</u>. As seen in comparison to the results shown in Figure [12,](#_bookmark39) selection pushdown is generally more efficient for RLE-encoded values than for bit-packed values.

> - [ ]  Fig. 18. Impact of page-level compression on TPC-H Q6

Next, we study scenarios with interleaved bit-packing and RLE runs within the same column. Figure [17b](#_bookmark44) shows the results as the percentage of values encoded by RLE and bit-packing varies. On average, both RLE and bit-packing runs contain 100 values. The bit-packed values are encoded using 8 bits per value. As illustrated in the figure, Parquet-Select performs the best when the column uses either RLE or bit-packing only. Its performance declines when Parquet-Select has to frequently switch between processing RLE and bit-packing runs, due to the overhead associated with parsing run metadata.

7.5.2 **Page-level compression**. In Parquet, encoded values are organized into data pages, which can be further compressed optionally using a variety of compression schemes. In our final experiment, we studied the impact of page-level compression on the performance comparison between Parquet and Parquet-Select using Q6 from the TPC-H benchmark. We chose two widely-used and fast compression schemes, LZ4 and Snappy, which both reduced the size of the Lineitem Parquet file by approximately 30%.

Figure [18a](#_bookmark45) and [18b](#_bookmark45) compare the query time of Parquet and Parquet-Select with various compression schemes in the standalone implementation and Spark, respectively. First, we observe that both Parquet and Parquet-Select suffer from the use of page-level compression, as the decompression cost outweights the saved I/O cost on NVMe SSDs. Second, as the decompression time is independent of the two approaches, the time difference between Parquet and Parquet-Select remains constant, but the speedup of Parquet-Select over Parquet is reduced when the Parquet file is compressed. These findings suggest that, to fully benefit from BMI, data files on fast storage devices should be stored in an uncompressed form. This conclusion is in line with recent work [[45](#_bookmark90)] from Databricks, which recommends caching uncompressed Parquet files in compute nodes after reading and decompressing them from remote storage.

## 8 RELATED WORK

We categorize the related work into the following broad lines of research: predicate pushdown and column-oriented storage.

**Predicate pushdown.** Predicate pushdown is a well-known technique that has been widely used in various use scenarios in Big Data, data warehouse, and data lake applications. In cloud, cloud object stores such as AWS S3 [[6](#_bookmark51)] and Azure Storage [[8](#_bookmark53), [20](#_bookmark65)] recently are offering predicate pushdown as cloud services, e.g., AWS S3 Select [[7](#_bookmark52)] and Azure ADLS query acceleration [[9](#_bookmark54)], to reduce the amount of data that needs to be transferred to compute nodes. Such techniques have been explored for database applications [[42](#_bookmark87), [43](#_bookmark88)].

Many techniques have also been developed to explore the idea of predicate pushdown for a variety of data formats, motivated by the predicate pushdown capability in the input adapter of database engines, e.g., the Spark data source API. For example, Mison [[28](#_bookmark75)] is a JSON parser that is capable of pushing down filters and projections to the parser, avoiding expensive parsing on unnecessary fields. Sparser [[33](#_bookmark79)] takes one step further and applies filters on textual formats even before parsing. Another area for leveraging predicate pushdown is in the context of data caching. Recent work [[22](#_bookmark67)] proposes to cache only parts of a table that match the predicates to improve query performance and cache utilization. We complement this line of work by enabling predicate pushdown in columnar data formats such as Parquet.

**Column-oriented storage.** Two influential pioneers in column stores are C-Store [[39](#_bookmark84)] and MonetDB [[19](#_bookmark64), [24](#_bookmark69)]. The compression efficiency of common encoding schemes in column stores has been studied in [[14](#_bookmark59)]. SIMD-Scan [[40](#_bookmark85), [41](#_bookmark86)] is a practical method to optimize the decoding process using SIMD vectorization [[34](#_bookmark80)â€“[36](#_bookmark81)]. This method has been adopted in Parquet, and serves as the baseline in our evaluation. Parquet-Select uses SIMD-Scan to decode encoded values as well, but unlike Parquet, it pushes down the select operator to minimize the number of values that need to be decoded.

There has been a large body of work on parallel predicate evaluation on encoded values [[23](#_bookmark68), [25](#_bookmark70), [26](#_bookmark71), [29](#_bookmark74), [40](#_bookmark85)]. All these techniques are based on two assumptions: 1) the encoding is order-preserving; 2) the predicates are simple enough such that they can be performed on the encoded values directly (e.g., basic comparison operators). The techniques proposed in this paper are complementary to these techniques, especially for the cases where either of the two assumptions does not hold. Parquet is such an example, since the dictionary encoding used in Parquet is not order-preserving. However, in Parquet-Select, we find one specific use case for these techniques: filtering levels (Section [5.3](#_bookmark27)), because all levels are small integers that are encoded with bit-packing encoding directly and are always evaluated with pre-defined equality predicates.

## 9 CONCLUSIONS

With the increasing demand for data analytics, there is a critical need for efficient and generic predicate pushdown in column stores. This paper presents techniques that address this need by leveraging BMI instructions. At the core of our approach is a BMI-based select operator and a framework that enables selection pushdown for arbitrary scan queries. Our experimental studies in the context of Apache Parquet demonstrate that the proposed techniques substantially improve query performance across a wide spectrum of queries.

## 10 REFERENCES

1.  Retrieved Oct 2022. *Amazon Athena*. <https://aws.amazon.com/athena/>
2.  Retrieved Oct 2022. *Apache Arrow*. <https://arrow.apache.org/>
3.  Retrieved Oct 2022. *Apache ORC*. <https://orc.apache.org/>
4.  Retrieved Oct 2022. *Apache Parquet*. <https://parquet.apache.org/>
1.  Retrieved Oct 2022. *AWS Graviton Processor*. <https://aws.amazon.com/ec2/graviton/>
2.  Retrieved Oct 2022. *AWS S3*. <https://aws.amazon.com/s3/>
3.  Retrieved Oct 2022. *AWS S3 Select*. <https://aws.amazon.com/blogs/aws/s3-glacier-select/>
4.  Retrieved Oct 2022. *Azure Blob Storage*. <https://azure.microsoft.com/en-us/products/storage/blobs/>
5.  Retrieved Oct 2022. *Azure Data Lake Storage Query Acceleration*. [https://learn.microsoft.com/en-us/azure/storage/](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration) [blobs/data-lake-storage-query-acceleration](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration)
6.  Retrieved Oct 2022. *Azure Synapse Analytics*. <https://azure.microsoft.com/en-us/products/synapse-analytics/>
7.  Retrieved Oct 2022. *Nvidia Grace CPU*. <https://www.nvidia.com/en-us/data-center/grace-cpu/>
8.  Retrieved Oct 2022. *Protocol Buffers: Googleâ€™s data interchange format*. <https://developers.google.com/protocol-buffers/>
9.  Retrieved Oct 2022. *TPC Benchmark H Standard Specification. Revision 2.17.1*. [https://www.tpc.org](https://www.tpc.org/)
10. Daniel J. Abadi, Samuel Madden, and Miguel Ferreira. 2006. Integrating compression and execution in column-oriented database systems. In *SIGMOD*. ACM, 671â€“682. <https://doi.org/10.1145/1142473.1142548>
11. Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and Marios Skounakis. 2001. Weaving Relations for Cache Performance. In *VLDB*. Morgan Kaufmann, 169â€“180. <http://www.vldb.org/conf/2001/P169.pdf>
12. Michael Armbrust, Tathagata Das, Sameer Paranjpye, Reynold Xin, Shixiong Zhu, Ali Ghodsi, Burak Yavuz, Mukul Murthy, Joseph Torres, Liwen Sun, Peter A. Boncz, Mostafa Mokhtar, Herman Van Hovell, Adrian Ionescu, Alicja Luszczak, Michal Switakowski, Takuya Ueshin, Xiao Li, Michal Szafranski, Pieter Senster, and Matei Zaharia. 2020. Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores. *Proc. VLDB Endow.* 13, 12 (2020), 3411â€“3424. <http://www.vldb.org/pvldb/vol13/p3411-armbrust.pdf>
13. Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, and Matei Zaharia. 2015. Spark SQL: Relational Data Processing in Spark. In *SIGMOD*.
14. Alexander Behm, Shoumik Palkar, Utkarsh Agarwal, Timothy Armstrong, David Cashman, Ankur Dave, Todd Greenstein, Shant Hovsepian, Ryan Johnson, Arvind Sai Krishnan, Paul Leventis, Ala Luszczak, Prashanth Menon, Mostafa Mokhtar, Gene Pang, Sameer Paranjpye, Greg Rahn, Bart Samwel, Tom van Bussel, Herman Van Hovell, Maryann Xue, Reynold Xin, and Matei Zaharia. 2022. Photon: A Fast Query Engine for Lakehouse Systems. In *SIGMOD â€™22: International Conference on Management of Data, Philadelphia, PA, USA, June 12 17, 2022*. ACM, 2326â€“2339. <https://doi.org/10.1145/3514221.3526054>
15. Peter A. Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: Hyper-Pipelining Query Execution. In *CIDR 2005, Second Biennial Conference on Innovative Data Systems Research, Asilomar, CA, USA, January 4-7, 2005, Online Proceedings*. [www.cidrdb.org,](http://www.cidrdb.org/) 225â€“237. <http://cidrdb.org/cidr2005/papers/P19.pdf>
16. Brad Calder, Ju Wang, Aaron Ogus, Niranjan Nilakantan, Arild Skjolsvold, Sam McKelvie, Yikang Xu, Shashwat Srivastav, Jiesheng Wu, Huseyin Simitci, Jaidev Haridas, Chakravarthy Uddaraju, Hemal Khatri, Andrew Edwards, Vaman Bedekar, Shane Mainali, Rafay Abbasi, Arpit Agarwal, Mian Fahim ul Haq, Muhammad Ikram ul Haq, Deepali Bhardwaj, Sowmya Dayanand, Anitha Adusumilli, Marvin McNett, Sriram Sankaran, Kavitha Manivannan, and Leonidas Rigas. 2011. Windows Azure Storage: a highly available cloud storage service with strong consistency. In *Proceedings of the 23rd ACM Symposium on Operating Systems Principles 2011, SOSP 2011, Cascais, Portugal, October 23-26, 2011*. ACM, 143â€“157. <https://doi.org/10.1145/2043556.2043571>
17. BenoÃ®t Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven Pelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unterbrunner. 2016. The Snowflake Elastic Data Warehouse. In *Proceedings of the 2016 International Conference on Management of Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 July 01, 2016*. ACM, 215â€“226. <https://doi.org/10.1145/2882903.2903741>
18. Dominik Durner, Badrish Chandramouli, and Yinan Li. 2021. Crystal: A Unified Cache Storage System for Analytical Databases. *Proc. VLDB Endow.* 14, 11 (2021), 2432â€“2444. <https://doi.org/10.14778/3476249.3476292>
23. <a id="_bookmark68"></a>Ziqiang Feng, Eric Lo, Ben Kao, and Wenjian Xu. 2015. ByteSlice: Pushing the Envelop of Main Memory Data Processing with a New Storage Layout. In *Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, Melbourne, Victoria, Australia, May 31 June 4, 2015*. ACM, 31â€“46. <https://doi.org/10.1145/2723372.2747642>

24. Stratos Idreos, Fabian Groffen, Niels Nes, Stefan Manegold, K. Sjoerd Mullender, and Martin L. Kersten. 2012. MonetDB: Two Decades of Research in Column-oriented Database Architectures. *IEEE Data Eng. Bull.* 35, 1 (2012), 40â€“45. <http://sites.computer.org/debull/A12mar/monetdb.pdf>
25. <a id="_bookmark70"></a>Hao Jiang and Aaron J. Elmore. 2018. Boosting data filtering on columnar encoding with SIMD. In *Proceedings of the 14th International Workshop on Data Management on New Hardware, Houston, TX, USA, June 11, 2018*. ACM, 6:1â€“6:10. <https://doi.org/10.1145/3211922.3211932>
26. Ryan Johnson, Vijayshankar Raman, Richard Sidle, and Garret Swart. 2008. Row-wise parallel predicate evaluation.Proc. VLDB Endow. 1, 1 (2008), 622â€“634. <https://doi.org/10.14778/1453856.1453925>

27. Donald E. Knuth. 2009. *The Art of Computer Programming, Volume 4, Fascicle 1: Bitwise Tricks & Techniques; Binary Decision Diagrams* (12th ed.). Addison-Wesley Professional.
28.  Yinan Li, Nikos R. Katsipoulakis, Badrish Chandramouli, Jonathan Goldstein, and Donald Kossmann. 2017. Mison: A Fast JSON Parser for Data Analytics. *PVLDB* 10, 10 (2017), 1118â€“1129.
29.   <a id="_bookmark74"></a>Yinan Li and Jignesh M. Patel. 2013. BitWeaving: fast scans for main memory data processing. In *Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2013, New York, NY, USA, June 22-27, 2013*.    ACM, 289â€“300. <https://doi.org/10.1145/2463676.2465322>

30.  Alicja Luszczak, Michal Szafranski, Michal Switakowski, and Reynold Xin. [n. d.]. *Databricks Cache Boosts Apache Spark Performance*. <https://databricks.com/blog/2018/01/09/databricks-cache-boosts-apache-spark-performance.html>
31.  Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton, and Theo Vassilakis. 2010. Dremel: Interactive Analysis of Web-Scale Datasets. *PVLDB* 3, 1 (2010), 330â€“339.
32.  Laurel J. Orr, Srikanth Kandula, and Surajit Chaudhuri. 2019. Pushing Data-Induced Predicates Through Joins in Big-Data Clusters. *Proc. VLDB Endow.* 13, 3 (2019), 252â€“265. <https://doi.org/10.14778/3368289.3368292>
33.  Shoumik Palkar, Firas Abuzaid, Peter Bailis, and Matei Zaharia. 2018. Filter Before You Parse: Faster Analytics on Raw Data with Sparser. *PVLDB* 11, 11 (2018), 1576â€“1589.
34.  <a id="_bookmark80"></a>Orestis Polychroniou, Arun Raghavan, and Kenneth A. Ross. 2015. [Rethinking SIMD Vectorization for In-Memory Databases](https://doi.org/10.1145/2723372.2747645). In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, Melbourne,Victoria, Australia, May 31 June 4, 2015. ACM, 1493â€“1508.

35.  <a id="_bookmark81"></a>Orestis Polychroniou and Kenneth A. Ross. 2019. Towards Practical Vectorized Analytical Query Engines. In *Proceedings of the 15th International Workshop on Data Management on New Hardware, DaMoN 2019, Amsterdam, The Netherlands, 1 July 2019*, Thomas Neumann and Ken Salem (Eds.). ACM, 10:1â€“10:7. <https://doi.org/10.1145/3329785.3329928>
36.  <a id="_bookmark81"></a>Orestis Polychroniou and Kenneth A. Ross. 2020. VIP: A SIMD vectorized analytical query engine. *VLDB J.* 29, 6 (2020), 1243â€“1261. <https://doi.org/10.1007/s00778-020-00621-w>
37. <a id="_bookmark82 "></a>Viswanath Poosala and Yannis E. Ioannidis. 1997. Selectivity Estimation Without the Attribute Value Independence Assumption. In *VLDBâ€™97, Proceedings of 23rd International Conference on Very Large Data Bases, August 25-29, 1997, Athens, Greece*, Matthias Jarke, Michael J. Carey, Klaus R. Dittrich, Frederick H. Lochovsky, Pericles Loucopoulos, and Manfred A. Jeusfeld (Eds.). Morgan Kaufmann, 486â€“495. <http://www.vldb.org/conf/1997/P486.PDF>
11. Raghav Sethi, Martin Traverso, Dain Sundstrom, David Phillips, Wenlei Xie, Yutian Sun, Nezih Yegitbasi, Haozhun Jin, Eric Hwang, Nileema Shingte, and Christopher Berner. 2019. Presto: SQL on Everything. In *35th IEEE International Conference on Data Engineering, ICDE 2019, Macao, China, April 8-11, 2019*. IEEE, 1802â€“1813. [https://doi.org/10.1109/](https://doi.org/10.1109/ICDE.2019.00196) [ICDE.2019.00196](https://doi.org/10.1109/ICDE.2019.00196)
12. Michael Stonebraker, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, Amerson Lin, Samuel Madden, Elizabeth J. Oâ€™Neil, Patrick E. Oâ€™Neil, Alex Rasin, Nga Tran, and Stanley B. Zdonik. 2005. C-Store: A Column-oriented DBMS. In *VLDB*. ACM, 553â€“564. [http://www.vldb.org/archives/website/2005/program/](http://www.vldb.org/archives/website/2005/program/paper/thu/p553-stonebraker.pdf) [paper/thu/p553-stonebraker.pdf](http://www.vldb.org/archives/website/2005/program/paper/thu/p553-stonebraker.pdf)
40. Thomas Willhalm, Ismail Oukid, Ingo MÃ¼ller, and Franz Faerber. 2013. Vectorizing Database Column Scans with Complex Predicates. In *International Workshop on Accelerating Data Management Systems Using Modern Processor and Storage Architectures ADMS 2013, Riva del Garda, Trento, Italy, August 26, 2013*. 1â€“12. [http://www.adms-](http://www.adms-conf.org/2013/muller_adms13.pdf) [conf.org/2013/muller_adms13.pdf](http://www.adms-conf.org/2013/muller_adms13.pdf)
41. <a id="_bookmark86"></a>Thomas Willhalm, Nicolae Popovici, Yazan Boshmaf, Hasso Plattner, Alexander Zeier, and Jan Schaffner. 2009. SIMDScan: **Ultra Fast in-Memory Table Scan using on-Chip Vector Processing Units**. *Proc. VLDB Endow.* 2, 1 (2009), 385â€“394. <https://doi.org/10.14778/1687627.1687671>
42. Yifei Yang, Matt Youill, Matthew E. Woicik, Yizhou Liu, Xiangyao Yu, Marco Serafini, Ashraf Aboulnaga, and Michael Stonebraker. 2021. FlexPushdownDB: Hybrid Pushdown and Caching in a Cloud DBMS. *Proc. VLDB Endow.* 14, 11 (2021), 2101â€“2113. <https://doi.org/10.14778/3476249.3476265>
43. Xiangyao Yu, Matt Youill, Matthew E. Woicik, Abdurrahman Ghanem, Marco Serafini, Ashraf Aboulnaga, and Michael Stonebraker. 2020. PushdownDB: Accelerating a DBMS Using S3 Computation. In *36th IEEE International Conference on Data Engineering, ICDE 2020, Dallas, TX, USA, April 20-24, 2020*. IEEE, 1802â€“1805. [https://doi.org/10.1109/ICDE48307.](https://doi.org/10.1109/ICDE48307.2020.00174) [2020.00174](https://doi.org/10.1109/ICDE48307.2020.00174)
44. Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauly, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing. In *NSDI*.
45. <a id="_bookmark90"></a>Matei Zaharia, Ali Ghodsi, Reynold Xin, and Michael Armbrust. 2021. **[Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics](https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)**. In *11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event, January 11-15, 2021, Online Proceedings*.