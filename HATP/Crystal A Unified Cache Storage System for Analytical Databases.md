# Crystal: A Unified Cache Storage System for Analytical Databases

> **ABSTRACT** Cloud analytical databases employ a **disaggregated storage model**, where the elastic compute layer accesses data persisted on remote cloud storage in block-oriented columnar formats. Given the high latency and low bandwidth to remote storage and the limited size of fast local storage, caching data at the compute node is important and has resulted in a renewed interest in caching for analytics. Today, each DBMS builds its own caching solution, usually based on file or block-level LRU. In this paper, we advocate a new architecture of a smart cache storage system called *Crystal*, that is co-located with compute. Crystalâ€™s clients are DBMS-specific â€œdata sourcesâ€ with push-down predicates. Similar in spirit to a DBMS, Crystal incorporates query processing and optimization components **==focusing on efficient caching and serving of single-table hyper-rectangles called <u>regions</u>==**. Results show that Crystal, with a small DBMS-specific data source connector, can significantly improve query latencies on unmodified Spark and Greenplum while also saving on bandwidth from remote storage.

**æ‘˜è¦**   äº‘åˆ†ææ•°æ®åº“é‡‡ç”¨**==åˆ†è§£å­˜å‚¨æ¨¡å‹==**ï¼Œå…¶ä¸­å¼¹æ€§è®¡ç®—å±‚è®¿é—®è¿œç¨‹äº‘å­˜å‚¨ä¸ŠæŒä¹…åŒ–çš„æ•°æ®ã€‚è€ƒè™‘åˆ°è¿œç¨‹å­˜å‚¨çš„é«˜å»¶è¿Ÿå’Œä½å¸¦å®½ä»¥åŠæœ‰é™çš„æœ¬åœ°å¿«é€Ÿå­˜å‚¨ï¼Œåœ¨è®¡ç®—èŠ‚ç‚¹ä¸Šç¼“å­˜æ•°æ®éå¸¸é‡è¦ï¼Œ**å¹¶å¯¼è‡´äº†å¯¹ç¼“å­˜è¿›è¡Œåˆ†æçš„æ–°å…´è¶£**ã€‚ä»Šå¤©ï¼Œæ¯ä¸ª DBMS éƒ½æ„å»ºè‡ªå·±çš„ç¼“å­˜è§£å†³æ–¹æ¡ˆï¼Œé€šå¸¸åŸºäºæ–‡ä»¶æˆ–å—çº§ LRUã€‚æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ™ºèƒ½ç¼“å­˜å­˜å‚¨æ¶æ„ï¼Œç§°ä¸º *Crystal*ï¼Œä¸è®¡ç®—å…±å­˜ã€‚***Crystal*** çš„å®¢æˆ·ç«¯æ˜¯`ç‰¹å®š DBMS çš„æ•°æ®æº`ï¼Œå¸¦æœ‰ä¸‹æ¨è°“è¯ã€‚æœ¬è´¨ä¸Šä¸ DBMS ç±»ä¼¼ï¼ŒCrystal åŒ…å«æŸ¥è¯¢å¤„ç†å’Œä¼˜åŒ–ç»„ä»¶ï¼Œ**==ä¸“æ³¨äºé«˜æ•ˆç¼“å­˜å’ŒæœåŠ¡ç§°ä¸º <u>region</u> çš„å•è¡¨è¶…çŸ©å½¢==**ã€‚ç»“æœè¡¨æ˜ï¼ŒCrystal å¸¦æœ‰ä¸€ä¸ªç‰¹å®šäº DBMS çš„å°å‹æ•°æ®æºè¿æ¥å™¨ï¼Œå¯ä»¥æ˜¾ç€æé«˜åŸç”Ÿ Spark å’Œ Greenplum ä¸Šçš„æŸ¥è¯¢å»¶è¿Ÿï¼ŒåŒæ—¶è¿˜èŠ‚çœè¿œç¨‹å­˜å‚¨çš„å¸¦å®½ã€‚

## 1   INTRODUCTION

> We are witnessing a paradigm shift of analytical database systems to the cloud, driven by its flexibility and **pay-as-you-go** capabilities. Such databases employ a tiered or disaggregated storage model, where the elastic *compute tier* accesses data persisted on independently scalable remote *cloud storage*, such as Amazon S3 [3] and Azure Blobs [36]. Today, nearly all big data systems including Apache Spark, Greenplum, Apache Hive, and Apache Presto support querying cloud storage directly. Cloud vendors also offer cloud services such as AWS Athena, Azure Synapse, and Google BigQuery to meet this increasingly growing demand.
>
> Given the relatively high latency and low bandwidth to remote storage, *caching data* at the compute node has become important. As a result, we are witnessing a <u>==renewed spike==</u> in caching technology for analytics, where the hot data is kept at the compute layer in fast local storage (e.g., SSD) of limited size. Examples include the Alluxio [1] analytics accelerator, the Databricks Delta Cache [9, 15], and the Snowflake cache layer [13].

åœ¨**çµæ´»æ€§**å’Œ**å³ç”¨å³ä»˜**åŠŸèƒ½çš„æ¨åŠ¨ä¸‹ï¼Œæˆ‘ä»¬æ­£åœ¨è§è¯åˆ†ææ•°æ®åº“ç³»ç»Ÿå‘äº‘æ¨¡å¼çš„è½¬å˜ã€‚æ­¤ç±»æ•°æ®åº“é‡‡ç”¨åˆ†å±‚æˆ–åˆ†è§£çš„å­˜å‚¨æ¨¡å‹ï¼Œå…¶ä¸­å¼¹æ€§**è®¡ç®—å±‚**è®¿é—®ä¿å­˜åœ¨ç‹¬ç«‹å¯æ‰©å±•è¿œç¨‹*äº‘å­˜å‚¨*ä¸Šçš„æ•°æ®ï¼Œä¾‹å¦‚ Amazon S3 [3] å’Œ Azure Blob [36]ã€‚å¦‚ä»Šï¼ŒåŒ…æ‹¬ Apache Sparkã€Greenplumã€Apache Hive å’Œ Apache Presto åœ¨å†…çš„å‡ ä¹æ‰€æœ‰å¤§æ•°æ®ç³»ç»Ÿéƒ½æ”¯æŒç›´æ¥æŸ¥è¯¢äº‘å­˜å‚¨ã€‚äº‘ä¾›åº”å•†è¿˜æä¾› AWS Athenaã€Azure Synapse å’Œ Google BigQuery ç­‰äº‘æœåŠ¡æ¥æ»¡è¶³è¿™ç§æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚

ç”±äºè¿œç¨‹å­˜å‚¨ç›¸å¯¹è¾ƒé«˜çš„å»¶è¿Ÿå’Œè¾ƒä½çš„å¸¦å®½ï¼Œåœ¨è®¡ç®—èŠ‚ç‚¹ä¸Š**ç¼“å­˜æ•°æ®**å˜å¾—å¾ˆé‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çœ‹åˆ°ç”¨äºåˆ†æçš„ç¼“å­˜æŠ€æœ¯å‡ºç°äº†æ–°çš„é«˜å³°ï¼Œå…¶ä¸­çƒ­æ•°æ®ä¿å­˜åœ¨è®¡ç®—å±‚çš„å¿«é€Ÿæœ¬åœ°å­˜å‚¨ï¼ˆä¾‹å¦‚ï¼ŒSSDï¼‰ä¸­ï¼Œå¤§å°æœ‰é™ã€‚ç¤ºä¾‹åŒ…æ‹¬ Alluxio [1] åˆ†æåŠ é€Ÿå™¨ã€Databricks Delta ç¼“å­˜ [9, 15] å’Œ Snowflake ç¼“å­˜å±‚ [13]ã€‚

### 1.1   Challenges

> These caching solutions usually operate as a black-box at the file or block level for simplicity, employing standard cache replacement policies such as LRU to manage the cache. In spite of their simplicity, these solutions have not solved several architectural and performance challenges for cloud databases:
>
> - Every DBMS today implements its own caching layer tailored to its specific requirements, resulting in a lot of work duplication across systems, reinventing choices such as what to cache, where to cache, when to cache, and how to cache.
>
> - Databases increasingly support analytics over raw data formats such as CSV and JSON, and row-oriented binary formats such as Apache Avro [6] â€“ all very popular in the data lake [16]. Compared to binary columnar formats such as Apache Parquet [7], data processing on these formats is slower and results in increased costs, even when data has been cached at compute nodes. At the same time, it is expensive (and often less desirable to users) to convert all data into a binary columnar format on storage, particularly because only a small and changing fraction of data is actively used and accessed by queries.
>
> - 
>   Cache utilization (i.e., value per cached byte) is low in existing solutions, as even one needed record or value in a page makes it necessary to retrieve and cache the entire page, wasting valuable space in the cache. This is true even for optimized columnar formats, which often build per-block *zone maps* [21, 40, 48] (min and max value per column in a block) to avoid accessing irrelevant blocks. While zone maps are cheap to maintain and potentially useful, their effectiveness at block skipping is limited by the fact that even one interesting record in a block makes it necessary to retrieve it from storage and scan for completeness.
>
> - Recently, cloud storage systems are offering predicate push-down as a native capability, for example, AWS S3 Select [4] and Azure Query Acceleration [35]. Push-down allows us to send predicates to remote storage and avoid retrieving all blocks, but exacerbates the problem of how to leverage it for effective local caching.
>

ä¸ºç®€å•èµ·è§ï¼Œè¿™äº›ç¼“å­˜è§£å†³æ–¹æ¡ˆé€šå¸¸åœ¨æ–‡ä»¶æˆ–å—çº§åˆ«ä½œä¸ºé»‘ç›’è¿è¡Œï¼Œé‡‡ç”¨æ ‡å‡†ç¼“çš„å­˜æ›¿æ¢ç­–ç•¥ï¼ˆå¦‚ LRUï¼‰æ¥ç®¡ç†ç¼“å­˜ã€‚å°½ç®¡ç®€å•ï¼Œä½†è¿™äº›è§£å†³æ–¹æ¡ˆ**==å¹¶æ²¡æœ‰è§£å†³äº‘æ•°æ®åº“çš„å‡ ä¸ªæ¶æ„å’Œæ€§èƒ½æŒ‘æˆ˜==**ï¼š

- ä»Šå¤©ï¼Œæ¯ä¸ª DBMS éƒ½å®ç°äº†è‡ªå·±çš„ç¼“å­˜å±‚ï¼Œæ ¹æ®å…¶ç‰¹å®šéœ€æ±‚é‡èº«å®šåˆ¶ï¼Œå¯¼è‡´äº†ç³»ç»Ÿé—´å¤§é‡çš„é‡å¤å·¥ä½œï¼Œé‡æ–°è®¾è®¡äº†ç¼“å­˜é‚£äº›å†…å®¹ã€ä½•å¤„ç¼“å­˜ã€ä½•æ—¶ç¼“å­˜å’Œå¦‚ä½•ç¼“å­˜ç­‰é€‰é¡¹ã€‚
- è¶Šæ¥è¶Šå¤šçš„æ•°æ®åº“æ”¯æŒåˆ†æåŸå§‹æ•°æ®æ ¼å¼ï¼ˆå¦‚ CSV å’Œ JSONï¼‰å’Œé¢å‘è¡Œçš„äºŒè¿›åˆ¶æ ¼å¼ï¼ˆå¦‚ Apache Avro [6]ï¼‰â€”â€” æ‰€æœ‰è¿™äº›æ ¼å¼åœ¨æ•°æ®æ¹– [16] ä¸­éƒ½éå¸¸æµè¡Œã€‚ä¸**äºŒè¿›åˆ¶åˆ—æ ¼å¼**ï¼ˆå¦‚ Apache Parquet [7]ï¼‰ç›¸æ¯”ï¼Œè¿™äº›æ ¼å¼çš„æ•°æ®å¤„ç†é€Ÿåº¦è¾ƒæ…¢å¹¶å¯¼è‡´æˆæœ¬å¢åŠ ï¼Œå³ä½¿æ•°æ®å·²ç¼“å­˜åœ¨è®¡ç®—èŠ‚ç‚¹ä¸Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚åŒæ—¶ï¼Œå°†æ‰€æœ‰æ•°æ®è½¬æ¢ä¸ºäºŒè¿›åˆ¶åˆ—æ ¼å¼å­˜å‚¨æˆæœ¬å¾ˆé«˜ï¼ˆç”¨æˆ·é€šå¸¸ä¸å¤ªæ„¿æ„ï¼‰ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºæŸ¥è¯¢æ€»æ˜¯è®¿é—®ä¸€å°éƒ¨åˆ†ä¸æ–­å˜åŒ–çš„æ•°æ®ã€‚
- ç°æœ‰è§£å†³æ–¹æ¡ˆä¸­çš„ç¼“å­˜åˆ©ç”¨ç‡ï¼ˆå³æ¯ä¸ªç¼“å­˜å­—èŠ‚çš„å€¼ï¼‰å¾ˆä½ï¼Œå³ä½¿åªéœ€è¦é¡µé¢ä¸­ä¸€ä¸ªè®°å½•æˆ–å€¼ä¹Ÿéœ€è¦æ£€ç´¢å’Œç¼“å­˜æ•´ä¸ªé¡µé¢ï¼Œä»è€Œæµªè´¹äº†å®è´µç¼“å­˜çš„ç©ºé—´ã€‚å¯¹äºä¼˜åŒ–çš„åˆ—æ ¼å¼ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè¿™äº›æ ¼å¼é€šå¸¸ä¸ºæ¯ä¸ªå—æ„å»º **zone map** [21, 40, 48]ï¼ˆå—ä¸­æ¯åˆ—çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼‰ä»¥é¿å…è®¿é—®ä¸ç›¸å…³çš„å—ã€‚è™½ç„¶ç»´æŠ¤ **zone map** çš„æˆæœ¬ä½ä¸”å¯èƒ½æœ‰ç”¨ï¼Œä½†å®ƒä»¬åœ¨å—è·³è¿‡æ–¹é¢çš„æœ‰æ•ˆæ€§å—åˆ°ä»¥ä¸‹äº‹å®çš„é™åˆ¶ï¼šå³ä½¿åªè®¿é—®å—ä¸­ä¸€æ¡è®°å½•ï¼Œä¹Ÿéœ€è¦ä»å­˜å‚¨ä¸­æ£€ç´¢å¹¶æ‰«æå®Œæ•´æ€§ã€‚
- æœ€è¿‘ï¼Œäº‘å­˜å‚¨ç³»ç»Ÿæä¾›åŸç”Ÿçš„è°“è¯ä¸‹æ¨åŠŸèƒ½ï¼Œä¾‹å¦‚ AWS S3 Select [4] å’Œ Azure Query Acceleration [35]ã€‚ ä¸‹æ¨å…è®¸æˆ‘ä»¬å°†è°“è¯å‘é€åˆ°è¿œç¨‹å­˜å‚¨ä»¥é¿å…å–å›æ‰€æœ‰å—ï¼Œä½†åŠ å‰§äº†åˆ©ç”¨è°“è¯æé«˜æœ¬åœ°ç¼“å­˜æ•ˆç‡çš„é—®é¢˜ã€‚

### 1.2   Opportunities

> In an effort to alleviate some of these challenges, several design trends are now becoming commonplace. Database systems such as Spark are adopting the model of a plug-in â€œdata sourceâ€ that serves as an input adapter to support data in different formats. These data sources allow the *push-down* of table-level predicates to the data source. While push-down was developed with the intention of data pruning at the source, we find that it opens up a new opportunity to leverage semantics and cache data in more efficient ways.
>
> Moreover, there is rapid convergence in the open-source community on Apache Parquet as a columnar data format, along with highly efficient techniques to apply predicates on them using LLVM with Apache Arrow [5, 8]. This opens up the possibility of system designs that perform a limited form of data processing and transformation *outside* the core DBMS easily and without sacrificing performance. Further, because most DBMSs support Parquet, it gives us an opportunity to cache data in a DBMS-agnostic way.
>

ä¸ºäº†ç¼“è§£å…¶ä¸­ä¸€äº›æŒ‘æˆ˜ï¼ŒæŸäº›è®¾è®¡è¶‹åŠ¿è¶Šæ¥è¶Šè¶‹åŒã€‚åƒ Spark è¿™ç±»æ•°æ®åº“ç³»ç»Ÿä¸ºæ”¯æŒä¸åŒæ ¼å¼çš„æ•°æ®ï¼Œæä¾› `DataSource` æ’ä»¶ä½œä¸ºè¾“å…¥é€‚é…å™¨ã€‚è¿™äº›æ•°æ®æºæ’ä»¶æ”¯æŒè¡¨çº§è°“è¯**ä¸‹æ¨**ã€‚è™½ç„¶**è°“è¯ä¸‹æ¨**æ˜¯ä¸ºäº†åœ¨æ•°æ®æºè¿›è¡Œæ•°æ®è£å‰ªè€Œå¼€å‘ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒæä¾›äº†æ–°çš„æœºä¼šï¼Œèƒ½ä»¥æ›´æœ‰æ•ˆçš„æ–¹å¼åˆ©ç”¨è¯­ä¹‰å’Œç¼“å­˜æ•°æ®ã€‚

æ­¤å¤–ï¼ŒApache Parquet åœ¨å¼€æºç¤¾åŒºä¸­ä½œä¸ºä¸€ç§åˆ—å¼æ•°æ®æ ¼å¼å¾—åˆ°äº†å¿«é€Ÿçš„èåˆï¼Œä»¥åŠ Apache Arrow ä½¿ç”¨ LLVM å¯¹ Parquet åº”ç”¨è°“è¯çš„é«˜æ•ˆæŠ€æœ¯[5,8]ã€‚è¿™ä¸ºç³»ç»Ÿè®¾è®¡æä¾›äº†ä¸€ç§å¯èƒ½æ€§ï¼Œåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œ**åœ¨æ ¸å¿ƒ DBMS ä¹‹å¤–**è½»æ¾åœ°æ‰§è¡Œæœ‰é™å½¢å¼çš„æ•°æ®å¤„ç†å’Œè½¬æ¢ã€‚åŒæ—¶å› ä¸ºå¤§å¤šæ•° DBMS éƒ½æ”¯æŒ Parquetï¼Œè¿™è®©æˆ‘ä»¬æœ‰æœºä¼šä»¥ä¸ DBMS æ— å…³çš„æ–¹å¼ç¼“å­˜æ•°æ®ã€‚

### 1.3   Introducing Crystal

> We propose a new â€œsmartâ€ storage middleware called *Crystal*, that is decoupled from the database and sits between the DB and raw storage. Crystal may be viewed as a mini-DBMS, or *cache management* *system* (CMS), for storage. It runs as two sub-components:
>
> - The Crystal CMS runs on the compute node, accessible to local â€œclientsâ€ and able to interact with remote storage.
>
> - Crystalâ€™s clients, called *connectors*, are DB-specific adapters that themselves implement the data source API with push-down predicates, similar to todayâ€™s CSV and Parquet data sources.
>
> Crystal manages fast local storage (SSD) as a cache and talks to remote storage to retrieve data as needed. Unlike traditional file caches, it determines which *regions* (parts of each table) to transform and cache locally in columnar form. Data may be cached in more than one region if necessary. Crystal receives â€œqueriesâ€ from clients, as requests with push-down predicates. It responds with local (in cache) or remote (on storage) paths for ==files that cover the request==. The connectors pass data to the *unmodified* DBMS for post-processing as usual. Benefits of this architecture include:
>
> - It can be shared across multiple unmodified databases, requiring only a lightweight DBMS-specific client connector component. 
> - It can download and transform data into automatically chosen ==semantic regions== in the cache, in a DBMS-agnostic columnar format, reusing new tools such as Parquet and Arrow to do so. 
> - It can independently optimize what data to transform, filter, and cache locally, allowing multiple views of the same data, and efficiently match and serve clients at query time.
>
> These architectural benefits come with technical challenges (Section 2 provides a system overview) that we address in this paper:
>
> - (Sections 2 & 3) Defining an API and protocol to communicate region requests and data between Crystal and its connector clients. 
>
> - (Section 3) Efficiently downloading and transforming data to regions in the local cache, managing cache contents, and storing meta-data for matching regions with push-down predicates over diverse data types, without impacting query latency.
>
> - (Section 4) Optimizing the contents of the cache while: (1) balancing short-term needs (e.g., a burst of new queries) vs. long-term query history; (2) handling queries that are not identical but often overlap; (3) exploiting the benefit of duplicating frequently accessed subsets of data in more than one region; and (4) taking into account the overhead incurred by creating many small files in block columnar format, instead of fewer larger ones; and (5) managing statistics necessary for the above tasks.
>
> Using Crystal, we get ***lower query latencies and more efficient use of the bandwidth between compute and storage***, as compared to state-of-the-art solutions. We validate this by implementing Crystal with Spark and Greenplum connectors (Section 5). Our evaluation using common workload patterns shows Crystalâ€™s ability to outperform block-based caching schemes with lower cache sizes, improve query latencies by up to 20x for individual queries (and up to 8x on average), adapt to workload changes, and save bandwidth from remote storage by up to 41% on average (Section 6). 
>
> We note that Crystalâ€™s cached regions may be considered as materialized views [20, 27, 44, 45] or semantic caches [14, 29, 30, 41â€“ 43, 47], thereby inheriting from this rich line of work. Our caches have the additional restriction that they are strictly the result of single-table predicates (due to the nature of the data source API). <u>==Specifically, Crystalâ€™s regions are **disjunctions** of **conjunctions** of predicates over each individual table==</u>. This restriction is exploited in our solutions to the technical challenges, allowing us to match better, generalize better, and search more efficiently for the best set of cached regions. As data sources mature, we expect them to push down cross-table predicates and aggregates in future, e.g., via *data-* *induced* *predicates* [28]. Such developments will require a revisit of our algorithms in future; for instance, our region definitions will need to represent cross-table predicates. We focus on readonly workloads in this paper; updates can be handled by view invalidation (easy) or refresh (more challenging), and are left as future work. Finally, we note that Crystal can naturally benefit from remote storage supporting push-down predicates; a detailed study is deferred until the technology matures to support columnar formats natively (only CSV files are supported in the current generation). We cover related work in Section 7 and conclude in Section 8.
>

æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º *Crystal* çš„æ–°â€œæ™ºèƒ½â€å­˜å‚¨ä¸­é—´ä»¶ï¼Œå®ƒä¸æ•°æ®åº“åˆ†ç¦»å¹¶ä½äºæ•°æ®åº“å’ŒåŸå§‹å­˜å‚¨ä¹‹é—´ã€‚ Crystal å¯è¢«è§†ä¸ºç”¨äºå­˜å‚¨çš„å°å‹ DBMSï¼Œæˆ–**ç¼“å­˜ç®¡ç†ç³»ç»Ÿ** (CMS)ã€‚ å®ƒä½œä¸ºä¸¤ä¸ªå­ç»„ä»¶è¿è¡Œï¼š

- Crystal CMS åœ¨è®¡ç®—èŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œèƒ½å¤Ÿä¸è¿œç¨‹å­˜å‚¨è¿›è¡Œäº¤äº’ï¼›æœ¬åœ°**å®¢æˆ·ç«¯**è®¿é—® CMSã€‚
- Crystal çš„å®¢æˆ·ç«¯ï¼Œç§°ä¸º *connectors*ï¼Œæ˜¯ç‰¹å®šäº DB çš„é€‚é…å™¨ï¼Œå®ç°æ•°æ®æº APIï¼Œä¸”æ”¯æŒè°“è¯ä¸‹æ¨ï¼Œç±»ä¼¼äºä»Šå¤© Spark é‡Œæ”¯æŒ CSV çš„ Parquet æ•°æ®æºã€‚

Crystal å°†æœ¬åœ°å¿«é€Ÿå­˜å‚¨ (SSD) ä½œä¸ºç¼“å­˜ç®¡ç†ï¼Œå¹¶ä¸è¿œç¨‹å­˜å‚¨é€šä¿¡ä»¥æ ¹æ®éœ€è¦å–å›æ•°æ®ã€‚ä¸ä¼ ç»Ÿçš„æ–‡ä»¶ç¼“å­˜ä¸åŒï¼Œå®ƒä¼šåˆ¤æ–­å“ªäº› **region**ï¼ˆè¡¨çš„ä¸€éƒ¨åˆ†ï¼‰ä¼šè¢«è½¬æ¢ä¸ºåˆ—æ ¼å¼ç¼“å­˜åœ¨æœ¬åœ°ã€‚å¦‚æœ‰å¿…è¦ï¼Œæ•°æ®å¯ä»¥ç¼“å­˜åœ¨å¤šä¸ª **region** ä¸­ã€‚Crystal ä»å®¢æˆ·ç«¯æ¥æ”¶å¸¦æœ‰è°“è¯ä¸‹æ¨çš„**æŸ¥è¯¢**ã€‚å®ƒä»¥æœ¬åœ°ï¼ˆç¼“å­˜ï¼‰æˆ–è¿œç¨‹ï¼ˆå­˜å‚¨ï¼‰ä¸­çš„==æ–‡ä»¶==æ¥å“åº”è¯·æ±‚ï¼Œè¿æ¥å™¨åƒå¾€å¸¸ä¸€æ ·å°†æ•°æ®ä¼ é€’ç»™**æœªä¿®æ”¹çš„** DBMS è¿›è¡Œåå¤„ç†ã€‚ è¿™ç§æ¶æ„çš„å¥½å¤„åŒ…æ‹¬ï¼š

- å¯ä»¥åœ¨å¤šä¸ªæœªä¿®æ”¹çš„æ•°æ®åº“ä¹‹é—´å…±äº«ï¼Œåªéœ€ä¸€ä¸ªç‰¹å®š DBMS çš„è½»é‡çº§å®¢æˆ·ç«¯è¿æ¥å™¨ç»„ä»¶ã€‚
- å¯ä»¥ä¸‹è½½å¹¶è½¬æ¢æ•°æ®ä¸ºç¼“å­˜ä¸­è‡ªåŠ¨é€‰æ‹©çš„ ==semantic regions==ï¼Œé‡‡ç”¨ä¸ DBMS æ— å…³çš„åˆ—æ ¼å¼ï¼Œé‡ç”¨ Parquet å’Œ Arrow ç­‰æ–°å·¥å…·æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚
- å¯ä»¥ç‹¬ç«‹ä¼˜åŒ–å“ªäº›æ•°æ®åœ¨æœ¬åœ°è¿›è¡Œè½¬æ¢ã€è¿‡æ»¤å’Œç¼“å­˜ï¼Œå…è®¸åŒä¸€æ•°æ®çš„å¤šä¸ªè§†å›¾ï¼Œå¹¶åœ¨æŸ¥è¯¢æ—¶é«˜æ•ˆåœ°åŒ¹é…å’ŒæœåŠ¡äºå®¢æˆ·ç«¯ã€‚

è¿™äº›ä½“ç³»ç»“æ„ä¼˜åŠ¿å¸¦æ¥äº†æŠ€æœ¯æŒ‘æˆ˜ï¼ˆç¬¬2èŠ‚æä¾›äº†ç³»ç»Ÿæ¦‚è¿°ï¼‰ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬æ–‡ä¸­è®¨è®ºè¿™äº›æŒ‘æˆ˜ï¼š

- ï¼ˆç¬¬ 2 å’Œç¬¬ 3 èŠ‚ï¼‰å®šä¹‰ API å’Œåè®®ï¼Œä»¥ä¾¿ Crystal è¿æ¥å™¨å®¢æˆ·ç«¯å‘å…¶è¯·æ±‚ **region** å’Œæ•°æ®ã€‚
- ï¼ˆç¬¬ 3 èŠ‚ï¼‰é«˜æ•ˆä¸‹è½½å’Œè½¬æ¢æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜ä¸­çš„ **region**ã€ç®¡ç†ç¼“å­˜å†…å®¹ã€å­˜å‚¨å…ƒæ•°æ®ï¼Œä»¥ä¾¿åœ¨å„ç§æ•°æ®ç±»å‹ä¸Šä½¿ç”¨è°“è¯ä¸‹æ¨åŒ¹é… **region**ï¼Œè€Œä¸å½±å“æŸ¥è¯¢å»¶è¿Ÿã€‚
- ï¼ˆç¬¬ 4 èŠ‚ï¼‰ä¼˜åŒ–ç¼“å­˜å†…å®¹ï¼šï¼ˆ1ï¼‰å¹³è¡¡çŸ­æœŸéœ€æ±‚ï¼ˆä¾‹å¦‚ï¼Œçªå‘çš„æ–°æŸ¥è¯¢ï¼‰ä¸é•¿æœŸå†å²æŸ¥è¯¢ï¼›(2) å¤„ç†ä¸å®Œå…¨ç›¸åŒä½†ç»å¸¸é‡å çš„æŸ¥è¯¢ï¼› (3) åˆ©ç”¨åœ¨å¤šä¸ª **region** å¤åˆ¶é¢‘ç¹è®¿é—®çš„æ•°æ®å­é›†çš„å¥½å¤„ï¼›(4) è€ƒè™‘åˆ°æ‰€äº§ç”Ÿçš„å¼€é”€ï¼ŒæŒ‰å—ä»¥åˆ—æ ¼å¼åˆ›å»ºè®¸å¤šå°æ–‡ä»¶è€Œä¸æ˜¯æ›´å°‘çš„å¤§æ–‡ä»¶ï¼›(5) ç®¡ç†ä¸Šè¿°ä»»åŠ¡æ‰€éœ€çš„ç»Ÿè®¡æ•°æ®ã€‚

ä¸æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œä½¿ç”¨ Crystal å¯ä»¥è·å¾—**æ›´ä½çš„æŸ¥è¯¢å»¶è¿Ÿ**ï¼Œ**æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—å’Œå­˜å‚¨ä¹‹é—´çš„å¸¦å®½**ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ Spark å’Œ Greenplum è¿æ¥å™¨å®ç° Crystal æ¥éªŒè¯è¿™ä¸€ç‚¹ï¼ˆç¬¬ 5 èŠ‚ï¼‰ã€‚ é€šè¿‡è¯„ä¼°å¸¸è§çš„å·¥ä½œè´Ÿè½½æ¨¡å¼è¡¨æ˜ï¼ŒCrystal èƒ½å¤Ÿä»¥è¾ƒå°çš„ç¼“å­˜å¤§å°è¶…è¶ŠåŸºäºå—çš„ç¼“å­˜æ–¹æ¡ˆï¼Œå°†å•ä¸ªæŸ¥è¯¢çš„æŸ¥è¯¢å»¶è¿Ÿæé«˜å¤šè¾¾ 20 å€ï¼ˆå¹³å‡å¤šè¾¾ 8 å€ï¼‰ï¼Œé€‚åº”å·¥ä½œè´Ÿè½½å˜åŒ–ï¼Œå¹³å‡èŠ‚çœ 41% çš„è¿œç¨‹å­˜å‚¨å¸¦å®½ï¼ˆç¬¬6èŠ‚ï¼‰ã€‚

æˆ‘ä»¬æ³¨æ„åˆ° Crystal çš„ç¼“å­˜ **region** å¯ä»¥è¢«è§†ä¸º**ç‰©åŒ–è§†å›¾** [20, 27, 44, 45] æˆ–**è¯­ä¹‰ç¼“å­˜** [14, 29, 30, 41-43, 47]ï¼Œä»è€Œç»§æ‰¿äº†è¿™ä¸€ä¸°å¯Œçš„å·¥ä½œã€‚ç”±äºæ•°æ®æº API çš„æ€§è´¨ï¼Œæˆ‘ä»¬çš„ç¼“å­˜æœ‰ä¸€ä¸ªé¢å¤–çš„é™åˆ¶ï¼Œå³å®ƒä»¬åªæ˜¯å•è¡¨è°“è¯çš„ç»“æœã€‚<u>==å…·ä½“æ¥è¯´ï¼ŒCrystal çš„ **region** æ˜¯æ¯å¼ è¡¨ä¸Šè°“è¯è¿æ¥çš„æå–==</u>ã€‚æˆ‘ä»¬åœ¨è§£å†³æŠ€æœ¯æŒ‘æˆ˜çš„æ–¹æ¡ˆä¸­åˆ©ç”¨äº†è¿™ä¸€é™åˆ¶ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°åŒ¹é…ã€æ›´å¥½åœ°æ¦‚æ‹¬å¹¶æ›´æœ‰æ•ˆåœ°æœç´¢æœ€ä½³ç¼“å­˜ region é›†ã€‚éšç€æ•°æ®æºçš„æˆç†Ÿï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥å¯ä»¥ä¸‹æ¨è·¨è¡¨è°“è¯å’Œèšåˆï¼Œä¾‹å¦‚ï¼Œé€šè¿‡ *data-induced predicates* [28]ã€‚è¿™æ ·çš„å‘å±•å°†éœ€è¦åœ¨æœªæ¥é‡æ–°å®¡è§†æˆ‘ä»¬çš„ç®—æ³•ï¼›ä¾‹å¦‚ï¼Œæˆ‘ä»¬ region çš„å®šä¹‰éœ€è¦æ”¯æŒ**è·¨è¡¨è°“è¯**ã€‚æœ¬æ–‡ä¸»è¦å…³æ³¨åªè¯»å·¥ä½œè´Ÿè½½ï¼›æ›´æ–°å¯ä»¥é€šè¿‡è§†å›¾å¤±æ•ˆï¼ˆç®€å•ï¼‰æˆ–åˆ·æ–°ï¼ˆæ›´å…·æŒ‘æˆ˜æ€§ï¼‰æ¥å¤„ç†ï¼Œå¹¶ç•™ä½œæœªæ¥çš„å·¥ä½œã€‚æœ€åï¼Œæˆ‘ä»¬æ³¨æ„åˆ° Crystal è‡ªç„¶è€Œç„¶åœ°èƒ½ä»æ”¯æŒ**è°“è¯ä¸‹æ¨**çš„è¿œç¨‹å­˜å‚¨ä¸­å—ç›Šï¼›å½“å‰åªæ”¯æŒ CSV æ ¼å¼çš„æ–‡ä»¶ï¼Œè¯¦ç»†ç ”ç©¶è¢«æ¨è¿Ÿåˆ°æŠ€æœ¯æˆç†Ÿä»¥æ”¯æŒåŸç”Ÿåˆ—æ ¼å¼ã€‚æˆ‘ä»¬åœ¨ç¬¬ 7 èŠ‚ä»‹ç»ç›¸å…³å·¥ä½œå¹¶åœ¨ç¬¬ 8 èŠ‚æ€»ç»“ã€‚


## 2   SYSTEM OVERVIEW

> Figure 1 shows where Crystal fits in todayâ€™s cloud analytics ecosystem. Each compute node runs a DBMS instance; Crystal is colocated on the compute node and serves these DBMS instances via data source connectors. The aim is to serve as a caching layer between big data systems and cloud storage, exploiting fast local storage in compute nodes to reduce data accesses to remote storage.

å›¾ 1 æ˜¾ç¤ºäº† Crystal åœ¨å½“ä»Šäº‘åˆ†æç”Ÿæ€ç³»ç»Ÿä¸­çš„ä½ç½®ã€‚ æ¯ä¸ªè®¡ç®—èŠ‚ç‚¹è¿è¡Œä¸€ä¸ª DBMS å®ä¾‹ï¼› Crystal ä½äºè®¡ç®—èŠ‚ç‚¹ä¸Šï¼Œå¹¶é€šè¿‡æ•°æ®æºè¿æ¥å™¨ä¸ºè¿™äº› DBMS å®ä¾‹æä¾›æœåŠ¡ã€‚ç›®çš„æ˜¯ä½œä¸ºå¤§æ•°æ®ç³»ç»Ÿå’Œäº‘å­˜å‚¨ä¹‹é—´çš„ç¼“å­˜å±‚ï¼Œåˆ©ç”¨è®¡ç®—èŠ‚ç‚¹ä¸­çš„å¿«é€Ÿæœ¬åœ°å­˜å‚¨æ¥å‡å°‘å¯¹è¿œç¨‹å­˜å‚¨çš„æ•°æ®è®¿é—®ã€‚

### 2.1   Architecture

> A key design goal is to make Crystal sufficiently generic so that it can be plugged into an existing big data system with minimum engineering effort. Therefore, Crystal is architected as two separate components: a light DBMS-specific data source connector and the Crystal CMS process. These are described next.
>
> **2.1.1     Data Source Connector**. Modern big data systems (e.g., Spark, Hive, and Presto) provide a *data source API* to support a variety of data sources and formats. A data source receives push-down filtering and column pruning requests from the DBMS through this API. Thus, the data source has the flexibility to leverage this additional information to reduce the amount of data that needs to be sent back to the DBMS, e.g., via block-level pruning in Parquet. In this paper, we refer to such push-down information as a *query* or *requested region*. A Crystal connector is integrated into the unmodified DBMS through this data source API. It is treated as another data source from the perspective of the DBMS, and as a client issuing queries from the perspective of the Crystal CMS.
>
> **2.1.2     Crystal CMS**. Figure 2 shows the Crystal CMS in detail. It maintains two local caches â€“ a small *requested region* (RR) cache and a large *oracle region* (OR) cache â€“ corresponding to shortand long-term knowledge respectively. Both caches store data in an efficient columnar open format such as Parquet. Crystal receives â€œqueriesâ€ from connectors via the Crystal API. <u>==A query consists of a request for a file (remote path) with push-down predicates==</u>. Crystal first checks with the Matcher to see if it can cover the query using one or more cached regions. If yes (cache hit), it returns a set of file paths from local storage. If not (cache miss), there are two options:
>
> 1. It responds with the remote path so that the connector can process it as usual. Crystal optionally requests the connector to store the downloaded and filtered region in its RR cache.
> 2. It downloads the data from remote, applies predicates, stores the result in the RR cache, and returns this path to the connector.
>
> Thus, the RR cache is populated **eagerly** by either Crystal or the DBMS. Not every requested region is cached eagerly; instead an LRU-2 based decision is taken per request.
>
> More importantly, in the background, Crystal collects a historical trace of queries and invokes a caching Oracle Plugin module to compute the best content for the OR cache. The new content is populated using a combination of remote storage and existing content in the RR and OR caches. Section 3 covers region processing in detail, while Section 4 covers cache optimization.

ä¸€ä¸ªå…³é”®çš„è®¾è®¡ç›®æ ‡æ˜¯ä½¿ Crystal å…·æœ‰è¶³å¤Ÿçš„é€šç”¨æ€§ï¼Œä»¥ä¾¿å®ƒèƒ½ä»¥æœ€å°‘çš„å·¥ç¨‹é‡æ¥å…¥åˆ°ç°æœ‰çš„å¤§æ•°æ®ç³»ç»Ÿä¸­ã€‚å› æ­¤ï¼ŒCrystal è¢«æ„å»ºä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ç»„ä»¶ï¼šä¸€ä¸ªç‰¹å®š DBMS çš„è½»é‡çº§æ•°æ®æºè¿æ¥å™¨å’Œ Crystal CMS è¿›ç¨‹ï¼š

**2.1.1 æ•°æ®æºè¿æ¥å™¨**ã€‚ ç°ä»£å¤§æ•°æ®ç³»ç»Ÿï¼ˆä¾‹å¦‚ Sparkã€Hive å’Œ Prestoï¼‰æä¾›äº†**æ•°æ®æº API** æ¥æ”¯æŒå„ç§æ•°æ®æºå’Œæ ¼å¼ã€‚ æ•°æ®æºé€šè¿‡æ­¤ API ä» DBMS æ¥æ”¶è°“è¯ä¸‹æ¨å’Œåˆ—è£å‰ªè¯·æ±‚ã€‚å› æ­¤ï¼Œæ•°æ®æºå¯ä»¥çµæ´»åœ°åˆ©ç”¨è¿™äº›é™„åŠ ä¿¡æ¯æ¥å‡å°‘éœ€è¦è¿”å› DBMS çš„æ•°æ®é‡ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡ Parquet ä¸­çš„å—çº§è£å‰ªã€‚æœ¬æ–‡å°†æ­¤ç±»ä¸‹æ¨ä¿¡æ¯ç§°ä¸º **query** æˆ– **requested region**ã€‚ Crystal è¿æ¥å™¨é€šè¿‡æ­¤æ•°æ®æº API é›†æˆåˆ°æœªä¿®æ”¹çš„ DBMS ä¸­ã€‚ä» DBMS çš„è§’åº¦æ¥çœ‹ï¼Œå®ƒè¢«è§†ä¸ºå¦ä¸€ä¸ªæ•°æ®æºï¼Œä» Crystal CMS çš„è§’åº¦æ¥çœ‹ï¼Œå®ƒè¢«è§†ä¸ºå‘å‡ºæŸ¥è¯¢çš„å®¢æˆ·ç«¯ã€‚

**2.1.2     Crystal CMS**ã€‚ å›¾ 2 è¯¦ç»†æ˜¾ç¤ºäº† Crystal CMSã€‚ å®ƒç»´æŠ¤ä¸¤ä¸ªæœ¬åœ°ç¼“å­˜ â€”â€” ä¸€ä¸ªå°çš„ **requested region**ï¼ˆRRï¼‰ç¼“å­˜å’Œä¸€ä¸ªå¤§çš„ **oracle region**ï¼ˆORï¼‰ç¼“å­˜ â€”â€” åˆ†åˆ«å¯¹åº”äºçŸ­æœŸå’Œé•¿æœŸç¼“å­˜ã€‚ä¸¤ä¸ªç¼“å­˜éƒ½ä»¥ Parquet æ ¼å¼å­˜å‚¨æ•°æ®ã€‚ Crystal é€šè¿‡ Crystal API ä»è¿æ¥å™¨æ¥æ”¶**æŸ¥è¯¢**ã€‚<u>==æŸ¥è¯¢ç”±å¯¹å¸¦æœ‰ä¸‹æ¨è°“è¯ï¼ˆè¿œç¨‹è·¯å¾„ï¼‰æ–‡ä»¶çš„è¯·æ±‚ç»„æˆ==</u>ã€‚ Crystal é¦–å…ˆæ£€æŸ¥**åŒ¹é…å™¨**ä»¥æŸ¥çœ‹å®ƒæ˜¯å¦å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªç¼“å­˜ **region** è¦†ç›–æŸ¥è¯¢ã€‚ å¦‚æœï¼ˆç¼“å­˜å‘½ä¸­ï¼‰ï¼Œå®ƒä¼šä»æœ¬åœ°å­˜å‚¨è¿”å›ä¸€ç»„æ–‡ä»¶è·¯å¾„ã€‚ å¦‚æœæ²¡æœ‰ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰ï¼Œåˆ™æœ‰ä¸¤ç§é€‰æ‹©ï¼š

1. ä»¥è¿œç¨‹è·¯å¾„å“åº”ï¼Œè¿æ¥å™¨å¯ä»¥ç…§å¸¸å¤„ç†å®ƒã€‚ Crystal å¯é€‰æ‹©è¯·æ±‚è¿æ¥å™¨å°†ä¸‹è½½å’Œè¿‡æ»¤çš„ region å­˜å‚¨åœ¨ RR ç¼“å­˜ä¸­ã€‚
2. ä»è¿œç¨‹ä¸‹è½½æ•°æ®ï¼Œåº”ç”¨è°“è¯å¹¶å°†ç»“æœå­˜å‚¨åœ¨ RR ç¼“å­˜ä¸­ï¼Œå¹¶å°†æ­¤è·¯å¾„è¿”å›ç»™è¿æ¥å™¨ã€‚

å› æ­¤ï¼ŒRR ç¼“å­˜ç”± Crystal æˆ– DBMS  **çƒ­åˆ‡åœ°**å¡«å……ã€‚ å¹¶éæ¯ä¸ªè¯·æ±‚çš„ region éƒ½è¢«**æ€¥åˆ‡åœ°**ç¼“å­˜ï¼›ç›¸åï¼Œæ¯ä¸ªè¯·æ±‚éƒ½ä¼šåŸºäº LRU-2 åšå‡ºå†³å®šã€‚

â€¼ï¸æ›´é‡è¦çš„æ˜¯åœ¨åå°ï¼ŒCrystal æ”¶é›†æŸ¥è¯¢å†å²å¹¶è°ƒç”¨ **oracle cache** æ’ä»¶æ¨¡å—æ¥è®¡ç®—æ”¾ç½®åœ¨ OR ç¼“å­˜ä¸­çš„æœ€ä½³å†…å®¹ã€‚ä½¿ç”¨è¿œç¨‹å­˜å‚¨å’Œ RR å’Œ OR ç¼“å­˜ä¸­ç°æœ‰å†…å®¹çš„ç»„åˆå¡«å……æ–°å†…å®¹ã€‚ç¬¬ 3 èŠ‚è¯¦ç»†ä»‹ç»äº† region å¤„ç†ï¼Œè€Œç¬¬ 4 èŠ‚ä»‹ç»äº†ç¼“å­˜ä¼˜åŒ–ã€‚

### 2.2   Generality of the Crystal Design

As mentioned above, Crystal is architected with a view to making it easy to use with any cloud analytics system. Crystal offers three extensibility points. **First**, users can replace the caching oracle with a custom implementation that is tailored to their workload. Second, the remote storage adapter may be replaced to work with any cloud remote storage. Third, a custom connector may be implemented for each DBMS that needs to use Crystal.

The connector interfaces with Crystal with a generic protocol based simply on file paths. Cached regions are stored in an open format (Parquet) rather than the internal format of a specific DBMS, making it DBMS-agnostic. Further, a connector can feed the cached region to the DBMS by simply invoking its built-in data source for the open format (e.g., the built-in Parquet reader in Spark) to read the region. Thus, the connector developer does not need to manually implement the conversion, making its implementation a fairly straightforward process. In Section 5, we discuss our connectors for Spark and Greenplum, which take less than 350 lines of code.

å¦‚ä¸Šæ‰€è¿°ï¼ŒCrystal çš„æ¶æ„æ—¨åœ¨ä½¿å…¶æ˜“äºä¸ä»»ä½•äº‘åˆ†æç³»ç»Ÿä¸€èµ·ä½¿ç”¨ã€‚ Crystal æä¾›äº†ä¸‰ä¸ªæ‰©å±•ç‚¹ã€‚**é¦–å…ˆ**ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨æ ¹æ®å…¶å·¥ä½œè´Ÿè½½å®šåˆ¶è‡ªå®šä¹‰çš„å®ç°ä»¥æ›¿æ¢ oracle cacheã€‚**å…¶æ¬¡**ï¼Œå¯ä»¥æ›´æ¢**è¿œç¨‹å­˜å‚¨é€‚é…å™¨**ä»¥ä¸**è¿œç¨‹äº‘å­˜å‚¨**ä¸€èµ·ä½¿ç”¨ã€‚ç¬¬ä¸‰ï¼Œå¯ä»¥ä¸ºæ¯ä¸ªéœ€è¦ä½¿ç”¨ Crystal çš„ DBMS å®ç°è‡ªå®šä¹‰è¿æ¥å™¨ã€‚

è¿æ¥å™¨é€šè¿‡åŸºäºæ–‡ä»¶è·¯å¾„çš„é€šç”¨åè®®ä¸ Crystal äº¤äº’ã€‚ç¼“å­˜åŒºåŸŸä»¥å¼€æ”¾æ ¼å¼ (Parquet) è€Œä¸æ˜¯ç‰¹å®š DBMS çš„å†…éƒ¨æ ¼å¼å­˜å‚¨ï¼Œä½¿å…¶ä¸ DBMS æ— å…³ã€‚

æ­¤å¤–ï¼Œè¿æ¥å™¨å¯ä»¥ç®€å•åœ°è°ƒç”¨å…¶å†…ç½®çš„æ•°æ®æºï¼ˆä¾‹å¦‚ï¼ŒSpark ä¸­å†…ç½®çš„ Parquet readerï¼‰æ¥è¯»å–è¯¥ **region**ï¼Œä»è€Œå°†ç¼“å­˜çš„ region æä¾›ç»™ DBMSã€‚

å› æ­¤ï¼Œè¿æ¥å™¨å¼€å‘äººå‘˜ä¸éœ€è¦æ‰‹åŠ¨å®ç°è½¬æ¢ï¼Œä»è€Œä½¿å…¶å®ç°è¿‡ç¨‹ç›¸å½“ç®€å•ã€‚åœ¨ç¬¬ 5 èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ç”¨äº Spark å’Œ Greenplum çš„è¿æ¥å™¨ï¼Œå®ƒä»¬åªéœ€è¦ä¸åˆ° 350 è¡Œä»£ç ã€‚

å› æ­¤ï¼Œè¿æ¥å™¨å¼€å‘äººå‘˜ä¸éœ€è¦æ‰‹åŠ¨å®ç°è½¬æ¢ï¼Œä»è€Œä½¿å…¶å®ç°æˆä¸ºä¸€ä¸ªç›¸å½“ç®€å•çš„è¿‡ç¨‹ã€‚åœ¨ç¬¬5èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºSparkå’ŒGreenplumçš„è¿æ¥å™¨ï¼Œå®ƒä»¬åªéœ€è¦ä¸åˆ°350è¡Œä»£ç ã€‚

### 2.3   Revisiting the Caching Problem

Leveraging push-down predicates, Crystal caches different subsets of data called regions. Regions can be considered as views on the table, and are a form of semantic caching [14, 29, 30, 42, 43, 47]. Compared to traditional file caching, the advantage of semantic caching is two-fold. First, it usually returns a much tighter view to the DBMS, and thus reduces the need to post-process the data, saving I/O and CPU cost. Second, regions can be much smaller than the original files, resulting in better cache space utilization and higher hit ratios. For example, Figure 3 shows a case where regions capture all views of all queries, whereas LRU-based file caching can only keep less than half of these views.

Cached regions in Crystal may overlap. In data warehouses and data lakes, it is common to see that a large number of queries access a few tables or files, making overlapping queries the norm rather than the exception at the storage layer. Therefore, Crystal has to take overlap into account when deciding which cached data should be evicted. To the best of our knowledge, previous work on replacement policies for semantic caching does not consider overlap of cached regions (see more details in Section 7).

With overlapping views, the replacement policy in Crystal becomes a very challenging optimization problem (details in Section 4). Intuitively, when deciding if a view should be evicted from the cache, all other views that are overlapping with this view should also be taken into consideration. As a result, traditional replacement policies such as LRU that evaluate each view independently are not suitable for Crystal, as we will show in the evaluation (Section 6). 

Recall that we split the cache into two regions: requested region (RR) and oracle region (RR). The OR cache models and solves the above problem as an optimization problem, which aims to find the nearly optimal set of overlapping regions that should be retained in the cache. Admittedly, solving the optimization problem is expensive and thus cannot be performed on a per-request basis. Instead, the OR cache recomputes its contents periodically, and thus mainly targets queries that have sufficient statistics in history. In contrast, the RR cache is optimized for new queries, and can react immediately to workload changes. Intuitively, the RR cache serves as a â€œbufferingâ€ region to temporarily store the cached views for recent queries, before the OR cache collects sufficient statistics to make longer-term decisions. This approach is analogous to the CStore architecture [46], where a writable row store is used to absorb newly updated data before it is moved to a highly optimized column store in batches. Collectively, the two regions offer an efficient and reactive solution for caching.

## 3   REGION PROCESSING

In this section, we focus on region matching and the creation of cached regions. Before we explain the details of the process of creating regions and matching cached regions to requests, we first show how to transform client requests into region requests.

### 3.1   API

Crystal acts as a storage layer of the DBMS. It runs outside the DBMS and transfers information via a minimalistic socket connection and shared space in the filesystem (e.g., SSDs, ramdisk). During a file request, the DBMS exchanges information about the file and the required region with Crystal. Because access to remote files is expensive, Crystal tries to satisfy the request with cached files.

The overall idea is that Crystal overwrites the accessed file path such that the DBMS is pointed to a local file. For redirecting queries, Crystal relies on query metadata such as the file path, push-down predicates, and accessed fields. Crystal evaluates the request and returns a cached local file or downloads the requested file. Afterward, the location of the local file is sent to the DBMS which redirects the scan to this local file. Crystal guarantees the completeness for a given set of predicates and fields. Internally, Crystal matches the query metadata with local cache metadata and returns a local file if it satisfies the requirements.

We use a tree string representation for push-down predicates in our API. Since predicates are conventionally stored as an AST in DBMS, we traverse the AST to build the string representation. Each individual item uses the syntax similar to *operation(left, right)*. We support binary operators, unary operators, and literals which are the leaf nodes of the tree. The binary operation is either a combination function of multiple predicates (such as *and*, *or*) or an atomic predicate (such as *gt*, *lt*, *eq*, . . . ). Atomic predicates use the same binary syntax form in which *left* represents the column identifier and *right* the compare value. To include the negation of sub-trees, our syntax allows *operation(exp)* with the operation *not*.

### 3.2   Transformation & Caching Granularity
Crystal receives the string of push-down predicates and transforms it back to an internal AST. Because arguing on arbitrarily nested logical expressions (with *and* and *or*) is hard, Crystal transforms the AST to Disjunctive Normal Form (DNF). In the DNF, all conjunctions are pushed down into the expression tree, and conjunctions and disjunctions are no longer interleaved. In Crystal, regions are identified by their disjunction of conjunctions of predicates. Regions also contain their sources (i.e., the remote files) and the projection of the schema. This allows us to easily evaluate equality, superset, and intersection between regions which we show in Section 3.3.

The construction of the DNF follows two steps. First, all negations are pushed as far as possible into the tree which results in Negation Normal Form (NNF). Besides using the De-Morgan rules to push down negations, Crystal pushes the negations inside the predicates. For example, *not(lt(id, 1))* will be changed to *gteq(id, 1)*.

After receiving the NNF, Crystal distributes conjunctions over disjunctions. The distributive law pushes *or*s higher up in the tree which results in the DNF. It transforms *and(a, or(b, c))* to *or(and(a, b), and(a, c))*. Although this algorithm could create 2*ğ‘›* leaves in theory, none of our experiments indicate issues with blow-up.

Because the tree is in DNF, the regions store the pushed-down conjunctions as a list of column restrictions. These conjunctions of restrictions can be seen as individual geometric hyper-rectangles. Regions are fully described by the disjunction of these hyperrectangles. Figure 4 shows the process of creating the DNF and extracting the individual hyper-rectangles. Although we use the term hyper-rectangles, the restrictions can have different shapes. Crystal supports restrictions, such as *noteq*, *isNull*, and *isNotNull*, that are conceptually different from hyper-rectangles.

Crystalâ€™s base granularity of items is on the level of regions, thus all requests are represented by a disjunction of conjunctions. However, individual conjunctions of different regions can be combined to satisfy an incoming region request. Some previous work on semantic caching (e.g., [14, 17]) considers only non-overlapping hyper-rectangles. Non-overlapping regions can help reduce the complexity of the decision-making process. Although this is desirable, non-overlapping regions impose additional constraints.

Splitting the requests into sets of non-overlapping regions is expensive. In particular, the number of non-overlapping hyperrectangles grows combinatorial. To demonstrate this issue, we evaluated three random queries in the lineitem space which we artificially restrict to 8 dimensions [23]. If we use these three random hyper-rectangles as input, 16 hyper-rectangles are needed to store all data non-overlapping. This issue arises from the number of dimensions that allow for multiple intersections of hyper-rectangles. 

Each intersection requires the split of the rectangle. In the worst case, this grows combinatorial in the number of hyper-rectangles. Because all extracted regions need statistics during the cache optimization phase, the sampling of this increased number of regions is not practical. Further, the runtime of the caching policies is increased due to the larger input which leads to outdated caches. 

Moreover, smaller regions require that more cached files are returned to the client. Figure 5 shows that each additional region incurs a linear overhead of roughly 50ms in Spark. The preliminary experiment demonstrates that splitting is infeasible due to the combinatorial growth of non-overlapping regions. Therefore, Crystal does not impose restrictions on the semantic regions themselves. This raises an additional challenge during the optimization phase of the oracle region cache, which we address in Section 4.5.

### 3.3   Region Matching

With the disjunction of conjunctions, Crystal determines the relation between different regions. Crystal detects equality, superset, intersections, and partial supersets relations. Partial supersets contain a non-empty number of conjunctions fully.

Crystal uses intersections and supersets of conjunctions to argue about regions. Conjunctions contain restrictions that specify the limits of a column. Every conjunction has exactly one restriction for each predicated column. Restrictions are described by their column identifier, their range (*min, max*), their potential equal value, their set of non-equal values and whether *isNull* or *isNotNull* is set. If two restrictions ğ‘*ğ‘¥* and ğ‘*ğ‘¦* are on the same column, Crystal computes if ğ‘*ğ‘¥* completely satisfies ğ‘*ğ‘¦* or if ğ‘*ğ‘¥* has an intersection with ğ‘*ğ‘¦* . For determining the superset, we first check if the null restrictions are not contradicting. Second, we test whether the (*min, max*) interval of ğ‘*ğ‘¥* is a superset of ğ‘*ğ‘¦* . Afterward, we check whether ğ‘*ğ‘¥* has restricting non-equal values that discard the superset property and if all additional equal values of ğ‘*ğ‘¦* are also included in ğ‘*ğ‘¥* .                                 

For two conjunctions ğ‘*ğ‘¥* and ğ‘*ğ‘¦* , ğ‘*ğ‘¥* âŠƒ  ğ‘*ğ‘¦* if ğ‘*ğ‘¥* only contains restrictions that are all less restrictive than the restrictions on the same column of ğ‘*ğ‘¦* . Thus, ğ‘*ğ‘¥* must have an equal number or fewer restrictions which are all satisfying the matched restrictions of ğ‘*ğ‘¦* . Otherwise, ğ‘*ğ‘¥* âŠ… ğ‘*ğ‘¦* . ğ‘*ğ‘¥* can have fewer restrictions because the absence of a restriction shows that the column is not predicated. 

In the following, we show the algorithms to determine the relation between two regions ğ‘Ÿ*ğ‘¥* and ğ‘Ÿ*ğ‘¦* .

- `ğ‘Ÿğ‘¥ âŠƒ ğ‘Ÿy`  holds if all conjunctions of ğ‘Ÿ*ğ‘¦* find a superset in ğ‘Ÿ*ğ‘¥* .
- `ğ‘Ÿğ‘¥ âˆ©  ğ‘Ÿğ‘¦ â‰  âˆ…` holds if at least one conjunction of ğ‘Ÿ*ğ‘¥* finds an intersecting conjunction of ğ‘Ÿ*ğ‘¦* .
- `âˆƒ conj âŠ‚  ğ‘Ÿğ‘¥ : conj âŠ‚  ğ‘Ÿğ‘¦` (partial superset) holds if at least one conjunctions of ğ‘Ÿ*ğ‘¦* finds a superset in ğ‘Ÿ*ğ‘¥* .
- `ğ‘Ÿğ‘¥ = ğ‘Ÿğ‘¦ : ğ‘Ÿğ‘¥ âŠƒ rğ‘¦ âˆ§ ğ‘Ÿğ‘¦ âŠƒ ğ‘Ÿğ‘¥`

Figure 6 shows an example that matches a query that consists of two hyper-rectangles to two of the stored regions.

### 3.4   Request Matching

During region requests, Crystal searches the caches to retrieve a local superset. Figure 7 shows the process of matching the request. First, the oracle region cache is scanned for matches. If the request is not fully cached, Crystal tries to match it with the requested region cache. If the query was not matched, the download manager fetches the remote files (optionally from a file cache).

During the matching, a full superset is prioritized. Only if no full superset is found, Crystal tries to satisfy the individual conjunctions. The potential overlap of multiple regions and the overhead shown in Section 3.2 are the reasons to prefer full supersets. If an overlap is detected between ğ´ and ğµ, Crystal needs to create a reduced temporary file. Otherwise, tuples are contained more than once which would lead to incorrect results. For example, it could return ğ´ and ğµ âˆ’ ğ´ to the client. The greedy algorithm, presented in Algorithm 1 reduces the number of regions if multiple choices are possible. We choose the region that satisfies most of the currently unsatisfied conjunctions and continue until all have been satisfied. 

We optimize the matching of regions by partitioning the cache according to the remote file names and the projected schema. The file names are represented as (bit-)set of the remote file catalog. This set is sharded by the tables. Similarly, the schema can be represented as a (bit-)set. The partitioning is done in multiple stages. After the fast file name superset check, all resulting candidates are tested for a superset of the schema. Only within this partition of superset regions, we scan for a potential match. Although no performance issues arise during region matching, multi-dimensional indexes (e.g., R-trees) can be used to further accelerate lookups.

### 3.5   Creating Regions

The cached regions of Crystal are stored as Apache Parquet files. Crystal leverages Apache Arrow for reading and writing snappy encoded Parquet files. Internally, Parquet is transformed into Arrow tables before Crystal creates the semantic regions.

Gandiva, which is a newly developed execution engine for Arrow, uses LLVM compiled code to filter Arrow tables [8]. As this promises superior performance in comparison to executing tuple-at-a-time filters, Crystal translates its restrictions to Gandiva filters. When Crystal builds new Parquet files to cache, the filters are compiled to LLVM and executed on the in-memory Arrow data. Afterward, the file is written to disk as snappy compressed Parquet file. If a file is accessed the first time, Crystal creates a sample that is used to predict region sizes and to speed up the clientâ€™s query planning.

### 3.6   Client Database Connector

Database systems are often able to access data from different formats and storage layers. Many systems implement a connection layer that is used as an interface between the DBMS and the different formats. For example, Spark uses such an abstraction layer known as data source.

Crystal is connected to the DBMS by implementing such a small data source connector. As DBMSs can process Parquet files already, we can easily adapt this connector for Crystal. Crystal interacts with the DBMS via a socket connection and transfers files via shared disk space or ramdisk. Since Crystal returns Parquet files, the DBMS can already process them without any code modifications.

The only additional implementation needed is the exchange of control messages. These consist of only three different messages and the responses of Crystal. One of the messages is optional and is used to speed up query planning. The scan request message and the message that indicates that a scan has finished are required by all Crystal clients. The first message includes the path of the remote file, the push-down predicates, and the required fields of the schema. Crystal replies with a collection of files that can be used instead of the original remote file. The finish message is required to delete cached files safely that are no longer accessed by the client. The optional message inquires a sample of the original data to prevent storage accesses during query planning.

### 3.7   Cloud Connection

Crystal itself also has an interface similar to the data source. This interface is used to communicate with various cloud connectors. The interface implements simple file operations, such as listings of directories and accesses to files. For blob storage, the later operation basically downloads the file from remote storage to the local node. 

Recently, cloud providers have been adding predicate push-down capabilities to their storage APIs, e.g., S3 Select [4]. Clients can push down filters to storage and receive the predicated subset. This feature can incur additional monetary costs, as well as a per-request latency. Crystal complements this feature naturally, as it is aware of semantic regions and can use predicate push-down to populate its cache efficiently. As Crystal can reuse cached results locally, it can save on future push-down costs as well.

Crystal implements a download manager that fetches blobs from remote and stores them into ramdisk. The client is pointed to this location, and as soon as it finishes accessing it, the file is deleted again. Multiple accesses can be shared by reference counting.

## 4   CACHE OPTIMIZATION

This section summarizes the architecture of our caches, followed by more details on caching. Finally, we explain our algorithms that explore and augment the overlapping search space.

### 4.1   Requested Region and Oracle Region Cache 

Recall that Crystal relies on two region caches to capture shortand long-term trends. The *RR* cache is an eager cache that stores the result of recently processed regions. The long-term insights of the query workload are captured by the *OR* cache. This cache leverages the history of region requests to compute the ideal set of regions to cache locally for best performance. Crystal allows users to plug-in a custom oracle; we provide a default oracle based on a variant of Knapsack (covered later). After the oracle determines a new set of regions to cache, Crystal computes these regions in the background and updates the *OR* cache. The creation in the background allows to schedule more expensive algorithms (runtime) to gather meaningful insights. This allows for computing (near-) optimal results and the usage of machine learning in future work. The oracle runs in low priority, consuming as little CPU as possible during high load.

An interesting opportunity emerges from the collaboration between the two caches. If the *OR* cache decided on a set of long-term relevant regions, the requested region cache does not need to compute any subset of the already cached long-term regions. On the other hand, if the requested region cache has regions that are considered for long-term usage, the *OR* cache can take control over these regions and simply move them to the new cache.

### 4.2   Metadata Management

A key component for predicting cached regions is the history of requested regions. To recognize patterns, the previously accessed regions are stored within Crystal. We use a ring-buffer to keep the most recent history. Each buffer element represents a single historic region request which has been computed by a collection of (remote) data files. These files are associated with schema information, tuple count, and size. The selectivity of the region is captured by result statistics. The database can either provide result statistics, or Crystal will compute them. Crystal leverages previously created samples to generate result statistics. In conjunction with the associated schema information, Crystal predicts the tuple count and the result size.

### 4.3   Oracle Region Cache

Long-term trends are detected by using the oracle region cache. An oracle decides according to the seen history which regions need to be created. The history is further used as a source of candidate regions that are considered to be cached.

The quality of the cached items is evaluated with the recent history of regions. Each cached region is associated with a benefit value. This value is the summation of bytes that do not need to be downloaded if the region is stored on the DBMS node. In other words, how much network traffic is saved by processing the history elements locally. Further, we need to consider the costs of storing candidate regions. The costs of a region are simply given by the size it requires to be materialized. The above caching problem can be expressed as the knapsack problem: maximize $\sum\nolimits_{i=1}^nb_ix_i$ subject to $\sum\nolimits_{i=1}^nw_ix_i \leqslant W$ Where $x_i \in \{0, 1\}$. The saved bandwidth by caching a region is denoted by ğ‘, the size of the materialized cache by ğ‘¤ . If the region is picked ğ‘¥ = 1, otherwise ğ‘¥ = 0. The goal is to maximize the benefit while staying within the capacity ğ‘Š .

However, the current definition cannot capture potential overlap in regions well. As the benefit value is static, history elements that occur in multiple regions would be added more than once to the overall value. Thus the maximization would result in a suboptimal selection of regions. In Section 4.5, we show the adaptations of our proposed algorithm to compensate for the overlapping issue.

### 4.4   Knapsack Algorithms

Dynamic programming (DP) can be used to solve the knapsack optimally in pseudo-polynomial time. The most widespread algorithm iterates over the maximum number of considered items and the cache size to solve the knapsack optimal for each sub-problem instance. Combining the optimally solved sub-problems results in the optimal knapsack, but the algorithm lies in the complexity of O( ğ‘› âˆ— ğ‘Š). Another possible algorithm iterates over the items and benefit values, and lies in O(ğ‘› âˆ— ğµ ) (ğµ denotes maximum benefit). 
In our caching scenario, we face two challenges with the DP approach. First, both ğ‘Š (bytes needed for storing the regions) and ğµ (bytes the cached element saves from being downloaded) are large. Relaxing these values by rounding to mega-bytes or gigabytes reduces the complexity, however, the instances are not solved optimally anymore. Second, the algorithm considers that each subproblem was solved optimally. To solve the overlapping issue, only one region is allowed to take the benefit of a single history element. An open question is to decide which sub-problem receives the benefit of an item that can be processed with several regions.

Since many knapsack instances face a large capacity ğ‘Š and unbound benefit ğµ, approximation algorithms were explored. In particular, the algorithm that orders items according to the benefitcost ratio has guaranteed bounds and a low runtime complexity of O(ğ‘› âˆ—ğ‘™ğ‘œğ‘”(ğ‘›)). The algorithm first calculates all benefit ratios ğ‘£ = *ğ‘/w* and orders the items accordingly. In the next step, it greedily selects the items as long as there is space in the knapsack. Thus, the items with the highest cost to benefit ratio ğ‘£ are contained in the knapsack. This algorithm solves the relaxed problem of the fractional knapsack optimal which loosens `ğ‘¥ âˆˆ {0, 1}` to `ğ‘¥ âˆˆ [0, 1]` [24].

### 4.5   Overlap-aware Greedy Algorithm

> - [ ] TODO

### 4.6   Region Augmentation

> - [ ] TODO

### 4.7   Requested Region Cache

The requested region cache is similar to a traditional cache but with semantic regions instead of pages. It decides in an online fashion whether the requested region should be cached. The algorithm must be simple to reduce decision latencies. Traditional algorithms, such as LRU and its variants, are good fits in terms of accuracy and efficiency. Besides the classic LRU cache, experiments showed the benefit of caching regions after the second (k-th in general) occurrence. With the history already available for *OR*, this adaption is simple and does not introduce additional latency. For combined *OR* and *RR* with *LRU-k*, it is beneficial to reduce the history size by the *RR/OR* split as long-term effects are captured by *OR*.

One of the biggest advantages of the *RR* cache is the fast reaction to changes in the queried workload. In comparison to the *OR* cache that only refreshes periodically, the request cache is updated constantly. This eager caching, however, might result in overhead due to additional writing of the region file. To overcome this issue, the client DBMS can simultaneously work on the raw data and provide the region as a file for Crystal; this extension is left as future work.

## 5   IMPLEMENTATION DETAILS

Crystal is implemented as a stand-alone and highly parallel process that sits between the DBMS and blob storage. This design helps to accelerate workloads across different database systems. Crystal is a fully functional system that works with diverse data types and query predicates, and is implemented in C++ for optimal performance. **Parallel Processing within Crystal.** Latency critical parts of Crystal are optimized for multiple connections. Each new connection uses a dedicated thread for building the predicate tree and matching cached files. If a file needs to be downloaded, it is retrieved by a pool of download threads to saturate the bandwidth. All operations are either implemented lock-free, optimistically, or with fine-grained shared locks. Liveness of objects and their managed cached files is tracked with smart pointers. Therefore, Crystal parallelizes well and can be used as a low latency DBMS middleware.

Crystal also handles large files since some systems do not split Parquet files into smaller chunks. During matching we recognize which parts of the original file would have been read and translate it to the corresponding region in the cached files. Further, we are able to parallelize reading and processing Parquet files.

**Spark Data Source.** For our evaluation, we built a data source to communicate between Spark and Crystal, by extending the existing Parquet connector of Spark with less than 350 lines of Scala code. The connector overrides the scan method of Parquet to retrieve the files suggested by Crystal. Because Spark pushes down predicates to the data source, we have all information available for using the Crystal API. As Spark usually processes one row iterator per file, we developed a meta-iterator that combines multiple file iterators transparently (Crystal may return multiple regions). The connector is packaged as a small and dynamically loaded Java jar.

**Greenplum Data Source.** Further, we built a connector for Greenplum which is a cloud scale PostgreSQL derivative with an external extension framework â€“ called PXF [34, 51]. PXF allows one to access Parquet data from blob storage [52]. We modified the Parquet reader such that it automatically uses Crystal if available. Our changes to the Greenplum connector consist of less than 150 lines of code. Without recompiling the core database, Crystal accelerates Greenplum by dynamically attaching the modified PXF module.

Both connectors currently do not support sending regions back to Crystal; instead, Crystal itself handles additions to the RR cache.

**Azure Cloud Connection.** We use Azure Blob Storage to store remote data, using a library called azure-storage-cpplite [37] to implement the storage connector. The library just translates the file accesses to CURL (HTTPS) requests. Other cloud providers have similar libraries with which connections can be easily established. Crystal infers the cloud provider from the remote file path. The file path also gives insights into the file owner (user with pre-configured access token) and the blob container that includes the file.

## 6   EXPERIMENTAL EVALUATION

## 7   RELATED WORK

The basic idea behind Crystal is to cache and reuse computations across multiple queries. This idea has been explored in a large body of research work including at least four broad lines of research: materialized view, semantic caching, intermediate results reusing, and mid-tier database caching. In general, Crystal differs from previous work in some or all of the following ways: 1) integrating Crystal with a DBMS requires no modification to the DBMS; 2) Crystal focuses on caching views at the storage layer, and can be used across multiple DBMSs; 3) Crystal can automatically choose cached views based on a replacement policy, which takes into account the semantic dependencies among queries. Below, we discuss the key differences between Crystal and previous work in each line of the four aforementioned research areas.

**Materialized View.** Materialized view is a well-known technique that caches the results of a query as a separate table [20, 44, 45]. However, unlike Crystal, views that need to be cached or materialized are often defined manually by users (e.g., a DBA). Additionally, implementing materialized views in a DBMS is a timeconsuming process, requiring advanced algorithms in the query optimizer to decide: 1) if a given query can be evaluated with a materialized view; and 2) if a materialized view needs to be updated when the base table is changed.

**Semantic Caching.** Semantic caching was first proposed in Postgres [47], and was later extended and improved by a large body of work [11, 14, 17, 29, 30, 42, 43]. This technique also aims to cache the results of queries to accelerate repeated queries. Similarly to Crystal, a semantic cache can automatically decide which views to keep in the cache, within a size budget. This decision is often made based on a cost-based policy that takes several properties of views into consideration such as size, access frequency, materialization cost. However, this approach caches the end results of entire queries, while Crystal caches only the intermediate results of the selection and projection operators of queries. The cached view of an entire query is especially beneficial for repeated queries, but on the other hand decreases the reusability of the cached view, i.e., the chance that this view can be reused by future queries. While most work in this area does not take into account overlap of cached views, some work [14, 17] does explore this opportunity. Dar et al. proposed to split overlapping queries into non-overlapping regions, and thus enable semantic cache to use traditional replacement policies to manage the (non-overlapping) regions [14]. However, this approach could result in a large number of small views, incurring significant overhead to process as we showed in Sec 3.2. Maintaining nonoverlapping views is also expensive, as access to an overlapping view may lead to splitting the view and rewriting the cached files. Chunk-based semantic caching [17] was proposed to solve this problem, by chunking the hyper space into a large number of regions that are independent to queries. However, the chunking is pre-defined and thus is static with respect to the query patterns.

**Intermediate Results Reusing.** Many techniques have also been developed to explore the idea of reusing intermediate results rather than end results of queries. Some of these techniques [49, 50] share the intermediate results across concurrent queries only, and thus impose limitations on the temporal locality of overlapping queries. Other work [19, 25â€“27, 38, 41] allows intermediate results to be stored so that they can be reused by subsequent queries. Similarly to Crystal, these techniques also use a replacement policy to evict intermediate results when the size limit is reached. However, these techniques require extensive effort to be integrated with a DBMS, whereas integrating Crystal requires only a lightweight database-specific connector. Additionally, a Crystal cache can be used with and share data across multiple DBMSs.

**Mid-tier Database Caching.** Another area where views can be cached and reused is in the context of multi-tier database architecture, where mid-tier caches [2, 10, 31] are often deployed at the mid-tier application servers to reduce the workload for the backend database servers. As mid-tier caches are not co-located with DBMSs, they usually include a shadow database at the mid-tier servers that mirrors the backend database but without actual content, and rely on materialized views in the shadow database to cache the results of queries. Unlike Crystal, the definition of the cached views in a mid-tier cache needs to be pre-defined manually by users, and it is difficult to change the cached views adaptively.

Finally, many vendors have developed cache solutions for big data systems to keep hot data in fast local storage (e.g., SSDs). Examples include the Databricks Delta Cache [9, 15], the Alluxio [1] analytics accelerator, and the Snowflake Cache Layer [13]. These solutions are based on standard techniques that simply cache files at the page or block level and employ standard replacement policies such as LRU. Compared to these standard approaches, Crystal is also a generic cache layer that can be easily integrated with unmodified big data systems, but has the flexibility to cache data in a more efficient layout (i.e., re-organizing rows based on queries) and format (i.e., Parquet), which speeds up subsequent query processing.

## 8   CONCLUSION

Cloud analytical databases employ a disaggregated storage model, where the elastic compute layer accesses data on remote cloud storage in columnar formats. Smart caching is important due to the high latency and low bandwidth to remote storage and the limited size of fast local storage. Crystal is a smart cache storage system that colocates with compute and can be used by any unmodified database via data source connector clients. Crystal operates over semantic data regions, and continuously adapts what is cached locally for maximum benefit. Results show that Crystal can significantly improve query latencies on unmodified Spark and Greenplum, while also saving on bandwidth from remote storage.

