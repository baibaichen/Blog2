# Alibaba Hologres

1. 它们作为重要功能立即在线提供。将传入的用户事件与这些特征结合起来，生成样本，用于搜索和推荐系统中的实时模型训练。
2. 在复杂的交互分析中，数据科学家也使用它们来获得模型调整和营销操作的见解。

这些使用模式清楚地展示了许多新的趋势，传统的联机分析处理（OLAP）概念已经不能准确地涵盖这些趋势：

**分析处理与服务的融合**。传统的OLAP系统通常在整个业务栈中扮演相当静态的角色。他们分析大量数据并获得知识（例如，预计算视图、学习模型等）离线，但将**衍生知识**交给另一个系统，以便为在线应用程序提供服务。不同的是，现代商业决策是一个不断调整的在线过程。**衍生知识**不仅要提供服务，而且还参与复杂的分析。分析处理和大数据服务的需求融合在一起。

**在线和离线分析的融合**。现代企业需要快速地将新获得的数据转化为见解。写入的数据必须能在几秒钟内读取。<u>一个冗长的离线ETL过程不再是可以忍受的</u>。此外，在收集的所有数据中，传统的同步OLTP系统数据的方法只占很小的一部分。数量级更多的数据来自事务性较少的场景，如用户点击日志。在处理查询时，系统必须以非常低的延迟处理高容量数据摄取。

现有的大数据解决方案通常通过混合不同系统来承载服务和分析处理工作负载。例如，使用 Flink 等系统提取数据进行实时预聚合，或将数据填充在 Druid 等多维分析系统中，并在 Cassandra 等系统中提供服务。这不可避免地导致过多的数据重复和复杂的跨系统数据同步，抑制了应用程序立即对数据采取行动的能力，并产生了不小的开发和管理开销。

在本文中，我们认为应该在单个系统中统一和处理混合服务/分析处理 (HSAP)。 在阿里巴巴，我们构建了一个名为 Hologres 的云原生 HSAP 服务。 作为一种新的服务范式，HSAP 面临着与现有大数据栈截然不同的挑战（详细讨论见 2.2 节）： (1) 系统需要处理远高于传统 OLAP 系统的查询工作负载。 这些工作负载是混合的，具有非常不同的延迟和吞吐量权衡。 (2) 在处理高并发查询工作量的同时，系统也需要跟上高吞吐量的数据摄取。 摄取的数据需要在几秒钟内可供读取，以满足服务和分析作业的严格新鲜度要求。 (3) 混合工作负载是高度动态的，通常会受到突然爆发的影响。 该系统必须具有高度弹性和可扩展性，能够迅速对这些突发事件做出反应。

为了应对这些挑战，Hologres 对系统设计进行了彻底的重新思考：

**存储设计**。 Hologres 采用了一种将存储与计算分离的架构。 数据远程保存在云存储中。 Hologres 管理表组中的表，并将一个表组划分为多个分片。 每个分片都是独立的，并且独立管理读写。 与物理worker节点解耦，数据分片可以在worker之间灵活迁移。 以数据分片作为Hologres的基本数据管理单元，故障恢复、负载均衡、集群横向扩展等流程可以通过分片迁移高效实现。

为了同时支持高吞吐量写的低延迟查，分片被设计为具有==版本控制==。每个表组分片上的读写关键路径是分开的。Hologres 使用 tablet 结构来统一存储表格。tablet 可以是行格式或列格式，并且都以类似 LSM 的方式进行管理，以最大限度地提高写入吞吐量，并最大限度地减少数据接收的新鲜度延迟。

**并发查询执行**。我们构建了一个面向服务的资源管理和调度框架，命名为HOS。 HOS 使用执行上下文作为系统线程之上的资源抽象。 执行上下文协同调度，上下文切换开销很小。 HOS 通过将查询划分为细粒度的工作单元并将工作单元映射到执行上下文来并行化查询执行。 这种架构可以充分利用高硬件并行性的潜力，允许我们并发地多路复用大量查询。 执行上下文还促进了资源隔离的实施，这样低延迟的服务工作负载可以与同一系统中的分析工作负载共存而不会停滞。 HOS 使系统可以根据实际工作负载轻松扩展。

回想起来，我们列出了以下贡献：

1. 本文介绍了一种新的面向服务/分析处理（HSAP）的大数据服务模式，并指出了这种模式下的新挑战。
2. 我们设计并实现了一个名为 Hologres 的云原生 HSAP 服务。 Hologres 具有新颖的存储设计，以及名为 HOS 的高效资源管理和调度层。 这些新颖的设计结合起来帮助 Hologres 实现实时摄取、低延迟服务、交互式分析处理，并且还支持与其他系统（如 PostgreSQL [12]）的联合查询执行。

3. 我们在阿里巴巴的内部大数据堆栈以及公共云产品中部署了 Hologres，并在实际工作负载下进行了彻底的性能研究。 我们的结果表明，即使与专门的服务系统和 OLAP 引擎相比，Hologres 也能实现卓越的性能。

# 2. 关键设计考虑因素

现代企业中的大数据系统正面临着越来越高的混合服务和分析处理的要求。在本节中，我们使用阿里巴巴的推荐服务演示了一个典型的HSAP 场景，并总结了 HSAP 给系统设计带来的新挑战。然后，我们提供了 Hologres 如何应对这些挑战的系统概述。

## 2.1 HSAP 实战

现代推荐服务非常注重反映实时用户趋势，提供个性化的推荐。为了实现这些目标，后端大数据栈已经演变成一种具有极端复杂性和多样化数据处理模式的状态。图1展示了阿里巴巴电子商务平台中支持推荐服务的大数据栈的示意图。

> TODO: 图一

为了捕获个性化的实时行为，推荐服务在很大程度上依赖于实时特征和不断更新的模型。通常有两种类型的实时功能：

1. 该平台积极收集大量实时事件，包括日志事件（例如页面浏览量、用户点击）以及交易（例如从 OLTP 数据库同步的付款）。正如我们从生产中观察到的，这些事件的数量非常大，其中大多数是没有事务的日志数据，例如，1000000 个事件/秒。这些事件会立即被摄取到数据栈 (a) 中以备将来使用，但更重要的是，它们会与各种维度数据动态结合以获得有用的特征 (1)，并将这些特征实时输入推荐系统.这种实时连接需要以极低的延迟和高吞吐量对维度数据进行点查找，以跟上摄取的速度。 

2. 该平台还通过聚合滑动窗口中的实时事件，沿各种维度和时间粒度，例如 5 分钟项目点击、7 天页面浏览量和 30 天周转率，衍生出许多功能。这些聚合根据滑动窗口粒度以批处理 (2) 或流方式执行，并被摄取到数据堆栈 (b) 中。 

这些实时数据还用于生成训练数据，通过在线和离线训练不断更新推荐模型。

尽管其重要性，上述过程只是整个管道的一小部分。有一整套的监控、验证、分析和优化过程来支持推荐系统。其中包括但不限于对收集的事件进行<u>连续仪表板查询</u>（3）以监控关键性能指标并指导 A/B 测试，以及定期批量查询（4）以生成 BI 报告。此外，数据科学家不断对收集到的数据进行复杂的交互分析，以获得对商业决策的实时洞察，对模型进行因果分析和细化。例如，在双11购物节上，传入的 OLAP 查询请求每秒可以增加数百个查询。

以上展示了一个高度复杂的 HSAP 场景，从实时摄取（a）到批量加载（b），从服务工作负载（1），持续聚合（3），到交互式分析（4），一直到批处理分析（2）。 如果没有统一的系统，上述场景就必须由多个孤立的系统共同服务，例如像 Hive 这样的系统进行批量分析； 通过 Cassandra 之类的系统为工作负载提供服务； 由 Druid 等系统持续聚合； 通过 Impala 或 Greenplum 等系统进行交互式分析。

## 2.2 HSAP服务的挑战

作为一种新的大数据服务模式，HSAP 服务提出了几年前还不那么突出的挑战。

**高并发的混合查询工作负载**。 HSAP 系统通常面临着传统 OLAP 系统中前所未有的高查询并发性。在实践中，与 OLAP 查询工作负载相比，服务查询工作负载的并发性通常要高得多。例如，我们在实际应用中观察到，服务查询的到达率可能高达每秒 1000000 次查询 (QPS)，比 OLAP 查询的 QPS 高五个数量级。此外，服务查询的延迟要求比 OLAP 查询要严格得多。如何在实现这些不同的查询 SLO 的同时，复用它们以充分利用计算资源确实具有挑战性。

> **服务级别目标**（Service-level objective，SLO）是指[服务提供者](https://zh.wikipedia.org/wiki/服务提供者)向[客户](https://zh.wikipedia.org/wiki/顧客)作出的[服务保证](https://zh.wikipedia.org/wiki/服務保證)的量化指标。服务级别目标与[服务级别协议](https://zh.wikipedia.org/wiki/服务级别协议)有所不同。[服务级别协议](https://zh.wikipedia.org/wiki/服务级别协议)是指[服务提供者](https://zh.wikipedia.org/wiki/服务提供者)向客户保证会提供什么样的服务，服务级别目标则是服务的量化说明[[1\]](https://zh.wikipedia.org/wiki/服务级别目标#cite_note-:1-1)。例如软件提供商向客户保证一年的时间内有99.95%的时间应用程序不会出现故障，或是一个月以内75％的拨打的呼叫中心求助电话将在一分钟内得到答复。这就是一种典型的服务级别目标说明。

现有的 OLAP 系统一般采用基于进程/线程的并发模型，即使用单独的进程 [5] 或线程 [6] 来处理查询，并依赖操作系统来调度并发查询。这种设计带来的昂贵的上下文切换对系统并发性提出了硬性限制，因此不再适用于 HSAP 系统。并且它会阻止系统有足够的调度控制来满足不同的查询 SLO。

**高吞吐量实时数据摄取**。 在处理高并发查询工作负载的同时，HSAP 系统还需要处理高吞吐量的数据摄取。 在所有摄取的数据中，传统的从 OLTP 系统同步数据的方式只占很小的一部分，而大部分数据来自各种数据源，例如实时日志数据，没有很强的事务语义。 摄取量可能比在混合交易分析处理 (HTAP) 系统中观察到的要高得多。 例如，在上述场景中，<u>摄取率高达每秒数千万个元组</u>。 更重要的是，与传统的 OLAP 系统不同，HSAP 系统需要实时数据摄取——写入的数据必须在亚秒内可见——以保证分析的数据新鲜度。

**弹性和可扩展性**。 摄取和查询工作负载可能会突然爆发，因此要求系统具有弹性和可扩展性，并及时做出反应。 我们在实际应用中观察到，峰值摄取吞吐量达到平均值的 2.5 倍，峰值查询吞吐量达到平均值的 3 倍。 此外，摄取和查询工作负载的爆发不一定一致，这需要系统独立扩展存储和计算。

## 2.3 数据存储

在本小节中，我们将讨论 Hologres 中数据存储的 <u>High-Level</u> 设计。

**存储/计算的解耦**。 Hologres 采用云原生设计，其中计算层和存储层分离。 Hologres 的所有数据文件和日志默认都持久化在盘古中，盘古是阿里云的一个高性能分布式文件系统。我们还支持开源分布式文件系统，例如 HDFS [3]。通过这种设计，计算层和存储层都可以根据工作负载和资源可用性独立扩展。

**基于 *tablet* 的数据布局**。在 Hologres 中，表和索引都被划分为细粒度的 ***tablet***。一个写请求被分解成许多小任务，每个小任务处理单个 ***tablet*** 的更新。相关表和索引的 tablets 进一步分组为分片，以提供有效的一致性保证。为了减少争用，我们使用无锁设计，每个 ***tablet*** 由单个 `writer` 管理，但可以有任意数量的 `reader`。我们可以为查询工作负载配置非常高的读取并行度，从而隐藏从远程存储读取所产生的延迟。

**读写分离**。 Hologres 将读写路径分开，同时支持高并发读取和高吞吐量写入。***tablet*** 的 `writer` 使用类似 LSM 的方法来维护 ***tablet*** 的 ==image==，已正确版本化其的记录。对于具有亚秒级延迟的读取，可以看到新写入。并发读取可以请求特定版的 ***tablet*** 的 ==image==，因此不会被写入阻止。

## 2.4 并发执行

在本小节中，我们将讨论 Hologres 中调度机制的 <u>High-Level</u> 设计。

**执行上下文**。 Hologres 构建了一个调度框架，简称 **HOS**，它提供了一个称为<u>**执行上下文**</u>的用户空间线程来抽象系统线程。执行上下文是超轻量级的，可以以微不足道的成本创建和销毁。 HOS 在系统线程池之上<u>==协作调度==</u>执行上下文，上下文切换开销很小。执行上下文提供异步任务接口。 HOS 将用户的读写查询划分为细粒度的工作单元，并将工作单元映射到执行上下文上进行调度。这种设计还使 Hologres 能够对突然的工作负载突发做出迅速反应。系统可以在运行时弹性伸缩。

**可定制的调度策略**。 HOS 将调度策略与基于执行上下文的调度机制解耦。 HOS 将来自不同查询的执行上下文分组到调度组中，每个组都有自己的资源**份额**。 HOS 负责监控每个调度组的消耗份额，并在调度组之间实施资源隔离和公平性。

## 2.5 系统概述

图 2 展示了 Hologres 的系统概览。前端节点（FE）接收客户端提交的查询并返回查询结果。对于每个查询，FE 节点中的查询优化器生成一个查询计划，并将其并行化为**片段实例的 DAG**。协调器将查询计划中的片段实例分派给**工作节点**，每个节点将片段实例映射到**工作单元**（第 4.1 节），**工作节点**是物理资源的单位，即 CPU 核和内存。每个工作节点可以为一个数据库保存多个<u>==表组==</u>分片（第 3.2 节）的内存表。在工作节点中，**工作单元**<u>作为 EC 池中的执行上下文</u>执行（第 4.2 节）。 HOS 调度程序按照预先配置的调度策略（第 4.5 节）在系统线程（第 4.3 节）之上调度 EC 池。

> TODO: 图2

**资源管理器**在工作节点之间分配<u>**表组**</u>分片：工作节点中的资源在逻辑上被划分为**槽**，每个**槽**只能分配给一个<u>==表组==</u>分片。资源管理器还负责在 Hologres 集群中添加/删除工作节点。工作节点定期向资源管理器发送心跳。在集群中出现工作节点故障或工作负载突发时，资源管理器会动态地将新的工作节点添加到集群中。

**存储管理器**维护一个<u>**表组**</u>分片目录（参见第 3.1 节），以及它们的元数据，例如物理位置和键范围。每个协调器缓存这些元数据的本地副本，以方便调度查询请求。


Hologres 允许执行单个查询以跨越 Hologres 和其他查询引擎（第 4.2.3 节）。例如，当片段实例需要访问未存储在 Hologres 中的数据时，协调器会将它们分发到存储所需数据的其他系统。我们设计并实现了一套用于查询处理的统一 API，以便在 Hologres 中执行的工作单元可以与其他执行引擎（如 PostgreSQL [12]）进行通信。非 Hologres 执行引擎拥有独立于 Hologres 的查询处理和调度机制。

# 3. 存储

Hologres 支持为 HSAP 场景量身定制的<u>**行列混合存储布局**</u>。 行存储针对<u>低延迟点查找</u>进行了优化，列存储旨在执行<u>高吞吐量</u>的列扫描。 在本节中，我们将介绍 Hologres 中混合存储的详细设计。 我们首先介绍**数据模型**并定义一些初步概念。 接下来，我们介绍<u>==表组==</u>分片的内部结构，并详细解释如何进行写入和读取。 最后，我们介绍了行和列存储的布局，并简要介绍了 Hologres 中的缓存机制。

## 3.1 数据模型

在 Hologres 中，每个表都有一个用户指定的**集群键**（如果未指定则为空）和一个唯一的**<u>行定位符</u>**。 如果**集群键**唯一，则直接作为行定位器； 否则，将唯一符附加到**集群键**以生成行定位符，即，**<u>==<clustering_key, uniquifier>==</u>**。

一个数据库的所有表都被分配到<u>==表组==</u>中。一个<u>==表组==</u>被分为若干个<u>==表组分片==</u>(TGS)，其中每个 TGS 包含每个表的<u>基本数据分区和所有相关索引的分区</u>。我们将**基本数据分区**和**索引分区**统一视为一个 ***tablet***。***tablet*** 有两种存储格式：行 ***tablet*** 和列 ***tablet***，分别针对点查找和顺序扫描进行了优化。基本数据和索引可以存储在行 ***tablet***、列 ***tablet*** 或两者即可。***tablet*** 需要有唯一 ***key***。因此，基本数据 ***tablet*** 的 ***key*** 是**行定位符**。而对于二级索引的 ***tablet***，如果索引是唯一的，则以索引列作为 ***tablet*** 的 key；否则，key 是通过将**行定位符**添加到<u>**索引列**</u>来定义。例如，考虑一个带有单个表和两个二级索引的 TGS——一个唯一的二级索引 (k~1~ -> v~1~) 和一个非唯一的二级索引 (k~2~ -> v~2~) —— 并且基本数据存储在行和列的 ***tablet*** 中。如上所述，基本数据（行和列）***tablet*** 的 key 是 **<u>==<row_locator>==</u>** ，唯一索引 ***tablet*** 的 key 是 k~1~ ，非唯一索引 ***tablet*** 的  key 是 <u>**==<k~2~, row_locator>==**</u> 。

<u>我们观察到数据库中的大多数**写操作**会访问几个密切相关的表，更新单表会同时更新基础数据和相关索引</u>。通过将表分组为<u>表组</u>，我们可以将在 TGS 中对不同 ***tablet*** 的相关写入视为一次**原子写入操作**，并且只在文件系统中<u>保留一个日志条目</u>。 这个机制通过减少日志刷新次数来提高写入效率。 此外，对经常<u>关联的表</u>进行分组，有助于消除不必要的数据 Shuffle。

## 3.2 表组分片

TGS 是 Hologres 中数据管理的基本单元。一个 TGS 主要包括一个 WAL 管理器和属于这个 TGS 中 table shards 的多个 ***tablets***，如图 3 所示。

> 图3

***Tablet*** 统一管理为 LSM 树：每个 Tablet 由工作节点内存中的<u>内存表</u>和一组持久化在分布式文件系统中==不可变==的<u>**分片文件**</u>组成。内存表作为分片文件定期刷新。分片文件被组织成多层， Level~0~, Level~1~, ..., Level~N~ 。在 Level~0~ 中，每个分片文件对应一个刷新的内存表。从 Level~1~ 开始，该层的所有记录按 key 排序，并划分到不同的shard文件中，因此同一层不同 shard 文件的key 范围不重叠。 Level~i+1~ 可以容纳比 Level~i~ 多 K 倍的分片文件，每个分片文件的最大大小为 M。行和列 **tablets** 的更多细节分别在第 3.3 和 3.4 节中解释。

***Tablet*** 还维护一个元数据文件，用于存储其分片文件的状态。元数据文件按照与 RocksDB [13] 类似的方法进行维护，并保存在文件系统中。

**由于记录是版本化的，因此 TGS 中的读取和写入完全解耦**。最重要的是，我们采用无锁方法， WAL 只允许单个 `writer`，但 TGS 上同时允许任意数量的 reader。由于 HSAP 场景的一致性要求比 HTAP 弱，因此 Hologres 选择只支持<u>原子写</u>和 <u>read-your-writes 读</u>，以实现读写的高吞吐量和低延迟。接下来，我们详细解释一下如何执行读写操作。

### 3.2.1 TGSs 中的写

Hologres 支持两种类型的写入：<u>写入单个分片</u>和<u>分布式批量写入</u>。 两种类型的写入都是原子的，即要么成功提交，要么失败回滚。单个分片写入一次更新一个 TGS，并且能以极高的速率执行。 另一方面，分布式批量写入用于将大量数据作为单个事务转储到多个 TGS 中，并且通常执行的频率要低得多。

**写入单个分片**。 如图 3 所示，<u>在接收到单个分片摄取时</u>，WAL 管理器 (1) 为写入请求分配一个 LSN，该 LSN 由时间戳和递增的序列号组成，并且 (2)创建一个新的日志条目并将其持久化到文件系统中。日志条目包含重放记录的写入所需的信息。在日志条目完全持久化后<u>**提交写入**</u>。 此后，（3）写请求中的操作被应用到对应 tablet 的内存表中，并且对新的读请求可见。值得注意的是，不同 tablet 上的更新可以并行化（参见第 4.1 节）。 一旦<u>**内存表**</u>已满，（4）则将其作为一个新的分片刷新到文件系统中，并初始化一个新的<u>**内存表**</u>。（5）最后，分片文件在后台异步<u>**合并**</u>。 在<u>**合并**</u>或内存表刷新结束时，tablet 的元数据文件会相应更新。

> 这里的**<u>合并</u>**就是 compaction

**分布式批量写入**。我们采用两阶段提交机制来保证分布式批量写入的写入原子性。接收批量写入请求的 FE 节点锁定所有涉及 TGS 中访问的 ***tablets***。然后每个 TGS：(1) 为该批写入分配一个 LSN，(2) 刷新相关 ***tablets*** 的内存表，（3）像写入单个分片的过程那样加载数据，并将其刷新为分片文件。请注意，步骤（3）可以通过构建多个内存表并将它们**==并行==刷新到文件系统**中来进一步优化。 完成后，每个 TGS 向给 FE 节点投票。当 FE 节点从参与的 TGS 收集到所有投票后，它会确认最终的决定<u>**是提交还是中止**</u>。 收到提交决定后，每个 TGS 都会保留一个日志，指示该批写入已提交； 否则，将删除本次批写入期间新生成的所有文件。 当两阶段提交完成后，释放相关 ***tablets*** 上的锁。

### 3.2.2 TGSs 中的读

Hologres 支持行和列 ***tablets*** 的多版本读取。读请求的一致性级别是 `read-your-writes`，即客户端总是看到自己最新提交的写操作。每个读请求都包含一个**读时间戳**，用于构造一个 LSN~read~。 此 LSN~read~ 用于过滤掉此读取不可见的记录，即 LSN 大于 LSN~read~ 的记录。

为了便于多版本读取，TGS 为每个表维护一个 LSN~ref~ ，它存储该表在 ***tablets*** 中最老的 LSN。 LSN~ref~ 根据用户指定的<u>保留周期</u>定期更新。 在内存表刷新和文件合并期间，对于给定的键： (1) 合并 LSN 等于或小于 LSN~ref~ 的记录； (2) LSN 大于 LSN~ref~ 的记录保持完整。

### 3.2.3 分布式 TGS 管理
当前实现，TGS 的 `writer` 和所有 `reader` 位于同一个工作节点中，以共享此 TGS 的内存表。如果工作节点正在经历工作负载突发，Hologres 支持从过载的工作节点迁移一些 TGS（参见第 4.4 节）。

我们正在研究一种解决方案，该解决方案将 TGS 只读远程副本<u>维护</u>到相应的 `writer`，以进一步平衡**并发读操作**。我们计划支持两种类型的只读副本：(1) **完全同步的副本**有 TGS 内存表和元数据文件的最新副本，可以为所有读请求提供服务； (2) 部分同步的副本只有元数据文件的最新副本，并且只能读取刷新到文件系统中的数据。对 TGS 的读取可以根据它们的读取版本分派到不同的副本。**<u>请注意，两个只读副本都不需要复制分片文件，如果需要，可以从分布式文件系统加载分片文件</u>**。

如果 TGS 失败，存储管理器向资源管理器请求可用槽位，同时向所有协调器广播 `TGS-fail` 消息。在恢复 TGS 时，我们从最新刷新的 LSN 中重放 WAL 日志以重建其内存表。一旦所有内存表完全重建，恢复就完成了。之后，存储管理器得到确认，然后向所有协调器广播包含新位置的 `TGS-recovery` 消息。协调器暂时保留对失败的 TGS 的请求，直到它被恢复。

## 3.3 Row Tablet

经过优化的行 ***tablets*** 支持高效的点查询。图 4(a) 说明了行 ***tablets*** 的结构：内存表是 Masstree [30]，按 Key 对记录排序。不同的是，分片文件是**按块的结构**。分片文件由两种类型的块组成：数据块和索引块。分片文件中的记录按 Key 排序。连续记录分组为一个数据块。为了便于通过 Key 查找记录，我们进一步跟踪每个数据块的起始 Key 及其在分片文件中的偏移量，索引块中按`<key, block_offset>` 保存。

为了支持多版本数据，存储在行 ***tablets*** 中的值被扩展为`<value_cols, del_bit, LSN>`：(1) *value_cols* 是不包含 Key 的列值； (2) *del_bit* 表示是否为删除记录； (3) *LSN* 为对应写入时的LSN。给定一个 Key，内存表和分片文件都可以有多个不同 LSN 的记录。

> 图 4

**从行 <u>*tablets*</u> 中读取**。行 ***tablets*** 中的每个读取都由一个 Key 和一个 LSN~read~ 组成。通过并行搜索 ***tablet*** 中的内存表和分片文件得到结果。<u>仅搜索 Key 的范围与给定 Key 重叠的分片文件</u>。搜索过程中，如果一条记录包含给定的 Key 并且具有等于或小于 LSN~read~ 的 LSN，则将该记录标记为**候选记录**。候选记录按照 LSN 的顺序合并结果记录，如果结果记录的 `del_bit` 为 1，或者没找到候选记录，则 LSN~read~ 版本中不存在给定键的记录。否则返回结果记录。

**写入行 <u>*tablets*</u>**。在行 ***tablets*** 中，<u>插入或更新</u>由键、列值和 LSN~write~ 组成。删除包含一个键、一个特殊的删除标记和一个 LSN~write~。每次写入都被转换成行 ***tablets*** 的键值对。对于插入和更新，`del_bit` 设置为 0。对于删除，列字段为空，`del_bit` 设置为 1。键值对首先追加到内存表中。一旦内存表满了，它就会作为 Level~0~ 的分片文件刷新到文件系统。如果 Level~i~ 已满，这可能会进一步触发从 Level~i~ 到 Leve~li+1~ 的级联合并。

## 3.4 Column Tablet

列 ***tablets*** 用于扫描。如图 4(b) 所示，与行 ***tablets*** 不同，列 ***tablets*** 由两个组件组成：**列 LSM 树**和**删除映射**。

存储在**列 LSM 树**中的值按以下格式扩展： `<value_cols, LSN>` ，其中 `value_cols` 是不包含 Key 的列值，`LSN` 是写入时的 `LSN`。==在列 LSM 树中，内存表以 Apache Arrow [2] 的格式存储记录==，记录按到达顺序连续添加到内存表中。在分片文件中，记录按 Key 排序并在逻辑上分成**行组**。行组中的每一列都存储为一个单独的数据块。同一列的数据块连续存放在分片文件中，方便顺序扫描。**<u>元数据块</u>**维护了每一列和整个分片文件的元数据，以加快大规模数据检索。**<u>元数据块</u>**存储：（1）对于每一列，数据块的偏移量，每个数据块的取值范围和编码方案，以及（2）对于分片文件，压缩方案，总行数，LSN和键范围。为了快速定位行，我们在索引块中保存了<u>**行组**</u>中的第一个 Key。

> 注意：**行组**有序

<u>==删除映射==</u>是行 ***tablets*** ，其中 key 是列 LSM 树中分片文件的 ID（内存表被当作特殊的分片文件处理），值是一个位图，指示在分片文件相应的 LSN 处新删除了哪些记录。在**删除映射**的帮助下，可以大规模并行顺序扫描列 ***tablets*** ，如下所述。

**从列 <u>*tablet*</u> 中读取**。对列 ***tablet*** 的读取操作由目标列和 LSN~read~ 组成。通过扫描内存表和所有分片文件得到读取结果。在扫描分片文件之前，我们将其 LSN 范围与 LSN~read~ 进行比较： (1) 如果其**==最小== LSN 大于 LSN~read~** ，则跳过该文件；(2) 如果其**==最大== LSN 等于或小于 LSN~read~** ，则整个分片文件在本次读取中可见； (3) 否则，在本次读取中，该文件只有部分记录可见。第三种情况，我们扫描此文件的 LSN 列并生成一个 LSN 位图，指示哪些行在读取版本中可见。为了过滤掉分片文件中被删除的行，我们使用分片文件的 ID 作为键，在**删除映射**中读取（如第 3.3 节所述）版本为 LSN~read~ 的位图 ，并 `union` 所有候选位图。得到的位图与 LSN 位图<u>相交</u>，再和目标数据块关联，过滤掉本次读取中被删除和不可见的行。请注意，与行 ***tablets*** 不同，在列 ***tablets*** 中，可以独立读取每个分片文件，而无需与其他 level 的分片文件合并，因为**删除映射**可以有效地知道分片文件中直到 LSN~read~ 为止的所有已删除行。

**写入列 <u>*tablets*</u>**。在列 ***tablets*** 中，插入操作由一个键、一组列值和一个 LSN~write~ 组成。 删除操作指定了要删除行的 Key，通过它可以快速找到包含该行的文件 ID 及其在该文件中的**<u>行号</u>**。 在**删除映射**中，按版本 LSN~write~ 执行插入，其中 Key 是文件 ID，值是已删除行的**<u>行号</u>**。 更新操作被实现为删除后跟插入。 对<u>列 LSM 树</u>和**<u>删除映射</u>**的插入可以触发内存表刷新和分片文件合并。

## 3.5 层次化缓存

Hologres 采用分层缓存机制来降低 I/O 和计算成本。 缓存一共有三层，分别是本地磁盘缓存、块缓存和行缓存。 每个 tablet 对应**一组存储在分布式文件系统中的分片文件**。**本地磁盘缓存**用于将分片文件缓存在本地磁盘（SSD）中，以减少文件系统中昂贵的 I/O 操作的频率。 在 SSD 之上，内存中的**块缓存**用于存储最近从分片文件中读取的块。由于服务和分析工作负载具有非常不同的数据访问模式，我们在物理上隔离了行和列 ***tablets*** 的块缓存。 在**块缓存**之上，我们进一步维护了一个内存中的行缓存，以存储行 ***tablet*** 中最近点查找的合并结果。

# 4. 查询处理和调度
在本节中，我们将介绍 Hologres 的并行查询执行范式和 HOS 调度框架。 

## 4.1 高度并行地执行查询

图 5 展示了 Hologres 中处理查询的流程。收到查询时，FE 节点中的**查询优化器**生成一个表示为 DAG 的查询计划，并在 shuffle 边界将 DAG 划分为 **fragments**，有三种类型的 **fragment**：**读/写/查询** **fragments**，包含访问表的<u>读/写运算符</u>的**读/写 fragments**，以及不包含<u>读/写运算符</u>的**查询 fragments**。然后以数据并行的方式将每个 **fragment** 并行化为多个 **fragment** 实例，例如，每个读/写 **fragment** 实例处理一个 TGS。

> 图 5 


FE 节点将查询计划转发给协调器。然后协调器将 **fragment** 实例分派到**工作节点**。**读/写 fragment** 实例总是被分派到管理 TGS 的工作节点。**查询 fragment** 实例可在任意一个工作节点执行，为了实现负载均衡，将在考虑**工作节点**现有工作负载的情况下进行调度。数据的位置信息和工作负载信息将分别与**存储管理器**和**资源管理器**同步。

在工作节点中，**fragment** 实例被映射到**工作单元** (WU)，这是 Hologres 中执行查询的基本单元。 WU 可以在运行时动态生成 WU。按如下方式映射：

1. 一个**读 fragment 实例**最初映射到一个 `read-sync WU`，它从元数据文件中获取 tablet 的当前版本，包括内存表的只读快照和分片文件列表。接下来，`read-sync WU` 产生多个 `read-apply WU` 以并行读取内存表和分片文件，以及对读取数据执行运算的**下游算子**。这种机制利用算子内部并行性，以更好地利用网络和 I/O 带宽。
2. 一个**写 fragment 实例**将所有只读算子映射到一个 `query WU` 中，然后是一个 `write-sync WU`，将写入的数据持久化为一条 WAL 日志记录。`write sync WU` 随后产生多个 `write-apply WU`，并行更新多个 ***tablet***。
3. 一个**查询 fragment** 实例被映射到一个 `query WU`。

## 4.2 执行上下文

作为 HSAP 服务，Hologres 被设计成同时执行不同用户提交的多个查询。并发 WU 之间的上下文切换开销可能成为并发的瓶颈。为了解决这个问题，Hologres 使用**<u>用户空间线程</u>**，称之为执行上下文（EC），作为 WU 的资源抽象。与抢占式调度的线程不同，**EC 是协作调度的**，无需使用任何系统调用或同步原语。因此，在 EC 之间切换的成本几乎可以忽略不计。 HOS 使用 EC 作为基本调度单元。计算资源以EC的粒度进行分配，进一步调度其内部任务。 EC 将在分配给它的线程上执行。

### 4.2.1 EC 池

我们在工作节点中，将 EC 分组到不同的池中，以实现隔离和优先级排序。 EC池可以分为三种类型：数据绑定EC池、查询EC池和后台EC池。

- 数据绑定 EC 池有两种类型的 EC：**WAL** EC 和 **tablet** EC。在 TGS 中，有一个 WAL EC 和多个 **tablet** EC，每个 ***tablet*** 一个。 WAL EC 执行`write-sync WU`，而 **tablet** EC 在相应的 **tablet** 上执行 `write-apply WU` 和  `read-sync WU`。 WAL/tablet ECs 以单线程的方式处理 WU，这消除了并发 WU 之间同步的必要性。
- 在查询 EC 池中，每个`query WU` 或 `read-apply WU` 都映射到一个查询 EC。
- 在后台 EC 池中，EC 用于从数据绑定 EC 池<u>==卸载==</u>昂贵的工作并提高写入吞吐量。这包括内存表刷新和分片文件合并等。通过这种设计，数据绑定的 EC 主要用于 WAL 上的操作和写入内存表，因此系统可以在没有锁定开销的情况下获得非常高的写入吞吐量。

为了限制后台 EC 的资源消耗，我们将后台EC与不同线程池中的数据绑定 EC 和查询 EC 物理隔离，并在优先级较低的线程池中执行后台EC。

### 4.2.2 执行上下文的内部结构

接下来，我们介绍 EC 的内部结构。

**任务队列**。 EC中有两个任务队列：（1）无锁的内部队列，保存 EC 自己提交的任务，（2）线程安全的提交队列，保存其他 EC 提交的任务。调度后，提交队列中的任务将重新定位到内部队列，以方便无锁调度。内部队列中的任务按 FIFO 顺序调度。

**状态**。在 EC 的生命周期内，它会在三种状态之间切换：**可运行**、**阻塞**和**挂起**。挂起意味着 EC 不能被调度，因为它的任务队列为空。向 EC 提交任务会将其状态切换为**可运行**，这表明 EC 可以被调度。如果 EC 中的所有任务都被阻塞，例如，由于 I/O 停顿，则将<u>==关闭==</u> EC，其状态被设置为阻塞。一旦接收到新任务或被阻塞的任务返回，阻塞的 EC 将再次变得可运行。可以从外部取消或 `join`（等待终止） EC 。**取消**一个 EC 将使未完成的任务失败并挂起它。 `join` 一个 EC 后，则其无法接收新任务，当前任务完成后自行挂起。EC 协同调度在系统线程池之上，因此上下文切换的开销几乎可以忽略不计。

### 4.2.3 联邦查询

Hologres 支持联邦查询，以与开源世界中提供的丰富服务（如，Hive [7] 和 HBase [6]）进行交互。我们允许单个查询跨越 Hologres 和<u>其他在不同进程中物理隔离的查询系统</u>。查询编译期间，要在不同系统中执行的运算符被编译为单独的片段，然后由 Hologres 中的协调器分派到它们的目标系统。与 Hologres 交互的其他系统被抽象为特殊的`stub WU`，每个 `stub WU` 都映射到一个在 Hologres 中统一管理的 EC。这个 `stub WU` 处理由 Hologres 中的 WU 提交的**<u>==拉取==</u>**请求。除了访问其他系统中的数据等功能性考虑之外，出于系统安全的考虑，这种抽象还充当了一个隔离沙盒。例如，用户提交的查询可能使用了不安全的 UDF 函数。Hologres 将这些函数传递到 PostgreSQL 进程，在与 Hologres 中的其他用户物理隔离的上下文中执行它们。

## 4.3 调度机制

在本小节中，我们将详细介绍如何调度查询的工作单元，以生成查询输出。

**基于拉取的异步查询执行**。查询是按照 Hologres 中<u>**基于拉取的范式**</u>异步执行。在查询计划中，**<u>叶 fragment</u>** 消耗外部输入，即分片文件，而**<u>接收 fragment</u>** 产生查询输出。基于拉取的查询执行从协调器开始，协调器将拉取请求发送到**<u>接收 fragment</u>** 的 WU。==处理拉取请求时，接收方 WU 进一步向其依赖的 WU 发送拉取请求==。一旦读运算符的 WU，即**列扫描**，接收到一个拉取请求，它会从对应的分片文件中读取一批数据，并以`<record_batch, EOS>`的格式返回结果，其中`record_batch ` 是一批结果记录，`EOS` 是一个布尔值，表示生产者 WU 是否已完成其工作。在接收到前一个拉取请求的结果时，协调器通过检查返回的 `EOS` 来确定查询是否已完成。如果查询未完成，它会发出另一轮拉取请求。依赖于多个<u>上游 WU</u> 的 WU 需要同时从多个输入中提取，以<u>**提高查询执行的并行性和计算/网络资源的利用率**</u>。 Hologres 通过发送多个异步拉取请求来支持并发拉取。与传统的需要多线程协作的并发模型相比，这种方式更加自然和高效。

<u>工作进程内拉取请求</u>被实现为一个函数调用，它将<u>**拉取任务**</u>插入到<u>**==管理==**</u>接收方 WU 的 EC 的任务队列中。<u>工作进程间拉取请求</u>被封装为<u>源</u>和<u>目标</u>工作进程节点之间的RPC调用。RPC 调用包含接收方 WU 的 ID，根据该 ID，目标工作节点将一个拉取任务插入到对应 EC 的任务队列中。

**背压。** 基于上述范式，我们实现了基于拉取的背压机制，以防止 WU 因接收太多拉取请求而不堪重负。首先，我们限制了 WU 一次可以发出的并发拉取请求的数量。其次，在为多个下游 WU 产生输出的 WU 中，处理拉取请求可能会导致为多个下游 WU 产生新的输出。这些输出被缓冲，等待来自相应 WU 的拉取请求。为了防止 WU 中的输出缓冲区增长过快，比其他 WU 更频繁拉取的下游 WU 将暂时减慢向该 WU 发送新的<u>**拉取请求**</u>的速度。

**预取**。 HOS 支持为未来的拉取请求预取结果以减少查询延迟。在这种情况下，一组预取任务被排队。预取任务的结果在预取缓冲区中排队。处理拉取请求时，可以立即返回预取缓冲区中的结果，并创建新的预取任务。

## 4.4 负载均衡

Hologres 中的负载平衡机制有两个方面：（1）在工作节点之间迁移 TGS，以及（2）在工作线程内重新分配 EC。

**TGS 的迁移：** 在我们当前的实现中，**读/写 fragments** 实例总是被分派到管理 TGS 的工作节点。如果一个 TGS 成为热点，或者一个工作节点过载，Hologres 支持将一些 TGS 从过载的工作节点迁移到具有更多可用资源的其他工作节点。<u>为了迁移 TGS，我们在存储管理器中将 TGS 标记为失败，然后按照标准的 TGS 恢复程序（参见第 3.2.3 节）在新的工作节点中恢复它</u>。如第 3.2.3 节所述，我们正在为 TGS 实现只读副本，这可以将读取片段实例平衡到位于多个工作节点中的 TGS 只读副本。

**EC 的重新分配：** 在工作节点中，HOS 在每个 EC 池内的线程之间重新分配 EC 以平衡工作负载。 HOS 执行三种类型的重新分配：（1）新创建的 EC 总是分配给线程池中 EC 数量最少的线程； (2) HOS周期性地在线程间重新分配EC，使得线程间EC数量的差异最小化； (3) HOS也支持**工作负载窃取**。一旦一个线程没有要调度的 EC，它就会从同一线程池中具有最大 EC 数量的线程“窃取”一个。EC 的重新分配仅在它没有运行任何任务时进行。

## 4.5 调度策略

HOS 面临的一个关键挑战是在多租户场景中保证查询级 SLO，例如，大规模分析查询不应阻塞对延迟敏感的服务查询。为了解决这个问题，我们提出**调度组（SG）**作为工作节点中<u>数据绑定</u>和<u>查询 EC</u> 的**虚拟资源**抽象。更具体地说，HOS 为每个 SG 分配一份**份额**，其值与分配给该 SG 的资源量成正比。 SG 的资源在其 EC 之间进一步拆分，EC 只能消耗分配给自己的 SG 的资源。

为了将<u>摄取工作负载</u>与<u>查询工作负载</u>分开，我们将数据绑定 EC 和查询 EC 隔离到不同的 SG。数据绑定 EC 处理需要<u>在所有查询之间同步的关键操作</u>，并且主要专用于摄取工作负载（`read sync WU` 通常非常轻量级），我们将所有数据绑定 EC 分组到一个**数据绑定的SG**。相反，我们将不同查询的查询 EC 放入单独的**查询 SG**。我们为数据绑定 SG 分配足够大的份额来处理所有摄取工作负载。默认情况下，所有查询 SG 都分配有相同的份额，以强制执行公平的资源分配。 SG 的份额可配置。

给定一个 SG，在一个时间间隔内分配给它的 EC 的 CPU 时间量受两个因素的影响：（1）它的份额，（2）它在最后一个时间间隔内占用的 CPU 时间量。 SG 的份额根据上一个时间间隔内其 EC 的状态进行调整，如下所述：

EC 只能在**可运行时**进行调度。将 *EC~i~* 的份额表示为 *EC_share~i~* ，我们计算 *EC_share_avg~i~* 来表示一个时间间隔内 *EC~i~* 的实际份额，而 *SG~i~* 的实际份额是其 EC 份额的总和：
$$
EC\_share\_avg_i = EC\_share_i \times \frac{\triangle T_{run}}{\triangle T_{run} + \triangle T_{spd} + \triangle T_{blk}}
$$
$$
SG\_share\_avg_i = \sum_{j=1}^{N}{EC\_share\_avg_j}
$$

∆*T~run~*、∆*T~spd~* 和 ∆*T~blk~* 表示 *EC~i~* 分别处于**可运行时**、**挂起**和**阻塞**状态的时间间隔。

对于 *SG~j~* 中的 *EC~i~*，我们维护一个 *Virtual Runtime*，反映其历史资源分配的状态。表示 *EC~i~* 在上一个时间间隔内分配的 CPU 时间为 ∆*CPU_time~i~* ， *EC~i~* 的 *Virtual Runtime* 上的增量，∆*vruntime~i~* , 在最后一个时间间隔内计算如下：

$$
EC\_vshare_i = \frac{EC\_share_i \times SG\_share_j}{SG\_share\_avg_j}
$$
$$
\triangle vruntime_i = \frac{\triangle CPU\_time_i}{EC\_vshare_i}
$$
当选择要调度的下一个 EC 时，线程调度程序总是选择具有最小 *vruntime* 的EC。

# 6. 相关的工作

**OLTP 和 OLAP 系统**。 OLTP 系统 [10, 12, 35] 采用行存来支持快速事务，这些事务常在少量行上执行点查找。OLAP 系统 [34, 37, 14, 27, 24, 22, 36] 利用列存来实现高效的列扫描，这是分析查询中的典型数据访问模式。与上述 OLTP/OLAP 系统不同，Hologres 支持混合行列存储。一个表可同时存储在行和列存中，以高效支持 HSAP 工作负载所需的点查询和列扫描。

像 Greenplum [5] 这样的 MPP 数据库通常将数据划分为**<u>==大段==</u>**，并将数据段与计算节点位于同一位置。 扩展系统时，MPP 数据库通常需要对数据进行重新分片。相反，Hologres 在 TGS 中管理数据，这是一个更小的**段单元**。 Hologres 将 TGS 动态映射到工作节点，并且可以在工作节点之间灵活迁移，而无需重新分片数据。此外，工作节点只需要将托管的 TGS 的内存表保存在内存中，但按需从远程文件系统中获取 TGS 的分片文件。在多租户调度方面，Greenplum 用不同进程处理不同的请求，依赖操作系统调度并发查询，很容易对查询并发造成<u>硬限制</u>。相反，Hologres 在一组用户空间线程上执行并发查询，从而实现更好了的查询并发性。

[31, 29] 研究了用于<u>**分析工作负载**</u>的高度并行执行查询的机制。它们将查询执行分解为小任务，在一组固定在物理内核中的线程中调度任务。Hologres 采用了类似高度并行的方法，但采用了分层调度框架，多租户场景中，**工作单元**的划分降低了调度大量任务的复杂性和开销。执行上下文和调度组提供了一种强大的机制来确保不同租户之间的资源隔离。[19] 讨论了一种用于多租户数据库中性能隔离的 CPU 共享技术。它强调了<u>数据库即服务环境</u>中所需的绝对 CPU 保留。而 Hologres 只需相对 CPU 预留，这足以防止分析查询延迟。

**HTAP 系统。** 近年来，随着对更多实时分析的需求快速增长，我们已经看到了很多关于在大数据集上提供**<u>混合事务/分析处理解决方案 (HTAP)</u>** 的研究兴趣。[33] 研究各种数据访问模式下，混合行和列格式如何有助于提高数据查询性能。后续系统如 SAP HANA [21]、MemSQL [9]、HyPer [23]、Oracle 数据库 [25] 和 SQL Server [20, 28] 等支持事务处理和分析处理。它们通常为 OLTP 使用行格式，为 OLAP 使用列格式，但需要在行和列格式之间转换数据。由于这些转换，新提交的数据可能不会立即反映在列存储中。**相反，Hologres 可以在行和列 *tablet* 中存储表，并且每次写入表同时更新两种类型的 *tablet***。 Hologres 同时并行写入所有 ***tablet***，以实现高写入吞吐量。此外，HSAP 场景的摄取率远高于 HTAP 场景中的事务率（例如，用户在进行购买交易之前通常会产生数十个页面查看事件），但通常具有较弱的一致性要求。Hologres 特意只支持原子写和 `read-your-write` 的读，通过避免复杂的并发控制，实现了更高的读/写吞吐量。 

[32] 研究了 HTAP 系统中<u>高并发工作负载</u>的任务调度。对于 OLTP 工作负载，因为 OLTP 任务包括大量使用同步，它调整并发级别以使 CPU 饱和。然而，Hologres 采用无锁方法，避免了频繁的阻塞。对于 OLAP 工作负载，它使用并发提示来调整<u>分析工作负载</u>的任务粒度，可以将其集成到 Hologres 中以调度执行上下文。 

**NewSQL**。Hologres 中采用的分片机制类似于 BigTable [16] 和 Spanner [18]。 BigTable 使用 tablet 抽象来方便<u>对排序数据的范围搜索</u>。Spanner 是一个全球分布的键值存储，支持强一致性。在存在**<u>分布式数据复制</u>**的情况下，使用 Spanner 中的数据分片作为保持数据一致性的基本单元。与主要作为 OLTP 解决方案的 Spanner 不同，Hologres 特意为 HSAP 场景选择支持较弱的一致性模型，以追求更好的性能。

# 7. 结论和未来的工作
现代大数据处理出现了许多**融合服务和分析处理** (HSAP) 的新趋势。在阿里巴巴，我们设计并实现了 Hologres，一种云原生 HSAP 服务。 Hologres 采用了一种新颖的基于 **<u>table</u>**t 的存储设计，基于**<u>执行上下文</u>**的调度机制，以及<u>存储/计算</u>和<u>读/写</u>的明确解耦。这使 Hologres 能够实时提供高吞吐量的数据摄取，并为混合服务和分析处理提供卓越的查询性能。我们对 Hologres 和许多大数据系统进行了全面的实验研究。我们的结果表明，Hologres 甚至优于专门用于分析或服务场景的最先进系统。

要在 HSAP 中获得更高的性能，还有许多公开的挑战。这些挑战包括针对<u>读取密集型热点</u>的更好的横向扩展机制、<u>更好的内存子系统</u>和<u>网络带宽资源隔离</u>以及<u>分布式环境中的绝对资源预留</u>。作为未来工作的一部分，我们计划探索这些问题。

---

# Read your write 一致性

在分布式系统中，在 master 上所做的更改并不总是在每个副本上都立即可用，尽管它们最终将可用。<u>通常，不直接参与事务提交确认的副本将落后于其他副本，因为它们不与主服务器同步提交</u>。

出于这个原因，您可能希望利用 read-your-writes 一致性功能。此功能允许您确保副本至少是最新的，可以进行特定事务所做的更改。由于事务是串行提交的，通过确保副本应用了特定的提交，您知道<u>在指定事务之前发生</u>的<u>所有事务提交</u>也**已应用于副本**。

您可以通过在 master 上生成提交令牌来确定事务是否已应用于副本。然后您将此提交令牌传输到副本，在那里它用于确定副本相对于 master 是否<u>**足够一致**</u>。

例如，假设您有一个 Web 应用程序，其中在负载平衡的 Web 服务器组中实现了一个复制组。对 Web 服务器的每个请求都包含一个更新操作，然后是读取操作（例如，来自同一个客户端），读取操作自然希望看到来自同一请求执行的更新的数据。但是，读取操作可能已路由到未执行更新的副本。

在这种情况下，更新请求将生成一个提交令牌，浏览器将重新提交该令牌以及后续读取请求。负载平衡器可以将读取请求定向到任何一个可用的 Web 服务器。为读取请求提供服务的副本将使用该提交令牌来确定它是否可以为读取操作提供服务。如果副本足够新，它可以立即执行事务并满足请求。

如果副本的一致性不足以为读取请求提供服务，副本将采取什么操作取决于您作为应用程序开发人员。您可以做任何事情，从在等待事务在本地应用时阻塞到完全拒绝读取请求。
