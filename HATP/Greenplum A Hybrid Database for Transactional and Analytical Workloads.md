# Greenplum: A Hybrid Database for Transactional and Analytical Workloads

对支持实时在线事务处理 (OLTP) 查询以及长时在线分析处理 (OLAP) 工作负载的企业数据仓库解决方案的需求正在增长。Greenplum 数据库传统上被称为 OLAP 数据仓库系统，其处理 OLTP 工作负载的能力有限。我们将 Greenplum 扩充为一个混合系统，以同时服务于 OLTP 和 OLAP 工作负载。我们在这里要解决的挑战是在保持 ACID 属性的同时，以最小的性能开销来实现这个目标。我们确定了工程上和上性能的瓶颈，例如，性能不佳的**限制性锁定**和**两阶段提交协议**。然后，我们解决事务查询和分析查询之间的资源争用问题。我们提出了一个**全局死锁检测器**来增加查询处理的并发性。当更新数据的事务保证只驻留在一个**段**上时，我们引入了**单阶段提交**来加速查询处理。我们的**资源组模型**引入的功能，将 OLAP 和 OLTP 工作负载分离为更合适的查询处理模式。对 TPC-B 和 CH-benCHmark 基准的实验评估证明了，我们的方法在提高 OLTP 性能而不牺牲 OLAP 性能方面的有效性。

> 1. restrictive locking：限制性锁定
> 2. segment：段

## 1. 简介

Greenplum 是一个成熟的大型数仓系统，企业和开源都有部署。Greenplum 的 MPP (大规模并行处理) 架构将数据拆分<u>**==为不相交的==**</u>部分，这些部分存储在各个**工作段**中。类似于 Oracle Exadata [5]、Teradata [1, 7] 和 Vertica [13] 等大规模数仓系统，包括 AWS Redshift [10]、AnalyticDB [27] 和 BigQuery 等 DWaaS 系统 [24]。 这些数仓系统能够以分布式方式高效**管理和查询 PB 级数据**。 相比之下，CockroachDB [23] 和 Amazon RDS[2] 等分布式关系数据库则专注于提供可扩展的解决方案，用于存储 **TB 级数据和快速处理事务查询**。

Greenplum 用户通过**协调器**节点与系统交互，底层分布式架构对用户透明。协调器优化查询以进行并行处理，并将生成的**计划**分派给各个**段**。每个**段**并行执行计划，并在需要时在**段**之间 shuffle 数据。对于长时运行的分析查询，这种方法实现了显著的加速。结果由协调器收集，然后转发给客户端。DML 操作可用于修改**工作段**中管理的数据。 <u>==通过两阶段提交协议确保原子性==</u>。并发事务使用分布式快照相互隔离。 Greenplum 的表支持<u>**列存**</u>，针对**追加**做了优化，且支持使用各种压缩算法。这种表非常适合 OLAP 工作负载中典型的批量读写操作。

> 这里两阶段提交协议确保**原子性**，我理解应该是**事务**。

图 [1](#_bookmark0) 显示了一个典型的数据处理工作流，涉及[操作型数据库](https://en.wikipedia.org/wiki/Operational_database)在短时间内管理热点（最有价值）事务数据。然后使用 ETL 工具定期转换这些数据，并将其加载到数仓中以进一步分析。人们越来越希望降低维护不同系统的复杂性 [[19](#_bookmark44)]。在这种情况下，用户更愿意拥有一个可以同时满足 OLAP 和 OLTP 工作负载的系统。 换句话说，这样的系统**<u>需要对点查询具有高度响应性，并且对于长时间运行的分析查询具有可扩展性</u>**。这种愿望在文献中多有描述，被称为**混合交易和分析处理系统**（HTAP）[[11](#_bookmark36)、[19](#_bookmark44)、[20](#_bookmark45)、[25](#_bookmark50)]。

> 1. 操作型数据库（operational database）：操作数据库是用来实时管理和存储数据的数据库。[操作数据库是数据仓库的源](https://zhuanlan.zhihu.com/p/344508825)。操作数据库中的元素可以动态地添加和删除。这些数据库可以是基于 SQL 的，也可以是基于 nosql 的，后者面向实时操作。
> 2. TODO：图一

为了满足 Greenplum 企业用户的需求，我们建议将 Greenplum 扩充到 HTAP 系统中。 我们在这项工作中重点关注以下领域，即：1) <u>==在 ACID 的保证下，提升数据加载到并行系统的性能==</u>，2) 优化点查询的响应时间，在 OLTP 工作负载中普遍存在，以及 3) 资源组，它能够 在不同类型的工作负载或用户组之间隔离资源。

Greenplum 将 OLAP 查询设计为一等公民，OLTP 工作负载则不是主要关注点。**两阶段提交**对只更新几个元组的事务造成性能损失。**协调器**大量强加的锁（旨在防止分布式死锁），被证明过于严格。这种<u>**惩罚**</u>会不成比例地影响短期运行的查询。 如图 2 所示，在 10 秒的样本中，连接数较少的情况下，锁定占用了超过 25% 的查询运行时间。当并发数超过 100 时，锁定时间变得不可接受。

> TODO：图2

我们在将 Greenplum 扩展到 HTAP 系统方面的主要贡献可以总结如下：

- 确定了将 OLAP 数据库系统转换为 HTAP 数据库系统的挑战。
- 设计**全局死锁检测器**，以减少锁定开销并提升 OLTP 响应时间，且不会牺牲 OLAP 工作负载的性能。
- 通过切换到一阶段提交协议，加速只在一个段上更新数据的事务。
- 实现了一个新的<u>资源隔离组件</u>来管理 OLTP 和 OLAP 工作负载，以避免高并发场景下的资源冲突。
- 对多个基准数据集进行了全面的性能评估。结果表明，Greenplum 的 HTAP 版本的性能与传统 OLTP 数据库不相上下，同时在高度并发和混合的工作负载上仍然提供实时计算能力。

**本文结果**。第 2 节，我们回顾相关工作，然后在第 3 节，详细描述 Greenplum 的 MPP 架构和概念。第 4 节详细介绍了**全局死锁检测**的设计和实现。 第 5 节演示了 Greenplum 中的**分布式事务管理**和相关优化以提高 OLTP 性能。第 6 节介绍我们在高并发、混合工作负载环境中，<u>缓解</u>由资源竞争引起的性能下降的方法。最后，第 7 节，介绍了我们的实验方法和分析。

## 2. 相关工作

混合事务和分析处理系统（HTAP）。与 OLAP 或 OLTP 系统相比，HTAP 系统 [6, 11, 26] 带来了几个好处。==首先，因为没有 ETL 传输延迟==，HTAP 明显减少分析新数据时的等待时间，无需额外组件或外部系统即可实时分析数据。其次，HTAP 系统还可以降低硬件和管理方面的总体业务成本。有许多广泛使用的商业 OLTP DBMS [23, 25, 26] 已经转向类似 HTAP 的 DBMS。然而，商业 OLAP DBMS 中对 OLTP 工作负载的支持仍然没有改变。随着 HTAP 的概念流行，越来越多的数据库系统尝试支持 HTAP 功能。 Özcan 等人 [19] 将 HTAP 数据库分为两类：<u>用于 OLTP 和 OLAP 的单一系统</u>和<u>单独的 OLTP 和 OLAP 系统</u>。本节的其余部分，我们将讨论 HTAP 数据库的不同演进路径。

**从 OLTP 到 HTAP 数据库**。OLTP 数据库旨在支持高并发和低延迟的事务处理。 Oracle Exadata [5] 设计用于在分析处理的同时运行OLTP工作负载。Exadata 引入了智能横向扩展存储、RDMA 和 infiniBand 网络以及 NVMe 闪存来提高 HTAP 性能。最近，它支持诸如带内存列缓存的列级校验和，以及智能OLTP缓存等功能，<u>**这些功能减少了闪存故障或更换的影响**</u>。Amazon Aurora [25] 是 AWS 的云 OLTP 数据库。 它遵循日志就是数据库的思想，并将繁重的日志处理<u>==卸载==</u>到存储层。为了支持 OLAP 工作负载，Aurora 的并行查询功能，将单个查询的计算工作下推到存储层，将其分布在数千个 CPU 上。查询操作的并行化可以将分析查询的速度提高两个数量级。

**从 NewSQL 数据库到 HTAP 数据库**。自从 Google Spanner [9] 成功之后，具有高扩展性和强 ACID 的 NewSQL 数据库应运而生，以克服 NoSQL 数据库具有高扩展性和弱 ACID 的局限性。NewSQL 的早期实现，例如 CockroachDB [23]、TiDB [11] 和 F1 [20]，基于 Paxos [15] 或 Raft [18] 等共识协议，侧重于支持地理分布式 OLTP 工作负载。最近，几个 NewSQL 数据库宣布自己为 HTAP 数据库。TiDB 引入了 TiFlash [11] 来处理不同硬件资源上的 OLAP 工作负载。它扩展了 Raft 以将数据异步复制到 Raft **<u>==学习器==</u>**中，并动态地将数据转换成列式存储以服务于 OLAP 工作负载。同样，传统的 OLAP 数据库（例如 Vertica [14]）也使用写优化存储（WOS）来处理插入、更新和删除查询。然后 WOS 被异步转换为读优化存储 (ROS) 以处理 OLAP 工作负载。 F1 Lighting [26] 提供 **HTAP 即服务**。在 Google 中，Lighting 用于从 Spanner 和 F1 DB 等 OLTP 数据库复制数据，并将这些数据转换为用于 OLAP 工作负载的列格式。与 TiFlash 不同，Lighting 提供与源 OLTP 数据库的<u>**强快照一致性**</u>。

Greenplum 展示了另一种演化为 HTAP 数据库的途径。它在传统的 OLAP 数据库中增加了 OLTP 能力，并支持细粒度的资源隔离。 下一节详细介绍 Greenplum 的架构。

## 3. Greenplum 的 MPP 架构

为了支持对PB级数据的存储和高性能分析，在使用单个主机数据库时，有几个问题很难解决：

- 数据可扩展性：数据总量太大，无法存储在单个主机中。
- 计算可扩展性：处理并发的能力受限于单个主机的计算资源，例如 CPU、内存和 IO。
- 高可用性：如果单个主机不可用，则整个数据库系统也不可用。

Greenplum 基于 MPP 架构构建了一个数据库集群来解决上述限制。 一个正在运行的 Greenplum 集群由多个运行的**工作段**组成，**可以将其视为增强的 PostgreSQL**。 图 3 显示了整个架构。

> 图 3

接下来介绍几个重要的模块以及 Greenplum MPP 架构的设计。 这些概念对于理解 Greenplum 的 HTAP 改进至关重要。

### 3.1 段的角色和职责

Greenplum 集群由跨多个主机的多个**段**组成。整个数据库系统中只有一个**段**称为**协调器段**。其他的简称为**段**。客户端直连协调器段。**协调器**接收来自用户的命令或查询，生成分布式查询计划，<u>根据计划生成分布式进程</u>，将其分派到每个进程，收集结果，最后发送回客户端。 **段**主要存储用户数据，接受从**协调器**分配来的特定的分布式计划片段，并执行之。为了实现高可用性，一些**段**被配置为镜像（或协调器的备用）。镜像（和备用）不会直接参与计算。 相反，它们会不断地从其相应的段（master）接收 WAL 日志，并即时重放日志。

Greenplum 遵循 **Share-Nothing** 架构。**协调器**和**段**有自己的共享内存和数据目录。协调器仅通过网络与段通信。

### 3.2 分布式计划和分布式执行器

对于分布式**关系**，每个**段**通常只存储整个数据的一小部分。**连接**两个**关系**时，我们经常需要检查来自不同段的两个元组是否匹配连接条件。这意味着 Greenplum 必须在段之间移动数据，以确保所有可能的匹配元组都在同一段中。 Greenplum 引入了一个名为 **Motion** 的新计划节点来实现此类数据移动。

**Motion** 计划节点通过网络从不同的**段**（主机）发送和接收数据。它自然而然地将计划分割成若干**==片==**，Motion 下方或上方的片在 Greenplum 中称为**切片**。每个切片由一组分布式进程执行，这组进程称为**==战队==**（**gang**）。

有了上述提出的 **Motion** 计划节点和**==战队==**（**gang**），Greenplum 的查询计划和执行器都变成了分布式。计划将被分派到每个进程，并根据其本地上下文和状态，每个进程执行自己的计划**切片**以完成查询执行。所描述的执行是**<u>单程序多数据</u>**（Single Program Multiple Data technique）技术：我们将相同的计划分派到集群中的一组进程，由不同**段**产生的不同进程具有各自的本地上下文、状态和数据。

我们用一个例子来说明上述概念。图 4（顶部）显示了一个 SQL 的分布式计划， 这个查询包含 `join`。图 4 底部显示了该计划在有两个**段**的集群中的<u>执行进度</u>。顶部**切片**由协调器上的单进程执行，其他**切片**在**段**上执行。一个**切片**扫描表，然后使用重新分配的 Motion 节点将元组发送出去。另一个切片执行哈希连接，从 Motion 节点接收元组，扫描 `student` 表，构建 `hash map`，计算哈希连接，最后将元组发送到顶部切片。

> 1. TODO：图 4
> 2. 术语不一样，执行过程和 Spark 差不多

### 3.3 分布式事务管理

在 Greenplum 集群中，每个**段**运行一个**增强的 PostgreSQL 实例**，每个段同步提交或中止事务。 为了确保 ACID 属性，Greenplum 使用<u>分布式快照</u>和<u>两阶段提交协议</u>。分布式事务管理的性能对于将 Greenplum 扩展为可行的 HTAP 系统至关重要。 详细信息将在第 5 节中讨论。

> 何为分布式快照？

### 3.4 混合存储和优化器

Greenplum 支持 PostgreSQL 原生**堆表**，这是一种行存，具有固定大小的块和缓冲区缓存，由在段上运行的查询执行进程共享，以方便并发读写操作。Greenplum 中引入了两种新的表类型：**追加优化**的行存（AO-row）和追加优化的列存（AO-column）。AO 表有利于批量 I/O 而不是随机访问，它们更适合分析工作负载。在 AO 列存中，每列分配一个单独的文件。这种设计进一步减少了从宽表中只选择几列查询的 I/O。 AO 表可以使用多种算法进行压缩，例如 zstd、quicklz 和 zlib。AO 列表的每列都可使用特定算法压缩，包括带增量压缩的行程编码 (RLE)。 **Greenplum 中的查询执行引擎与表存储类型无关**。AO 行、AO 列和堆表可以在同一个查询中共存。

表可以按用户指定的**键**和**分区策略**（列表或范围）进行分区。这是通过**在根表下**创建表层次结构来实现的，只有**叶表**包含用户数据。后来上游 PostgreSQL 采用了类似设计的分区功能。**层次结构中的每个分区可以是堆、AO 行、AO 列或外部表**。 外部表用于读取/写入存储在 Greenplum 外部的数据，例如，在亚马逊 S3 中。

图 5 显示了一个按销售日期分区的 `SALES` 表，每个分区由一个日期范围定义。 最近的分区创建为本地堆表（6 月至 8 月）。 AO 列存储用于存储较旧的销售数据（9 月至 12 月），而前几年的销售数据存档在外部表中。可以对 SALES 表或其各个分区进行查询，而无需知道该表的存储空间。这种策略类似于[16]中数据的冷热分类。

和存储一样，Greenplum 中的查询优化也很灵活。查询优化取决于工作负载。 分析工作负载由涉及许多<u>连接</u>和<u>聚合</u>的即席和复杂查询组成。查询性能主要由查询计划的效率决定。 Greenplum 中的 Orca [22] 查询优化器是一个基于成本的优化器，专为分析工作负载而设计。另一方面，事务性工作负载由短查询组成，这些查询对查询计划延迟很敏感。它要求优化器能要快速生成简单的计划。Greenplum <u>支持 MPP 版本的 PostgreSQL 优化器</u>适用于此类事务性工作负载。用户可以分别在查询、会话或数据库层选择这两个优化器。灵活选择最合适的优化器有助于 Greenplum 更有效地处理 HTAP 工作负载。