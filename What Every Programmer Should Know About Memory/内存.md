**概要**

随着 CPU 内核速度越来越快，数量也越来越多，现在大多数程序的限制因素是内存访问，将来一段时间也会如此。硬件设计人员已经提出了更复杂的内存处理和加速技术——例如 CPU 缓存——但如果没有程序员的帮助，这些技术无法以最佳方式工作。不幸的是，大多数程序员都不了解计算机内存子系统或 CPU 芯片上缓存的结构和成本。本文解释了现代商品硬件上使用的内存子系统的结构，说明了开发 CPU 缓存的原因、它们的工作原理以及程序应该如何利用它们来实现最佳性能。

> 注 1
>
> 这个系列文章源于`What Every Programmer Should Know About Memory`[1](https://lrita.github.io/2018/06/10/programmer-should-know-about-memory-0/#fn:1)，粗读下来觉得很不错，最好能留存下来。同时发现这个系列的文章已经有一部分被人翻译了。故在此转发留存一份，毕竟存在自己收留的才是最可靠的，经常发现很多不错的文章链接失效的情况。
>
> 本文转载自[2](https://lrita.github.io/2018/06/10/programmer-should-know-about-memory-0/#fn:2)，翻译自[3](https://lrita.github.io/2018/06/10/programmer-should-know-about-memory-0/#fn:3)。本人进行了轻微的修改，感觉更符合原义。
>

# 1 简介

早期计算机比现在更为简单。系统的各种组件例如 CPU、内存、大容量存储器和网口，由于被共同开发因而有非常均衡的表现。**例如，内存和网口并不比 CPU 在提供数据的时候更（特别的）快**。

曾今计算机稳定的基本结构悄然改变，硬件开发人员开始致力于优化单个子系统。于是电脑一些组件的性能大大的落后因而成为了瓶颈。由于成本的原因，大容量存储器和内存子系统相对于其他组件来说改善得更为缓慢。

大容量存储的性能问题往往靠软件来改善：操作系统将常用（且最有可能被用）的数据放在主存中，因为后者的速度要快上几个数量级。或者将缓存加入存储设备中，这样就可以在不修改操作系统的前提下提升性能。（然而，为了在使用缓存时保证数据的完整性，仍然要作出一些修改）。这些内容不在本文的谈论范围之内，就不作赘述了。

而解决内存的瓶颈更为困难，它与大容量存储不同，几乎每种方案都需要对硬件作出修改。目前，这些变更主要有以下这些方式:

- RAM 的硬件设计(速度与并发度)
- 内存控制器的设计
- CPU 缓存
- 设备的直接内存访问(DMA)

本文主要关心的是 CPU 缓存和内存控制器的设计。在讨论这些主题的过程中，我们还会研究 DMA。不过，我们首先会从当今商用硬件的设计谈起。这有助于我们理解目前在使用内存子系统时可能遇到的问题和限制。我们还会详细介绍 RAM 的分类，说明为什么会存在这么多不同类型的内存。

本文不会包括所有内容，也不会包括最终性质的内容。我们的讨论范围仅止于商用硬件，而且只限于其中的一小部分。另外，本文中的许多论题，我们只会点到为止，以达到本文目标为标准。对于这些论题，大家可以阅读其它文档，获得更详细的说明。

当本文提到操作系统特定的细节和解决方案时，针对的都是Linux。无论何时都不会包含别的操作系统的任何信息，作者无意讨论其他操作系统的情况。如果读者认为他/她不得不使用别的操作系统，那么必须去要求供应商提供其操作系统类似于本文的文档。

在开始之前最后的一点说明，本文包含大量出现的术语“通常”和别的类似的限定词。这里讨论的技术在现实中存在于很多不同的实现，所以本文只阐述使用得最广泛最主流的版本。在阐述中很少有地方能用到绝对的限定词。

## 1.1 文档结构

这个文档主要视为软件开发者而写的。本文不会涉及太多硬件细节，所以喜欢硬件的读者也许不会觉得有用。但是在我们讨论一些有用的细节之前，我们先要描述足够多的背景。

在这个基础上，本文的第二部分将描述RAM（随机寄存器）。懂得这个部分的内容很好，但是此部分的内容并不是懂得其后内容必须部分。我们会在之后引用不少之前的部分，所以心急的读者可以跳过任何章节来读他们认为有用的部分。

第三部分会谈到不少关于CPU缓存行为模式的内容。我们会列出一些图标，这样你们不至于觉得太枯燥。第三部分对于理解整个文章非常重要。第四部分将简短的描述虚拟内存是怎么被实现的。这也是你们需要理解全文其他部分的背景知识之一。

第五部分会提到许多关于Non Uniform Memory Access (NUMA)系统。

第六部分是本文的中心部分。在这个部分里面，我们将回顾其他许多部分中的信息，并且我们将给阅读本文的程序员许多在各种情况下的编程建议。如果你真的很心急，那么你可以直接阅读第六部分，并且我们建议你在必要的时候回到之前的章节回顾一下必要的背景知识。

本文的第七部分将介绍一些能够帮助程序员更好的完成任务的工具。即便在彻底理解了某一项技术的情况下，距离彻底理解在非测试环境下的程序还是很遥远的。我们需要借助一些工具。

第八部分，我们将展望一些在未来我们可能认为好用的科技。

## 1.2 反馈问题

作者会不定期更新本文档。这些更新既包括伴随技术进步而来的更新也包含更改错误。非常欢迎有志于反馈问题的读者发送电子邮件。

## 1.3 致谢

我首先需要感谢Johnray Fuller尤其是Jonathan Corbet，感谢他们将作者的英语转化成为更为规范的形式。Markus Armbruster提供大量本文中对于问题和缩写有价值的建议。

## 1.4 关于本文

本文题目对David Goldberg的经典文献《What Every Computer Scientist Should Know About Floating-Point Arithmetic》[goldberg]表示致敬。Goldberg的论文虽然不普及，但是对于任何有志于严格编程的人都会是一个先决条件

# 2 商用硬件现状

鉴于目前专业硬件正在逐渐淡出，理解商用硬件的现状变得十分重要。现如今，人们更多的采用水平扩展，也就是说，用大量小型、互联的商用计算机代替巨大、超快(但超贵)的系统。原因在于，快速而廉价的网络硬件已经崛起。那些大型的专用系统仍然有一席之地，但已被商用硬件后来居上。2007年，Red Hat认为，未来构成数据中心的“积木”将会是拥有最多4个插槽的计算机，每个插槽插入一个四核CPU，这些CPU都是超线程的。（超线程使单个处理器核心能同时处理两个以上的任务，只需加入一点点额外硬件）。也就是说，这些数据中心中的标准系统拥有最多64个虚拟处理器（至今来看2018那年，96核/128核的服务已经是很常见的服务器配置了）。当然可以支持更大的系统，但人们认为4插槽、4核CPU是最佳配置，绝大多数的优化都针对这样的配置。

在不同商用计算机之间，也存在着巨大的差异。不过，我们关注在主要的差异上，可以涵盖到超过90%以上的硬件。需要注意的是，这些技术上的细节往往日新月异，变化极快，因此大家在阅读的时候也需要注意本文的写作时间。

这么多年来，个人计算机和小型服务器被标准化到了一个芯片组上，它由两部分组成: 北桥和南桥，见图2.1。

<p align="center">
<img src="./memory/cpumemory.4.png"/>
图2.1 北桥和南桥组成的结构
</p>

CPU通过一条通用总线(前端总线，FSB)连接到北桥。北桥主要包括内存控制器和其它一些组件，内存控制器决定了RAM芯片的类型。不同的类型，包括DRAM、Rambus和SDRAM等等，要求不同的内存控制器。

为了连通其它系统设备，北桥需要与南桥通信。南桥又叫I/O桥，通过多条不同总线与设备们通信。目前，比较重要的总线有PCI、PCI Express、SATA和USB总线，除此以外，南桥还支持PATA、IEEE 1394、串行口和并行口等。比较老的系统上有连接北桥的AGP槽。那是由于南北桥间缺乏高速连接而采取的措施。现在的PCI-E都是直接连到南桥的。

这种结构有一些需要注意的地方:

- 从某个CPU到另一个CPU的数据需要走它与北桥通信的同一条总线。
- 与RAM的通信需要经过北桥
- RAM只有一个端口。(本文不会介绍多端口RAM，因为商用硬件不采用这种内存，至少程序员无法访问到。这种内存一般在路由器等专用硬件中采用。)
- CPU与南桥设备间的通信需要经过北桥

在上面这种设计中，瓶颈马上出现了。第一个瓶颈与设备对RAM的访问有关。早期，所有设备之间的通信都需要经过CPU，结果严重影响了整个系统的性能。为了解决这个问题，有些设备加入了直接内存访问(DMA)的能力。DMA允许设备在北桥的帮助下，无需CPU的干涉，直接读写RAM。到了今天，所有高性能的设备都可以使用DMA。虽然DMA大大降低了CPU的负担，却占用了北桥的带宽，与CPU形成了争用。

第二个瓶颈来自北桥与RAM间的总线。总线的具体情况与内存的类型有关。在早期的系统上，只有一条总线，因此不能实现并行访问。近期的RAM需要两条独立总线(或者说通道，DDR2就是这么叫的，见图2.8)，可以实现带宽加倍。北桥将内存访问交错地分配到两个通道上。更新的内存技术(如FB-DRAM)甚至加入了更多的通道。

由于带宽有限，我们需要以一种使延迟最小化的方式来对内存访问进行调度。我们将会看到，处理器的速度比内存要快得多，需要等待内存。如果有多个超线程核心或CPU同时访问内存，等待时间则会更长。对于DMA也是同样。

除了并发以外，访问模式也会极大地影响内存子系统、特别是多通道内存子系统的性能。关于访问模式，可参见2.2节。

在一些比较昂贵的系统上，北桥自己不含内存控制器，而是连接到外部的多个内存控制器上(在下例中，共有4个)。

<p align="center">
<img src="https://lrita.github.io/images/posts/memory/cpumemory.5.png"/>
图2.2 拥有外部控制器的北桥
</p>

这种架构的好处在于，多条内存总线的存在，使得总带宽也随之增加了。而且也可以支持更多的内存。通过同时访问不同内存区，还可以降低延时。对于像图2.2中这种多处理器直连北桥的设计来说，尤其有效。而这种架构的局限在于北桥的内部带宽，非常巨大(来自Intel)。（出于完整性的考虑，还需要补充一下，这样的内存控制器布局还可以用于其它用途，比如说「内存RAID」，它可以与热插拔技术一起使用。）

使用外部内存控制器并不是唯一的办法，另一个最近比较流行的方法是将控制器集成到CPU内部，将内存直连到每个CPU。这种架构的走红归功于基于AMD Opteron处理器的SMP系统。图2.3展示了这种架构。Intel则会从Nehalem处理器开始支持通用系统接口(CSI)，基本上也是类似的思路——集成内存控制器，为每个处理器提供本地内存。

<p align="center">
<img src="https://lrita.github.io/images/posts/memory/cpumemory.6.png"/>
图2.3 集成的内存控制器
</p>

通过采用这样的架构，系统里有几个处理器，就可以有几个内存库(memory bank)。比如，在4 CPU的计算机上，不需要一个拥有巨大带宽的复杂北桥，就可以实现4倍的内存带宽。另外，将内存控制器集成到CPU内部还有其它一些优点，这里就不赘述了。

同样也有缺点。首先，系统仍然要让所有内存能被所有处理器所访问，导致内存不再是统一的资源(NUMA即得名于此)。处理器能以正常的速度访问本地内存(连接到该处理器的内存)。但它访问其它处理器的内存时，却需要使用处理器之间的互联通道。比如说，CPU1如果要访问CPU2的内存，则需要使用它们之间的互联通道。如果它需要访问CPU4的内存，那么需要跨越两条互联通道。

使用互联通道是有代价的。在讨论访问远端内存的代价时，我们用「NUMA因子」这个词。在图2.3中，每个CPU有两个层级: 相邻的CPU，以及两个互联通道外的CPU。在更加复杂的系统中，层级也更多。甚至有些机器有不止一种连接，比如说IBM的x445和SGI的Altix系列。CPU被归入节点，节点内的内存访问时间是一致的，或者只有很小的NUMA因子。而在节点之间的连接代价很大，而且有巨大的NUMA因子。

目前，已经有商用的NUMA计算机，而且它们在未来应该会扮演更加重要的角色。人们预计，从2008年底开始，每台SMP机器都会使用NUMA。每个在NUMA上运行的程序都应该认识到NUMA的代价。在第5节中，我们将讨论更多的架构，以及Linux内核为这些程序提供的一些技术。

除了本节中所介绍的技术之外，还有其它一些影响RAM性能的因素。它们无法被软件所左右，所以没有放在这里。如果大家有兴趣，可以在第2.1节中看一下。介绍这些技术，仅仅是因为它们能让我们绘制的RAM技术全图更为完整，或者是可能在大家购买计算机时能够提供一些帮助。

以下的两节主要介绍一些入门级的硬件知识，同时讨论内存控制器与DRAM芯片间的访问协议。这些知识解释了内存访问的原理，程序员可能会得到一些启发。不过，这部分并不是必读的，心急的读者可以直接跳到第2.2.5节。

## 2.1 RAM类型

这些年来，出现了许多不同类型的RAM，各有差异，有些甚至有非常巨大的不同。那些很古老的类型已经乏人问津，我们就不仔细研究了。我们主要专注于几类现代RAM，剖开它们的表面，研究一下内核和应用开发人员们可以看到的一些细节。

第一个有趣的细节是，为什么在同一台机器中有不同的RAM？或者说得更详细一点，为什么既有静态RAM(SRAM {SRAM还可以表示「同步内存」。})，又有动态RAM(DRAM)。功能相同，前者更快。那么，为什么不全部使用SRAM？答案是，价格。无论在生产还是在使用上，SRAM都比DRAM要贵得多。生产和使用，这两个开销因素都很重要，后者则是越来越重要。为了理解这一点，我们分别看一下SRAM和DRAM一个位的存储的实现过程。

在本节的余下部分，我们将讨论RAM实现的底层细节。我们将尽量控制细节的层面，比如，在「逻辑的层面」讨论信号，而不是硬件设计师那种层面，因为那毫无必要。

### 2.1.1 静态RAM

<p align="center">
<img src="https://lrita.github.io/images/posts/memory/cpumemory.7.png"/>
图2.4 6-T静态RAM
</p>

图 2.4 展示了 6 晶体管 SRAM 的一个单元。核心是 4 个晶体管 M1 - M4，它们组成两个交叉耦合的反相器。它们有两个稳定的状态，分别代表 0 和 1。只要保持 Vdd 有电，状态就是稳定的。

当访问单元的状态时，需要拉升WL的电平。使得 BLBL 和 ¯¯¯¯¯¯¯¯BLBL¯ 上可以读取状态。如果需要覆盖单元状态，先将 BLBL 和¯¯¯¯¯¯¯¯BLBL¯设置为期望的值，然后升起WL电平。由于外部的驱动强于内部的4个晶体管（M1 - M4），所以旧状态会被覆盖。

更多详情，可以参考[sramwiki]。为了下文的讨论，需要注意以下问题:

- 一个单元需要6个晶体管。也有采用4个晶体管的SRAM，但有缺陷。
- 维持状态需要恒定的电源。
- 升起 WL 后立即可以读取状态。信号与其它晶体管控制的信号一样，是直角的(快速在两个状态间变化)。
- 状态稳定，不需要刷新循环。

SRAM 也有其它形式，不那么费电，但比较慢。由于我们需要的是快速 RAM，因此不在关注范围内。这些较慢的 SRAM 的主要优点在于接口简单，比动态 RAM 更容易使用。

### 2.1.2 动态RAM

动态RAM比静态RAM要简单得多。图2.5展示了一种普通DRAM的结构。它只含有一个晶体管和一个电容器。显然，这种复杂性上的巨大差异意味着功能上的迥异。

![图2.5 1-T动态RAM](https://lrita.github.io/images/posts/memory/cpumemory.8.png)

图2.5 1-T动态RAM

动态RAM的状态是保持在电容器C中。晶体管M用来控制访问。如果要读取状态，拉升访问线AL，这时，可能会有电流流到数据线DL上，也可能没有，取决于电容器是否有电。如果要写入状态，先设置DL，然后升起AL一段时间，直到电容器充电或放电完毕。

动态RAM的设计有几个复杂的地方。由于读取状态时需要对电容器放电，所以这一过程不能无限重复，不得不在某个点上对它重新充电。更糟糕的是，为了容纳大量单元(现在一般在单个芯片上容纳109以上的RAM单元)，电容器的容量必须很小(0.000000000000001法拉以下)。这样，完整充电后大约持有几万个电子。即使电容器的电阻很大(若干兆欧姆)，仍然只需很短的时间就会耗光电荷，称为「泄漏」。

这种泄露就是现在的大部分DRAM芯片每隔64ms就必须进行一次刷新的原因。在刷新期间，对于该芯片的访问是不可能的，这甚至会造成半数任务的延宕。（相关内容请察看【highperfdram】一章）

这个问题的另一个后果就是无法直接读取芯片单元中的信息，而必须通过信号放大器将0和1两种信号间的电势差增大，才能分辨出来。

最后一个问题在于电容器的冲放电是需要时间的，这就导致了信号放大器读取的信号并不是典型的方波信号。所以当放大器输出信号的时候就需要一个小小的延宕，相关公式如下

![img](https://lrita.github.io/images/posts/memory/capcharge.png)

这就意味着需要一些时间（时间长短取决于电容C和电阻R）来对电容进行冲放电。另一个负面作用是，信号放大器的输出电流不能立即就作为信号载体使用。图2.6显示了冲放电的曲线，x轴表示的是单位时间下的R*C。

![img](https://lrita.github.io/images/posts/memory/cpumemory.57.png)

不像SRAM可以即刻读取数据，当要读取DRAM的时候，必须花一点时间来等待电容的冲放电完全。这一点点的时间最终限制了DRAM的速度。

当然了，这种读取方式也是有好处的。最大的好处在于缩小了尺寸。一个DRAM的尺寸是小于SRAM的，SRAM还需要独立的电源来维持状态。DRAM单元结构上更简单也更规律，意味着把他们封装在一个[`die`](https://zhuanlan.zhihu.com/p/29767262)上更简单。

综上所述，由于不可思议的成本差异，除了一些特殊的硬件（包括路由器什么的）之外，我们的硬件大多是使用DRAM的。这一点深深的影响了咱们这些程序员，后文将会对此进行讨论。在此之前，我们还是先了解下DRAM的更多细节。

### 2.1.3 DRAM 访问

程序使用虚拟地址来访问内存位置。处理器转换虚拟地址到物理地址，最后通过内存控制器选择与地址相对应的RAM芯片。为了选择在RAM芯片上的单个内存单元，部分的物理地址以一组地址线的形式被传递。

从存储器控制器中单独地寻址存储器位置是完全不切实际的：4GB RAM需要 232 地址线。取而代之的是，使用较小的地址线集合将地址编码为二进制数。传递到DRAM芯片上的地址首先应该被解码。一个解码器有N个地址线和 2N 输出线。这些输出线将被用来选择内存单元。使用这个直接方法对于小容量芯片不是个大问题。

但是随着单元的增加，这种方法不再适合。一个1G容量的芯片（我反感那些SI前缀，对于我一个giga-bit将总是230 而不是109字节）将需要30条地址线和230 条选择线。在不牺牲速度的条件下，一个解码器的输出线随着地址线的增加而指数增长。一个30跟地址线的解码器需要很大的芯片尺寸，同时复杂性也增加了。更重要的是，在地址线上同步发送30个脉冲要比仅仅传递15脉冲困难的多。较少列能精确布局相同长度或时序正常（现代DRAM类型像DDR3能自动调整时序但是也有容忍的限度）。

![图2.7: Dynamic RAM Schematic](https://lrita.github.io/images/posts/memory/cpumemory.9.png)

图2.7: Dynamic RAM Schematic

图2.7展示了一个很高级别的一个DRAM芯片，DRAM被组织在行和列里。他们能在对齐在一行里但DRAM芯片需要一个大的解码器。通阵列方法，该设计可以通过一个解码器和一半大小的编码器来实现。（编码器和解码器是等价的，当写数据时，编码器像解码器一样工作。那么从现在开始我们开始忽略其区别）。在例子中，行地址线a0和a1通过行地址解码器（¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯）选择整行单元格的地址线。当读的时候，内容可以通过列地址选择器（¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯）访问。依这发生了许多次在许多DRAM芯片产生一个总记录数的字节匹配给一个宽范围的数据总线。这样的操作会在芯片上并行的发生很多次，以产生与数据总线宽度相同的数据。

对于写操作，内存单元的数据新值被放到了数据总线，当使用¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯和¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯方式选中内存单元时，将其存储在单元格中。这是一个相当直观的设计，在现实中——很显然——会复杂得多，在数据将在数据总线上读取之前，需要有规定的延迟。如前一节所述，电容不会立刻自动放电，从内存单元发出的信号是如此这微弱以至于它需要被放大。对于写，必须规定从数据¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯和¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯操作完成后，需要有多长的延时，才能将新值存在在单元中（再次，电容器不能立即充电或放电）。这些时间常数对于DRAM芯片的性能是至关重要的，我们将在下章讨论它。

另一个关于伸缩性的问题是，用30根地址线连接到每一个RAM芯片是行不通的。芯片的针脚是非常珍贵的资源，以至数据必须尽可能并行地传输（比如：64位为一组）。内存控制器必须有能力解析每一个RAM模块（RAM芯片集合）。如果因为性能的原因要求并发行访问多个RAM模块并且每个RAM模块需要自己独占的30或多个地址线，那么对于8个RAM模块，仅仅是解析地址，内存控制器就需要240+之多的针脚。

在很长一段时间里，地址线被复用以解决DRAM芯片的这些次要的可扩展性问题。这意味着地址被转换成两部分。第一部分由地址位a0和a1选择行（如图2.7）。这个选择保持有效直到撤销。然后是第二部分，地址位a2和a3选择列。关键差别在于：只需要两根外部地址线。需要一些很少的线指明¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯和¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯信号有效，但是把地址线的数目减半所付出的代价更小。可是地址复用也带来自身的一些问题。我们将在2.2章中提到。

### 2.1.4 总结

如果这章节的内容有些难以应付，不用担心。纵观这章节的重点，有：

- 为什么不是所有的存储器都是SRAM的原因
- 存储单元需要单独选择来使用
- 地址线数目直接负责存储控制器，主板，DRAM模块和DRAM芯片的成本
- 在读或写操作结果之前需要占用一段时间是可行的

接下来的章节会涉及更多的有关访问DRAM存储器的实际操作的细节。我们不会提到更多有关访问SRAM的具体内容，它通常是直接寻址。这里是由于速度和有限的SRAM存储器的尺寸。SRAM现在应用在CPU的高速缓存和芯片，它们的连接件很小而且完全能在CPU设计师的掌控之下。我们以后会讨论到CPU高速缓存这个主题，但我们所需要知道的是SRAM存储单元是有确定的最大速度，这取决于花在SRAM上的艰难的尝试。这速度与CPU核心相比略慢一到两个数量级。

## 2.2 DRAM访问细节

在上文介绍DRAM的时候，我们已经看到DRAM芯片为了节约资源，对地址进行了复用。而且，访问DRAM单元是需要一些时间的，因为电容器的放电并不是瞬时的。此外，我们还看到，DRAM需要不停地刷新。在这一节里，我们将把这些因素拼合起来，看看它们是如何决定DRAM的访问过程。

我们将主要关注在当前的技术上，不会再去讨论异步DRAM以及它的各种变体。如果对它感兴趣，可以去参考[highperfdram]及[arstechtwo]。我们也不会讨论Rambus DRAM(RDRAM)，虽然它并不过时，但在系统内存领域应用不广。我们将主要介绍同步DRAM(SDRAM)及其后继者双倍速DRAM(DDR)。

同步DRAM，顾名思义，是相对于时间源工作的。由内存控制器提供一个时钟，时钟的频率决定了前端总线(FSB)的速度。FSB是内存控制器提供给DRAM芯片的接口。在我写作本文的时候，FSB已经达到800MHz、1066MHz，甚至1333MHz，并且下一代的1600MHz也已经宣布。但这并不表示时钟频率有这么高。实际上，目前的总线都是双倍或四倍传输的，每个周期传输2次或4次数据。报的越高，卖的越好，所以这些厂商们喜欢把四倍传输的200MHz总线宣传为“有效的”800MHz总线。

以今天的SDRAM为例，每次数据传输包含64位，即8字节。所以FSB的传输速率应该是有效总线频率乘于8字节(对于4倍传输200MHz总线而言，传输速率为6.4GB/s)。听起来很高，但要知道这只是峰值速率，实际上无法达到的最高速率。我们将会看到，与RAM模块交流的协议有大量时间是处于非工作状态，不进行数据传输。我们必须对这些非工作时间有所了解，并尽量缩短它们，才能获得最佳的性能。

### 2.2.1 读访问协议

![图2.8: SDRAM读访问的时序](https://lrita.github.io/images/posts/memory/cpumemory.10.png)

图2.8: SDRAM读访问的时序

图2.8展示了某个DRAM模块一些连接器上的活动，可分为三个阶段，图上以不同颜色表示。按惯例，时间为从左向右流逝。这里忽略了许多细节，我们只关注时钟频率、¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯和¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯信号、地址总线和数据总线。首先，内存控制器将行地址放在地址总线上，并拉低¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯信号，读周期开始。所有信号都在时钟(CLK)的上升沿读取，因此，只要信号在读取的时间点上保持稳定，就算不是标准的方波也没有关系。设置行地址会促使RAM芯片锁住指定的行。

¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯信号在tRCD(¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯到¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯时延)个时钟周期后发出。内存控制器将列地址放在地址总线上，拉低¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯线。这里我们可以看到，地址的两个组成部分是怎么通过同一条总线传输的。

至此，寻址结束，是时候传输数据了。但RAM芯片任然需要一些准备时间，这个时间称为¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯时延(CL)。在图2.8中CL为2。这个值可大可小，它取决于内存控制器、主板和DRAM模块的质量。CL还可能是半周期。假设CL为2.5，那么数据将在蓝色区域内的第一个下降沿准备就绪。

既然数据的传输需要这么多的准备工作，仅仅传输一个字显然是太浪费了。因此，DRAM模块允许内存控制指定本次传输多少数据。可以是2、4或8个字。这样，就可以一次填满高速缓存的整条线，而不需要额外的¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯/¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯序列。另外，内存控制器还可以在不重置行选择的前提下发送新的¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯信号。这样，读取或写入连续的地址就可以变得非常快，因为不需要发送¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯信号，也不需要把行置为非激活状态(见下文)。是否要将行保持为“打开”状态是内存控制器判断的事情。让它一直保持打开的话，对真正的应用会有不好的影响(参见[highperfdram])。¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯信号的发送仅与RAM模块的命令速率(Command Rate)有关(常常记为Tx，其中x为1或2，高性能的DRAM模块一般为1，表示在每个周期都可以接收新命令)。

在上图中，SDRAM的每个周期输出一个字的数据。这是第一代的SDRAM。而DDR可以在一个周期中输出两个字。这种做法可以减少传输时间，但无法降低时延。DDR2尽管看上去不同，但在本质上也是相同的做法。对于DDR2，不需要再深入介绍了，我们只需要知道DDR2更快、更便宜、更可靠、更节能(参见[ddrtwo])就足够了。

### 2.2.2 预充电与激活

图2.8并不完整，它只画出了访问DRAM的完整循环的一部分。在发送¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯信号之前，必须先把当前锁住的行置为非激活状态，并对新行进行预充电。在这里，我们主要讨论由于显式发送指令而触发以上行为的情况。协议本身作了一些改进，在某些情况下是可以省略这个步骤的，但预充电带来的时延还是会影响整个操作。

![图2.9: SDRAM的预充电与激活](https://lrita.github.io/images/posts/memory/cpumemory.11.png)

图2.9: SDRAM的预充电与激活

图2.9显示的是两次¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯信号的时序图。第一次的数据在CL周期后准备就绪。图中的例子里，是在SDRAM上，用两个周期传输了两个字的数据。如果换成DDR的话，则可以传输4个字。

即使是在一个命令速率为1的DRAM模块上，也无法立即发出预充电命令，而要等数据传输完成。在上图中，即为两个周期。刚好与CL相同，但只是巧合而已。预充电信号并没有专用线，某些实现是用同时降低写使能(WE)线和¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯线的方式来触发。这一组合方式本身没有特殊的意义(参见[micronddr])。

发出预充电信命令后，还需等待tRP(行预充电时间)个周期之后才能使行被选中。在图2.9中，这个时间(紫色部分)大部分与内存传输的时间(淡蓝色部分)重合。不错。但tRP大于传输时间，因此下一个¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯信号只能等待一个周期。

如果我们补充完整上图中的时间线，最后会发现下一次数据传输发生在前一次的5个周期之后。这意味着，数据总线的7个周期中只有2个周期才是真正在用的。再用它乘于FSB速度，结果就是，800MHz总线的理论速率6.4GB/s降到了1.8GB/s。真是太糟了，必须避免。第6节将介绍一些技术，可以帮助我们提高总线有效速率。程序员们也需要尽自己的努力。

SDRAM还有一些定时值，我们并没有谈到。在图2.9中，预充电命令仅受制于数据传输时间。除此之外，SDRAM模块在¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯信号之后，需要经过一段时间，才能进行预充电(记为tRAS)。它的值很大，一般达到tRP的2到3倍。如果在某个¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯信号之后，只有一个¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯信号，而且数据只传输很少几个周期，那么就有问题了。假设在图2.9中，第一个¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯信号是直接跟在一个¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯信号后面的，而tRAS为8个周期。那么预充电命令还需要被推迟一个周期，因为tRCD、CL和tRP加起来才7个周期。

DDR模块往往用w-z-y-z-T来表示。例如，2-3-2-8-T1，意思是:

- w 2 ¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯时延(CL)
- x 3 ¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯-to-¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯时延(tRCDtRCD)
- y 2 ¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯预充电时间(tRPtRP)
- z 8 激活到预充电时间(tRAStRAS)
- T T1 命令速率

当然，除以上的参数外，还有许多其它参数影响命令的发送与处理。但以上5个参数已经足以确定模块的性能。

在解读计算机性能参数时，这些信息可能会派上用场。而在购买计算机时，这些信息就更有用了，因为它们与FSB/SDRAM速度一起，都是决定计算机速度的关键因素。

喜欢冒险的读者们还可以利用它们来调优系统。有些计算机的BIOS可以让你修改这些参数。SDRAM模块有一些可编程寄存器，可供设置参数。BIOS一般会挑选最佳值。如果RAM模块的质量足够好，我们可以在保持系统稳定的前提下将减小以上某个时延参数。互联网上有大量超频网站提供了相关的文档。不过，这是有风险的，需要大家自己承担，可别怪我没有事先提醒哟。

### 2.2.3 重充电

谈到DRAM的访问时，重充电是常常被忽略的一个主题。在2.1.2中曾经介绍，必须不断刷新DRAM单元，这对于系统的其余部分来说，不是完全透明的。当某行在充电时是无法访问的。[highperfdram]的研究发现，“令人吃惊，DRAM刷新对性能有着巨大的影响”。

根据JEDEC规范，DRAM单元必须保持每64ms刷新一次。对于8192行的DRAM，这意味着内存控制器平均每7.8125µs就需要发出一个刷新命令(在实际情况下，由于刷新命令可以纳入队列，因此这个时间间隔可以更大一些)。刷新命令的调度由内存控制器负责。DRAM模块会记录上一次刷新行的地址，然后在下次刷新请求时自动对这个地址进行递增。

对于刷新及发出刷新命令的时间点，程序员无法施加影响。但我们在解读性能参数时有必要知道，它也是DRAM生命周期的一个部分。如果系统需要读取某个重要的字，而刚好它所在的行正在刷新，那么处理器将会被延迟很长一段时间。刷新的具体耗时取决于DRAM模块本身。

### 2.2.4 内存类型

我们有必要花一些时间来了解一下目前流行的内存，以及那些即将流行的内存。首先从SDR(单倍速)SDRAM开始，因为它们是DDR(双倍速)SDRAM的基础。SDR非常简单，内存单元和数据传输率是相等的。

![图2.10: SDR SDRAM的操作](https://lrita.github.io/images/posts/memory/cpumemory.15.png)

图2.10: SDR SDRAM的操作

在图2.10中，DRAM单元阵列能以等同于内存总线的速率输出内容。假设DRAM单元阵列工作在100MHz上，那么总线的数据传输率可以达到100Mb/s。所有组件的频率f保持相同。由于提高频率会导致耗电量增加，所以提高吞吐量需要付出很高的的代价。如果是很大规模的内存阵列，代价会非常巨大。{功率 = 动态电容 x 电压2 x 频率}。而且，提高频率还需要在保持系统稳定的情况下提高电压，这更是一个问题。因此，就有了DDR SDRAM(现在叫DDR1)，它可以在不提高频率的前提下提高吞吐量。

![图2.11 DDR1 SDRAM的操作](https://lrita.github.io/images/posts/memory/cpumemory.16.png)

图2.11 DDR1 SDRAM的操作

我们从图2.11上可以看出DDR1与SDR的不同之处，也可以从DDR1的名字里猜到那么几分，DDR1的每个周期可以传输两倍的数据，它的上升沿和下降沿都传输数据。有时又被称为“双泵(double-pumped)”总线。为了在不提升频率的前提下实现双倍传输，DDR引入了一个缓冲区。缓冲区的每条数据线都持有两位。它要求内存单元阵列的数据总线包含两条线。实现的方式很简单，用同一个列地址同时访问两个DRAM单元。对单元阵列的修改也很小。

SDR DRAM是以频率来命名的(例如，对应于100MHz的称为PC100)。为了让DDR1听上去更好听，营销人员们不得不想了一种新的命名方案。这种新方案中含有DDR模块可支持的传输速率(DDR拥有64位总线):

```
   100MHz x 64位 x 2 = 1600MB/s
```

于是，100MHz频率的DDR模块就被称为PC1600。由于1600 > 100，营销方面的需求得到了满足，听起来非常棒，但实际上仅仅只是提升了两倍而已。(我接受两倍这个事实，但不喜欢类似的数字膨胀戏法。)

![图2.12: DDR2 SDRAM的操作](https://lrita.github.io/images/posts/memory/cpumemory.17.png)

图2.12: DDR2 SDRAM的操作

为了更进一步，DDR2有了更多的创新。在图2.12中，最明显的变化是，总线的频率加倍了。频率的加倍意味着带宽的加倍。如果对单元阵列的频率加倍，显然是不经济的，因此DDR2要求I/O缓冲区在每个时钟周期读取4位。也就是说，DDR2的变化仅在于使I/O缓冲区运行在更高的速度上。这是可行的，而且耗电也不会显著增加。DDR2的命名与DDR1相仿，只是将因子2替换成4(四泵总线)。图2.13显示了目前常用的一些模块的名称。

| 阵列频率 | 总线频率 | 数据率    | 名称(速率) | 名称 (FSB) |
| -------- | -------- | --------- | ---------- | ---------- |
| 133MHz   | 266MHz   | 4,256MB/s | PC2-4200   | DDR2-533   |
| 166MHz   | 333MHz   | 5,312MB/s | PC2-5300   | DDR2-667   |
| 200MHz   | 400MHz   | 6,400MB/s | PC2-6400   | DDR2-800   |
| 250MHz   | 500MHz   | 8,000MB/s | PC2-8000   | DDR2-1000  |
| 266MHz   | 533MHz   | 8,512MB/s | PC2-8500   | DDR2-1066  |

图2.13: DDR2模块名

在命名方面还有一个拧巴的地方。FSB速度是用有效频率来标记的，即把上升、下降沿均传输数据的因素考虑进去，因此数字被撑大了。所以，拥有266MHz总线的133MHz模块有着533MHz的FSB“频率”。

DDR3要求更多的改变(这里指真正的DDR3，而不是显卡中假冒的GDDR3)。电压从1.8V下降到1.5V。由于耗电是与电压的平方成正比，因此可以节约30%的电力。加上管芯(die)的缩小和电气方面的其它进展，DDR3可以在保持相同频率的情况下，降低一半的电力消耗。或者，在保持相同耗电的情况下，达到更高的频率。又或者，在保持相同热量排放的情况下，实现容量的翻番。

DDR3模块的单元阵列将运行在内部总线的四分之一速度上，DDR3的I/O缓冲区从DDR2的4位提升到8位。见图2.14。

![图2.14: DDR3 SDRAM的操作](https://lrita.github.io/images/posts/memory/cpumemory.47.png)

图2.14: DDR3 SDRAM的操作

一开始，DDR3可能会有较高的CAS时延，因为DDR2的技术相比之下更为成熟。由于这个原因，DDR3可能只会用于DDR2无法达到的高频率下，而且带宽比时延更重要的场景。此前，已经有讨论指出，1.3V的DDR3可以达到与DDR2相同的CAS时延。无论如何，更高速度带来的价值都会超过时延增加带来的影响。

DDR3可能会有一个问题，即在1600Mb/s或更高速率下，每个通道的模块数可能会限制为1。在早期版本中，这一要求是针对所有频率的。我们希望这个要求可以提高一些，否则系统容量将会受到严重的限制。

图2.15显示了我们预计中各DDR3模块的名称。JEDEC目前同意了前四种。由于Intel的45nm处理器是1600Mb/s的FSB，1866Mb/s可以用于超频市场。随着DDR3的发展，可能会有更多类型加入。

![图2.15: DDR3模块名](https://lrita.github.io/images/posts/memory/cpumemory-21.png)

图2.15: DDR3模块名

所有的DDR内存都有一个问题：不断增加的频率使得建立并行数据总线变得十分困难。一个DDR2模块有240根引脚。所有到地址和数据引脚的连线必须被布置得差不多一样长。更大的问题是，如果多于一个DDR模块通过菊花链连接在同一个总线上，每个模块所接收到的信号随着模块的增加会变得越来越扭曲。DDR2规范允许每条总线（又称通道）连接最多两个模块，DDR3在高频率下只允许每个通道连接一个模块。每条总线多达240根引脚使得单个北桥无法以合理的方式驱动两个通道。替代方案是增加外部内存控制器（如图2.2），但这会提高成本。

这意味着商品主板所搭载的DDR2或DDR3模块数将被限制在最多四条，这严重限制了系统的最大内存容量。即使是老旧的32位IA-32处理器也可以使用64GB内存。即使是家庭对内存的需求也在不断增长，所以，某些事必须开始做了。

一种解法是，在处理器中加入内存控制器，我们在第2节中曾经介绍过。AMD的Opteron系列和Intel的CSI技术就是采用这种方法。只要我们能把处理器要求的内存连接到处理器上，这种解法就是有效的。如果不能，按照这种思路就会引入NUMA架构，当然同时也会引入它的缺点。而在有些情况下，我们需要其它解法。

Intel针对大型服务器方面的解法(至少在未来几年)，是被称为全缓冲DRAM(FB-DRAM)的技术。FB-DRAM采用与DDR2相同的器件，因此造价低廉。不同之处在于它们与内存控制器的连接方式。FB-DRAM没有用并行总线，而用了串行总线(Rambus DRAM had this back when, too, 而SATA是PATA的继任者，就像PCI Express是PCI/AGP的继承人一样)。串行总线可以达到更高的频率，串行化的负面影响，甚至可以增加带宽。使用串行总线后

- 每个通道可以使用更多的模块。
- 每个北桥/内存控制器可以使用更多的通道。
- 串行总线是全双工的(两条线)。

FB-DRAM只有69个脚。通过菊花链方式连接多个FB-DRAM也很简单。FB-DRAM规范允许每个通道连接最多8个模块。

在对比下双通道北桥的连接性，采用FB-DRAM后，北桥可以驱动6个通道，而且脚数更少——6x69对比2x240。每个通道的布线也更为简单，有助于降低主板的成本。

全双工的并行总线过于昂贵。而换成串行线后，这不再是一个问题，因此串行总线按全双工来设计的，这也意味着，在某些情况下，仅靠这一特性，总线的理论带宽已经翻了一倍。还不止于此。由于FB-DRAM控制器可同时连接6个通道，因此可以利用它来增加某些小内存系统的带宽。对于一个双通道、4模块的DDR2系统，我们可以用一个普通FB-DRAM控制器，用4通道来实现相同的容量。串行总线的实际带宽取决于在FB-DRAM模块中所使用的DDR2(或DDR3)芯片的类型。

我们可以像这样总结这些优势：

```
DDR2 FB-DRAM
```

![img](https://lrita.github.io/images/posts/memory/cpumemory-22.png)

如果在单个通道上使用多个DIMM，会有一些问题。信号在每个DIMM上都会有延迟(尽管很小)，也就是说，延迟是递增的。不过，如果在相同频率和相同容量上进行比较，FB-DRAM总是能快过DDR2及DDR3，因为FB-DRAM只需要在每个通道上使用一个DIMM即可。而如果说到大型内存系统，那么DDR更是没有商用组件的解决方案。

### 2.2.5 结论

通过本节，大家应该了解到访问DRAM的过程并不是一个快速的过程。至少与处理器的速度相比，或与处理器访问寄存器及缓存的速度相比，DRAM的访问不算快。大家还需要记住CPU和内存的频率是不同的。Intel Core 2处理器运行在2.933GHz，而1.066GHz FSB有11:1的时钟比率(注: 1.066GHz的总线为四泵总线)。那么，内存总线上延迟一个周期意味着处理器延迟11个周期。绝大多数机器使用的DRAM更慢，因此延迟更大。在后续的章节中，我们需要讨论延迟这个问题时，请把以上的数字记在心里。

前文中读命令的时序图表明，DRAM模块可以支持高速数据传输。每个完整行可以被毫无延迟地传输。数据总线可以100%被占。对DDR而言，意味着每个周期传输2个64位字。对于DDR2-800模块和双通道而言，意味着12.8GB/s的速率。

但是，除非是特殊设计，DRAM的访问并不总是串行的。访问不连续的内存区意味着需要预充电和¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯信号。于是，各种速度开始慢下来，DRAM模块急需帮助。预充电的时间越短，数据传输所受的惩罚越小。

硬件和软件的预取(参见第6.3节)可以在时序中制造更多的重叠区，降低延迟。预取还可以转移内存操作的时间，从而减少争用。我们常常遇到的问题是，在这一轮中生成的数据需要被存储，而下一轮的数据需要被读出来。通过转移读取的时间，读和写就不需要同时发出了。

## 2.3 主存的其它用户

除了CPU外，系统中还有其它一些组件也可以访问主存。高性能网卡或大规模存储控制器是无法承受通过CPU来传输数据的，它们一般直接对内存进行读写(直接内存访问，DMA)。在图2.1中可以看到，它们可以通过南桥和北桥直接访问内存。另外，其它总线，比如USB等也需要FSB带宽，即使它们并不使用DMA，但南桥仍要通过FSB连接到北桥。

DMA当然有很大的优点，但也意味着FSB带宽会有更多的竞争。在有大量DMA流量的情况下，CPU在访问内存时必然会有更大的延迟。我们可以用一些硬件来解决这个问题。例如，通过图2.3中的架构，我们可以挑选不受DMA影响的节点，让它们的内存为我们的计算服务。还可以在每个节点上连接一个南桥，将FSB的负荷均匀地分担到每个节点上。除此以外，还有许多其它方法。我们将在第6节中介绍一些技术和编程接口，它们能够帮助我们通过软件的方式改善这个问题。

最后，还需要提一下某些廉价系统，它们的图形系统没有专用的显存，而是采用主存的一部分作为显存。由于对显存的访问非常频繁(例如，对于1024x768、16bpp、60Hz的显示设置来说，需要95MB/s的数据速率)，而主存并不像显卡上的显存，并没有两个端口，因此这种配置会对系统性能、尤其是时延造成一定的影响。如果大家对系统性能要求比较高，最好不要采用这种配置。这种系统带来的问题超过了本身的价值。人们在购买它们时已经做好了性能不佳的心理准备。

## 参考

1. [What-Every-Programmer-Should-Know-About-Memory](https://lrita.github.io/images/posts/memory/What-Every-Programmer-Should-Know-About-Memory.pdf) [↩](https://lrita.github.io/2018/06/10/programmer-should-know-about-memory-0/#fnref:1)
2. [每个程序员都应该了解的内存知识【第一部分】](https://www.oschina.net/translate/what-every-programmer-should-know-about-memory-part1) [↩](https://lrita.github.io/2018/06/10/programmer-should-know-about-memory-0/#fnref:2)
3. [What every programmer should know about memory, Part 1](http://lwn.net/Articles/250967/) [↩](https://lrita.github.io/2018/06/10/programmer-should-know-about-memory-0/#fnref:3)


# 3 CPU 缓存

> 注 2
>
> 这个系列文章源于`What Every Programmer Should Know About Memory`[1](https://lrita.github.io/2018/06/30/programmer-should-know-about-memory-1/#fn:1)，粗读下来觉得很不错，最好能留存下来。同时发现这个系列的文章已经有一部分被人翻译了。故在此转发留存一份，毕竟存在自己收留的才是最可靠的，经常发现很多不错的文章链接失效的情况。
>
> 本文转载自[2](https://lrita.github.io/2018/06/30/programmer-should-know-about-memory-1/#fn:2)，翻译自[3](https://lrita.github.io/2018/06/30/programmer-should-know-about-memory-1/#fn:3)。本人进行了轻微的修改，感觉更符合原义。

现在的 CPU 比 25 年前要精密得多了。在那个年代，CPU 的频率与内存总线的频率基本在同一层面上。内存的访问速度仅比寄存器慢那么一点点。但是，这一局面在上世纪90年代被打破了。CPU 的频率大大提升，但内存总线的频率与内存芯片的性能却没有得到成比例的提升。并不是因为造不出更快的内存，只是因为太贵了。内存如果要达到目前CPU那样的速度，那么它的造价恐怕要贵上好几个数量级。

如果有两个选项让你选择，一个是速度非常快、但容量很小的内存，一个是速度还算快、但容量很多的内存，如果你的工作集比较大，超过了前一种情况，那么人们总是会选择第二个选项。原因在于辅助存储介质(一般为磁盘)的速度。由于工作集超过主存，那么必须用辅存来保存交换出去的那部分数据，而辅存的速度往往要比主存慢上好几个数量级。


好在这问题也并不全然是非黑即白的选择。在配置大量 DRAM 的同时，我们还可以配置少量 SRAM。将地址空间的某个部分划给 SRAM，剩下的部分划给 DRAM。一般来说，SRAM 可以当作扩展的寄存器来使用。

上面的做法看起来似乎可以，但实际上并不可行。首先，将 SRAM 内存映射到进程的虚拟地址空间就是个非常复杂的工作，而且，在这种做法中，每个进程都需要管理这个 SRAM 区内存的分配。每个进程可能有大小完全不同的 SRAM 区，而组成程序的每个模块也需要索取属于自身的 SRAM，更引入了额外的同步需求。简而言之，快速内存带来的好处完全被额外的管理开销给抵消了。

因此，SRAM 是作为 CPU 自动使用和管理的一个资源，而不是由 OS 或者用户管理的。在这种模式下，SRAM  用来复制保存（或者叫缓存）主内存中有可能即将被 CPU 使用的数据。这意味着，在较短时间内，CPU 很有可能重复运行某一段代码，或者重复使用某部分数据。从代码上看，这意味着 CPU 执行了一个循环，所以相同的代码一次又一次地执行（空间局部性的绝佳例子）。数据访问也相对局限在一个小的区间内。即使程序使用的物理内存不是相连的，在短期内程序仍然很有可能使用同样的数据（时间局部性）。这个在代码上表现为，程序在一个循环体内调用了入口一个位于另外的物理地址的函数。这个函数可能与当前指令的物理位置相距甚远，但是调用的时间差不大。在数据上表现为，程序使用的内存是有限的（相当于工作集的大小）。但是实际上由于 RAM 的随机访问特性，程序使用的物理内存并不是连续的。正是由于空间局部性和时间局部性的存在，我们才提炼出今天 CPU 缓存的概念。

我们先用一个简单的计算来展示一下高速缓存的效率。假设，访问主存需要200个周期，而访问高速缓存需要15个周期。如果使用100个数据元素100次，那么在没有高速缓存的情况下，需要2000000个周期，而在有高速缓存、而且所有数据都已被缓存的情况下，只需要168500个周期。节约了91.5%的时间。

用作高速缓存的 SRAM 容量比主存小得多。以我的经验来说，高速缓存的大小一般是主存的千分之一左右(目前一般是4GB主存、4MB缓存)。这一点本身并不是什么问题。只是，计算机一般都会有比较大的主存，因此工作集的大小总是会大于缓存。特别是那些运行多进程的系统，它的工作集大小是所有进程加上内核的总和。

处理高速缓存大小的限制需要制定一套很好的策略来决定在给定的时间内什么数据应该被缓存。由于不是所有工作集的数据都是在完全相同的时间段内被使用，因此我们可以用一些技术手段将需要用到的数据临时替换那些当前并未使用的缓存数据。这种预取将会减少部分访问主存的成本，因为它与程序的执行是异步的。所有的这些技术将会使高速缓存在使用的时候看起来比实际更大。我们将在3.3节讨论这些问题。 我们将在第6章讨论如何让这些技术能很好地帮助程序员，让处理器更高效地工作。

## 3.1 高速缓存的位置

在深入介绍高速缓存的技术细节之前，有必要说明一下它在现代计算机系统中所处的位置。

<p align="center">
<img src="https://lrita.github.io/images/posts/memory/cpumemory.1.png"/>
图3.1: 最简单的高速缓存配置图
</p>

图3.1展示了最简单的高速缓存配置。早期的一些系统就是类似的架构。在这种架构中，CPU核心不再直连到主存。{在一些更早的系统中，高速缓存像CPU与主存一样连到系统总线上。那种做法更像是一种hack，而不是真正的解决方案。}数据的读取和存储都经过高速缓存。CPU核心与高速缓存之间是一条特殊的快速通道。在简化的表示法中，主存与高速缓存都连到系统总线上，这条总线同时还用于与其它组件通信。我们管这条总线叫“FSB”——就是现在称呼它的术语，参见第2.2节。在这一节里，我们将忽略北桥。

在过去的几十年，经验表明使用了冯诺伊曼结构的计算机，将用于代码和数据的高速缓存分开是存在巨大优势的。自1993年以来，Intel 并且一直坚持使用独立的代码和数据高速缓存。由于所需的代码和数据的内存区域是几乎相互独立的，这就是为什么独立缓存工作得更完美的原因。近年来，独立缓存的另一个优势慢慢显现出来：常见处理器解码 指令的步骤是缓慢的，尤其当管线为空的时候，往往会伴随着错误的预测或无法预测的分支的出现， 将高速缓存技术用于 指令 解码可以加快其执行速度。

在高速缓存出现后不久，系统变得更加复杂。高速缓存与主存之间的速度差异进一步拉大，直到加入了另一级缓存。新加入的这一级缓存比第一级缓存更大，但是更慢。由于加大一级缓存的做法从经济上考虑是行不通的，所以有了二级缓存，甚至现在的有些系统拥有三级缓存，如图3.2所示。随着单个CPU中核数的增加，未来甚至可能会出现更多层级的缓存。

<p align="center">
<img src="https://lrita.github.io/images/posts/memory/cpumemory.2.png"/>
图3.2: 三级缓存的处理器
</p>

图3.2展示了三级缓存，并介绍了本文将使用的一些术语。`L1d`是一级数据缓存，`L1i`是一级指令缓存，等等。请注意，这只是示意图，真正的数据流并不需要流经上级缓存。CPU的设计者们在设计高速缓存的接口时拥有很大的自由。而程序员是看不到这些设计的。

另外，我们有多核CPU，每个核心可以有多个“线程”。核心与线程的不同之处在于，核心拥有独立的硬件资源({早期的多核CPU甚至有独立的二级缓存。})。在不同时使用相同资源(比如，通往外界的连接)的情况下，核心可以完全独立地运行。而线程只是共享资源。Intel的线程只有独立的寄存器，而且还有限制——不是所有寄存器都独立，有些是共享的。综上，现代CPU的结构就像图3.3所示。

<p align="center">
<img src="https://lrita.github.io/images/posts/memory/cpumemory.3.png"/>
</p>

在上图中，有两个处理器，每个处理器有两个核心，每个核心有两个线程。线程们共享一级缓存。核心(以深灰色表示)有独立的一级缓存，同时共享二级缓存。处理器(淡灰色)之间不共享任何缓存。这些信息很重要，特别是在讨论多进程和多线程情况下缓存的影响时尤为重要。

## 3.2 高级的缓存操作

了解成本和节约使用缓存，我们必须结合在第二节中讲到的关于计算机体系结构和RAM技术，以及前一节讲到的缓存描述来探讨。

默认情况下，CPU核心所有的数据的读或写都存储在缓存中。当然，也有内存区域不能被缓存的，但是这种情况只发生在操作系统的实现者对数据考虑的前提下；对程序实现者来说，这是不可见的。这也说明，程序设计者可以故意绕过某些缓存，不过这将是第六节中讨论的内容了。

如果CPU需要访问某个字(word)，先检索缓存。很显然，缓存不可能容纳主存所有内容(否则还需要主存干嘛)。系统用字的内存地址来对缓存条目进行标记。如果需要读写某个地址的字，那么根据标签来检索缓存即可。这里用到的地址可以是虚拟地址，也可以是物理地址，取决于缓存的具体实现。

缓存的标签是需要额外空间的，用字(word)作为缓存的粒度显然毫无效率。比如，在x86机器上，32位字的标签可能需要32位，甚至更长。另一方面，由于空间局部性的存在，与当前地址相邻的地址有很大可能会被一起访问。再回忆下2.2.1节——内存模块在传输位于同一行上的多份数据时，由于不需要发送新 ¯¯¯¯¯¯¯¯¯¯¯¯CASCAS¯ 信号，甚至不需要发送 ¯¯¯¯¯¯¯¯¯¯¯¯RASRAS¯ 信号，因此可以实现很高的效率。基于以上的原因，缓存条目并不存储单个字(word)，而是存储若干连续字组成的“线”(cache line)。在早期的缓存中，线长是32字节，现在一般是64字节。对于64位宽的内存总线，每条线需要8次传输。而DDR对于这种传输模式的支持更为高效。

当处理器需要内存中的某块数据时，整条cache line被装入`L1d`。cache line的地址通过对内存地址进行掩码操作生成。对于64字节的cache line，地址的低6 bit为0，这些不使用的bit位（低6bit）作为线内偏移量。其它的位作为标签，并用于在缓存内定位。在实践中，我们将地址分为三个部分。32位地址的情况如下:

<p align="center">
<img src="https://lrita.github.io/images/posts/memory/cpumemory.12.png"/>
</p>

如果cache line长度为 2O2O，那么地址的低`O`位用作线内偏移量，接着的`S`位选择“缓存集”。后面我们会说明使用缓存集的原因。现在只需要明白一共有 2S2S 个缓存集就够了。剩下的`32 - S - O` = `T`位组成缓存标签，它们用来区分在缓存集中别名相同的各条线{有相同`S`部分的cache line被称为有相同的别名}。`S`位用于定位缓存集，不需要存储，因为属于同一缓存集的所有线的`S`部分都是相同的。

当某条指令修改内存时，仍然要先装入cache line，因为任何指令都不可能同时修改整条线(只有一个例外——第6.1节中将会介绍的写合并(write-combine))。因此需要在写操作前先把cache line装载进来。如果cache line被写入，但还没有写回主存，那就是所谓的“脏了”。脏了的线一旦写回主存，脏标记即被清除。

为了加载新数据，基本上总是要先在缓存中清理出位置。`L1d`将内容逐出`L1d`，推入`L2`(线长相同)。当然，`L2`也需要清理位置。于是`L2`将内容推入`L3`，最后`L3`将它推入主存。这种逐出操作一级比一级昂贵。这里所说的是现代AMD和VIA处理器所采用的独占型缓存(exclusive cache)。而Intel采用的是包容型缓存(inclusive cache)，{并不完全正确，Intel有些缓存是独占型的，还有一些缓存具有独占型缓存的特点。}`L1d`的每条线同时存在于`L2`里。对这种缓存，逐出操作就很快了。如果有足够`L2`，对于相同内容存在不同地方造成内存浪费的缺点可以降到最低，而且在逐出时非常有利。而独占型缓存在装载新数据时只需要操作`L1d`，不需要碰`L2`，因此会比较快。

处理器体系结构中定义的作为存储器的模型只要还没有改变，那就允许多CPU按照自己的方式来管理高速缓存。这表示，例如，设计优良的处理器，利用很少或根本没有内存总线活动，并主动写回主内存脏高速缓存行。这种高速缓存架构在如x86和x86-64各种各样的处理器间存在。制造商之间，即使在同一制造商生产的产品中，证明了的内存模型抽象的力量。

在对称多处理器（SMP）架构的系统中，CPU的高速缓存不能独立的工作。在任何时候，所有的处理器都应该拥有相同的内存内容。保证这样的统一的内存视图被称为“缓存一致性”。如果在其自己的高速缓存和主内存间，处理器设计简单，它将不会看到在其他处理器上的脏高速缓存行的内容。从一个处理器直接访问另一个处理器的高速缓存这种模型设计代价将是非常昂贵的，它是一个相当大的瓶颈。相反，当另一个处理器要读取或写入到高速cache line上时，处理器会去检测。

如果CPU检测到一个写访问，而且该CPU的缓存中已经缓存了一个cache line的原始副本，那么这个cache line将被标记为无效的cache line。接下来在引用这个cache line之前，需要重新加载该cache line。需要注意的是读访问并不会导致cache line被标记为无效的。

更精确的缓存实现需要考虑到其他更多的可能性，比如第二个CPU在读或者写它的cache line时，发现该cache line在第一个CPU的缓存中被标记为脏数据了，此时我们就需要做进一步的处理。在这种情况下，主存储器已经失效，第二个CPU需要读取第一个CPU的cache line。通过测试，我们知道在这种情况下第一个CPU会将自己的cache line数据自动发送给第二个CPU。这种操作是绕过主存储器的，但是有时候存储控制器是可以直接将第一个CPU中的cache line数据存储到主存储器中。对第一个CPU的缓存的写访问会导致本地cache line的所有拷贝被标记为无效。

随着时间的推移，一大批缓存一致性协议已经开发。其中，最重要的是MESI,我们将在第3.3.4节进行介绍。以上结论可以概括为几个简单的规则:

- 一个脏cache line不存在于任何其他处理器的缓存之中。
- 同一cache line中的干净拷贝可以驻留在任意多个其他缓存之中。

如果遵守这些规则，处理器甚至可以在多处理器系统中更加有效的使用它们的缓存。所有的处理器需要做的就是监控其他每一个写访问和比较本地缓存中的地址。在下一节中，我们将介绍更多细节方面的实现，尤其是存储开销方面的细节。

最后，我们至少应该关注高速缓存命中或未命中带来的消耗。下面是英特尔`Pentium M`的数据：

| To Where    | Cycles |
| ----------- | ------ |
| `Register`  | <= 1   |
| `L1d`       | ~3     |
| `L2`        | ~14    |
| Main Memory | ~240   |

这是在CPU周期中的实际访问时间。有趣的是，对于L2高速缓存的访问时间很大一部分（甚至是大部分）是由线路的延迟引起的。这是一个限制，增加高速缓存的大小变得更糟。只有当减小时（例如，从60纳米的Merom到45纳米Penryn处理器），可以提高这些数据。

表格中的数字看起来很高，但是，幸运的是，整个成本不必须负担每次出现的缓存加载和缓存失效。某些部分的成本可以被隐藏。现在的处理器都使用不同长度的内部管道，在管道内指令被解码，并为准备执行。如果数据要传送到一个寄存器，那么部分的准备工作是从存储器（或高速缓存）加载数据。如果内存加载操作在管道中足够早的进行，它可以与其他操作并行发生，那么加载的全部发销可能会被隐藏。对`L1d`常常可能如此；某些有长管道的处理器的`L2`也可以。

提早启动内存的读取有许多障碍。它可能只是简单的不具有足够资源供内存访问，或者地址从另一个指令获取，然后加载的最终地址才变得可用。在这种情况下，加载成本是不能隐藏的（完全的）。

对于写操作，CPU并不需要等待数据被安全地放入内存。只要指令具有类似的效果，就没有什么东西可以阻止CPU走捷径了。它可以早早地执行下一条指令，甚至可以在影子寄存器(shadow register)的帮助下，更改这个写操作将要存储的数据。

<p align="center">
<img src="https://lrita.github.io/images/posts/memory/cpumemory.21.png"/>
图3.4: 随机写操作的访问时间
</p>

图3.4展示了缓存的效果。关于产生图中数据的程序，我们会在稍后讨论。这里大致说下，这个程序是连续随机地访问某块大小可配的内存区域。每个数据项的大小是固定的。数据项的多少取决于选择的工作集大小。Y轴表示处理每个元素平均需要多少个CPU周期，注意它是对数刻度。X轴也是同样，工作集的大小都以2的n次方表示。

图中有三个比较明显的不同阶段。很正常，这个处理器有`L1d`和`L2`，没有`L3`。根据经验可以推测出，`L1d`有213213字节，而`L2`有220220字节。因为，如果整个工作集都可以放入`L1d`，那么只需不到10个周期就可以完成操作。如果工作集超过`L1d`，处理器不得不从`L2`获取数据，于是时间飘升到28个周期左右。如果工作集更大，超过了`L2`，那么时间进一步暴涨到480个周期以上。这时候，许多操作将不得不从主存中获取数据。更糟糕的是，如果修改了数据，还需要将这些脏了的cache line写回内存。

看了这个图，大家应该会有足够的动力去检查代码、改进缓存的利用方式了吧？这里的性能改善可不只是微不足道的几个百分点，而是几个数量级呀。在第6节中，我们将介绍一些编写高效代码的技巧。而下一节将进一步深入缓存的设计。虽然精彩，但并不是必修课，大家可以选择性地跳过。

## 3.3 CPU缓存实现的细节

缓存的实现者们都要面对一个问题：主存中每一个单元都可能需被缓存。如果程序的工作集很大，就会有许多内存位置为了缓存而打架。前面我们曾经提过缓存与主存的容量比，1:1000 也十分常见。

### 3.3.1 关联性

我们可以让缓存的每条线能存放任何内存地址的数据。这就是所谓的全关联缓存(fully associative cache)。对于这种缓存，处理器为了访问某条线，将不得不检索所有线的标签。而标签则包含了整个地址，而不仅仅只是线内偏移量(也就意味着，图 3.2 中的`S`为0)。

高速缓存有类似这样的实现，但是，看看在今天使用的`L2`的数目，表明这是不切实际的。给定4MB的高速缓存和64B的高速缓存段，高速缓存将有65,536个项。为了达到足够的性能，缓存逻辑必须能够在短短的几个时钟周期内，从所有这些项中，挑一个匹配给定的标签。实现这一点的工作将是巨大的。

![](https://lrita.github.io/images/posts/memory/cpumemory.70.png)
Figure 3.5: 全关联高速缓存原理图

对于每个高速缓存行，比较器是需要比较大标签（注意，`S`是零）。每个连接旁边的字母表示位的宽度。如果没有给出，它是一个单比特线。每个比较器都要比较两个`T`位宽的值。然后，基于该结果，适当的高速缓存行的内容被选中，并使其可用。这需要合并多套`O`数据线，因为他们是缓存桶（译注：这里类似把`O`输出接入多选器，所以需要合并）。实现仅仅一个比较器，需要晶体管的数量就非常大，特别是因为它必须非常快。没有迭代比较器是可用的。节省比较器的数目的唯一途径是通过反复比较标签，以减少它们的数目。这是不适合的，出于同样的原因，迭代比较器不可用：它消耗的时间太长。

全关联高速缓存对 小缓存是实用的（例如，在某些Intel处理器的TLB缓存是全关联的），但这些缓存都很小，非常小的。我们正在谈论的最多几十项。

对于`L1i`，`L1d`和更高级别的缓存，需要采用不同的方法。可以做的就是是限制搜索。最极端的限制是，每个标签映射到一个明确的缓存条目。计算很简单：给定的4MB/64B缓存有65536项，我们可以使用地址的bit6到bit21（16位）来直接寻址高速缓存的每一个项。地址的低6位作为高速缓存段的索引。

![img](https://lrita.github.io/images/posts/memory/cpumemory.71.png)

图3.6: 直接映射缓存原理图

在图3.6中可以看出，这种直接映射的高速缓存，速度快，比较容易实现。它只是需要一个比较器，一个多路复用器（在这个图中有两个，标记和数据是分离的，但是对于设计这不是一个硬性要求），和一些逻辑来选择只是有效的高速缓存行的内容。由于速度的要求，比较器是复杂的，但是现在只需要一个，结果是可以花更多的精力让其变得快速。这种方法的复杂性在于在多路复用器。一个简单的多路转换器中的晶体管的数量增速是`O(log N)`的，其中`N`是高速cache line的数目。这是可以容忍的，但可能会很慢，在某种情况下，速度可提升，通过增加多路复用器晶体管数量，来并行化的一些工作和自身增速。晶体管的总数只是随着快速增长的高速缓存缓慢的增加，这使得这种解决方案非常有吸引力。但它有一个缺点：只有用于直接映射地址的相关的地址位均匀分布，程序才能很好工作。如果分布的不均匀，而且这是常态，一些缓存项频繁的使用，并因此多次被换出，而另一些则几乎不被使用或一直是空的。

![img](https://lrita.github.io/images/posts/memory/cpumemory.73.png)

图3.7: 组关联高速缓存原理图

可以通过使高速缓存的组关联来解决此问题。组关联结合高速缓存的全关联和直接映射高速缓存特点，在很大程度上避免这些设计的弱点。图3.7显示了一个组关联高速缓存的设计。标签和数据存储分成不同的组并可以通过地址选择。这类似直接映射高速缓存。但是，少量个数的元素可以在同一个高速缓存组缓存，而不是一个缓存组只有一个元素，用于在高速缓存中的每个设定值是相同的一组值的缓存。所有组的成员的标签可以并行比较，这类似全关联缓存的功能。

其结果是高速缓存不容易被不幸地或故意被分配到同一缓存组中，同时高速缓存的大小并不限于由比较器的数目，可以以并行的方式实现。如果高速缓存增长，只（在该图中）增加列的数目，而不增加行数。只有高速缓存之间的关联性增加，行数才会增加。今天，处理器的`L2`高速缓存或更高的高速缓存，使用的关联性高达16路。 L1高速缓存通常使用8路。

![img](https://lrita.github.io/images/posts/memory/cpumem-table-0.png)

Table 3.1: 高速缓存大小，关联行，段大小的影响

给定我们4MB/64B高速缓存，8路组关联，相关的缓存留给我们的有8192组，只用标签的13位，就可以寻址缓集。要确定哪些（如果有的话）的缓存组设置中的条目包含寻址的高速缓存行，8个标签都要进行比较。在很短的时间内做出来是可行的。通过一个实验，我们可以看到，这是有意义的。

表3.1显示一个程序在改变缓存大小，缓存段大小和关联集大小时的`L2`高速缓存的缓存失效数量（根据Linux内核相关的方面人的说法，GCC在这种情况下，是他们所有中最重要的指标）。在7.2节中，我们将介绍工具来模拟此测试要求的高速缓存。

万一这还不是很明显，所有这些值之间的关系是高速缓存的大小为：

```
cache line size × associativity × number of sets
```

地址被映射到高速缓存使用

O=log2(cache line size)S=log2(number of sets)O=log2(cache line size)S=log2(number of sets)

在第3.2节中的图显示的方式

![img](https://lrita.github.io/images/posts/memory/cpumemory.33.png)

图3.8: 缓存段大小 vs 关联行 (CL=32)

图3.8表中的数据更易于理解。它显示一个固定的32个字节大小的cache line，对于一个给定的高速缓存大小，我们可以看出，关联行的增加，的确可以帮助明显减少高速缓存未命中的数量。对于8MB的缓存，从直接映射到2路组相联，可以减少近44％的高速缓存未命中。组相联高速缓存和直接映射缓存相比，该处理器可以把更多的工作集保持在缓存中。

在文献中，偶尔可以读到，引入关联行，和加倍高速缓存的大小具有相同的效果。在从4M缓存跃升到8MB缓存的极端的情况下，这是正确的。关联行再提高一倍那就肯定不正确啦。正如我们所看到的数据，后面的收益要小得多。我们不应该完全低估它的效果，虽然。在示例程序中的内存使用的峰值是5.6M。因此，具有8MB缓存不太可能有很多（两个以上）使用相同的高速缓存集。从较小的缓存的关联性的巨大收益可以看出，较大工作集可以节省更多。

在一般情况下，增加8以上的高速缓存之间的关联性似乎对只有一个单线程工作量影响不大。随着介绍一个使用共享`L2`的多核处理器，形势发生了变化。现在你基本上有两个程序命中相同的缓存，实际上导致高速缓存减半（对于四核处理器是1/4）。因此，可以预期，随着核的数目的增加，共享高速缓存的相关性也应增长。一旦这种方法不再可行（16 路组关联性已经很难）处理器设计者不得不开始使用共享的三级高速缓存和更高级别的，而`L2`高速缓存只被核的一个子集共享。

从图3.8中，我们还可以研究缓存大小对性能的影响。这一数据需要了解工作集的大小才能进行解读。很显然，与主存相同的缓存比小缓存能产生更好的结果，因此，缓存通常是越大越好。

上文已经说过，示例中最大的工作集为5.6M。它并没有给出最佳缓存大小值，但我们可以估算出来。问题主要在于内存的使用并不连续，因此，即使是16M的缓存，在处理5.6M的工作集时也会出现冲突(参见2路集合关联式16MB缓存vs直接映射式缓存的优点)。不管怎样，我们可以有把握地说，在同样5.6M的负载下，缓存从16MB升到32MB基本已没有多少提高的余地。但是，工作集是会变的。如果工作集不断增大，缓存也需要随之增大。在购买计算机时，如果需要选择缓存大小，一定要先衡量工作集的大小。原因可以参见图3.10。

![img](https://lrita.github.io/images/posts/memory/cpumemory.75.png)

图3.9: 测试的内存分布情况

我们执行两项测试。第一项测试是按顺序地访问所有元素。测试程序循着指针n进行访问，而所有元素是链接在一起的，从而使它们的被访问顺序与在内存中排布的顺序一致，如图3.9的下半部分所示，末尾的元素有一个指向首元素的引用。而第二项测试(见图3.9的上半部分)则是按随机顺序访问所有元素。在上述两个测试中，所有元素都构成一个单向循环链表。

### 3.3.2 Cache的性能测试

用于测试程序的数据可以模拟一个任意大小的工作集：包括读、写访问，随机、连续访问。在图3.4中我们可以看到，程序为工作集创建了一个与其大小和元素类型相同的数组：

```
struct l {
    struct l *n;
    long int pad[NPAD];
};
```

n字段将所有节点随机得或者顺序的加入到环形链表中，用指针从当前节点进入到下一个节点。pad字段用来存储数据，其可以是任意大小。在一些测试程序中，pad字段是可以修改的, 在其他程序中，pad字段只可以进行读操作。

在性能测试中，我们谈到工作集大小的问题，工作集使用结构体`l`定义的元素表示的。2N2N字节的工作集包含2Nsizeof(struct l)2Nsizeof(struct l)个元素. 显然`sizeof(struct l)`的值取决于`NPAD`的大小。在32位系统上，`NPAD=7`意味着数组的每个元素的大小为32字节，在64位系统上，`NPAD=7`意味着数组的每个元素的大小为64字节。

#### 单线程顺序访问

最简单的情况就是遍历链表中顺序存储的节点。无论是从前向后处理，还是从后向前，对于处理器来说没有什么区别。下面的测试中，我们需要得到处理链表中一个元素所需要的时间，以CPU时钟周期最为计时单元。图3.10显示了测试结构。除非有特殊说明, 所有的测试都是在`Pentium 4`64-bit平台上进行的，因此结构体`l`中`NPAD=0`，大小为8字节。

![img](https://lrita.github.io/images/posts/memory/cpumemory.22.png)

图 3.10: 顺序读访问`NPAD=0`

![img](https://lrita.github.io/images/posts/memory/cpumemory.23.png)

图 3.11: 顺序读多个字节

一开始的两个测试数据收到了噪音的污染。由于它们的工作负荷太小，无法过滤掉系统内其它进程对它们的影响。我们可以认为它们都是4个周期以内的。这样一来，整个图可以划分为比较明显的三个部分:

- 工作集小于214214字节的
- 工作集从215215字节到220220字节的
- 工作集大于221221字节的

这样的结果很容易解释——是因为处理器有16KB的`L1d`和1MB的`L2`。而在这三个部分之间，并没有非常锐利的边缘，这是因为系统的其它部分也在使用缓存，我们的测试程序并不能独占缓存的使用。尤其是L2，它是统一式的缓存，处理器的指令也会使用它(注: Intel使用的是包容式缓存)。

测试的实际耗时可能会出乎大家的意料。`L1d`的部分跟我们预想的差不多，在一台P4上耗时为4个周期左右。但`L2`的结果则出乎意料。大家可能觉得需要14个周期以上，但实际只用了9个周期。这要归功于处理器先进的处理逻辑，当它使用连续的内存区时，会**预取**下一条缓存线的数据。这样一来，当真正使用下一条线的时候，其实已经早已读完一半了，于是真正的等待耗时会比`L2`的访问时间少很多。

在工作集超过`L2`的大小之后，**预取**的效果更明显了。前面我们说过，主存的访问需要耗时200个周期以上。但在预取的帮助下，实际耗时保持在9个周期左右。200 vs 9，效果非常不错。

我们可以观察到**预取**的行为，至少可以间接地观察到。图3.11中有4条线，它们表示处理不同大小结构时的耗时情况。随着结构的变大，元素间的距离变大了。图中4条线对应的元素距离分别是0、56、120和248字节。

图中最下面的这一条线来自前一个图，但在这里更像是一条直线。其它三条线的耗时情况比较差。图中这些线也有比较明显的三个阶段，同时，在小工作集的情况下也有比较大的错误(请再次忽略这些错误)。在只使用`L1d`的阶段，这些线条基本重合。因为这时候还不需要**预取**，只需要访问`L1d`就行。

在`L2`阶段，三条新加的线基本重合，而且耗时比老的那条线高很多，大约在28个周期左右，差不多就是`L2`的访问时间。这表明，从`L2`到`L1d`的**预取**并没有生效。这是因为，对于最下面的线(`NPAD=0`)，由于结构小，8次循环后才需要访问一条新缓存线，而上面三条线对应的结构比较大，拿相对最小的`NPAD=7`来说，光是一次循环就需要访问一条新线，更不用说更大的`NPAD=15`和`NPAD=31`了。而预取逻辑是无法在每个周期装载新线的，因此每次循环都需要从`L2`读取，我们看到的就是从L2读取的时延。

更有趣的是工作集超过`L2`容量后的阶段。4条线远远地拉开了。元素的大小变成了主角，左右了性能。处理器应能识别每一步(stride)的大小，不去为`NPAD=15`和`NPAD=31`等获取那些实际并不需要的cache line(参见6.3.1)。元素大小对预取的约束是根源于硬件预取的限制：它无法跨越页边界。如果允许预取器跨越页边界，而下一页不存在或无效，那么OS还得去寻找它。这意味着，程序需要遭遇一次并非由它自己产生的页错误，这是完全不能接受的。在`NPAD=7`或者更大的时候，由于每个元素都至少需要一条cache line，预取器已经帮不上忙了，它没有足够的时间去从内存装载数据。

另一个导致慢下来的原因是TLB缓存的未命中。TLB是存储虚实地址映射的缓存，参见第4节。为了保持快速，TLB只有很小的容量。如果有大量页被反复访问，超出了TLB缓存容量，就会导致反复地进行地址翻译，这会耗费大量时间。TLB查找的代价分摊到所有元素上，如果元素越大，那么元素的数量越少，每个元素承担的那一份就越多。

为了观察TLB的影响，我们可以进行另两项测试。第一项：我们还是顺序存储列表中的元素，使`NPAD=7`，让每个元素占满整个cache line，第二项：我们将列表的每个元素存储在一个单独的页上，忽略每个页没有使用的部分以用来计算工作集的大小。（这样做可能不太一致，因为在前面的测试中，我计算了结构体中每个元素没有使用的部分，从而用来定义NPAD的大小，因此每个元素占满了整个页，这样以来工作集的大小将会有所不同。但是这不是这项测试的重点，预取的低效率多少使其有点不同）。结果表明，第一项测试中，每次列表的迭代都需要一个新的cache line，而且每64个元素就需要一个新的页。第二项测试中，每次迭代都会在一个新的页中加载一个新的cache line。

![img](https://lrita.github.io/images/posts/memory/cpumemory.59.png)

图 3.12: TLB 对顺序读的影响

结果见图3.12。该测试与图3.11是在同一台机器上进行的。基于可用RAM空间的有限性，测试设置容量空间大小为224224字节，这就需要1GB的容量将对象放置在分页上。图3.12中下方的红色曲线正好对应了图3.11中`NPAD=7`的曲线。我们看到不同的步长显示了高速缓存`L1d`和`L2`的大小。第二条曲线看上去完全不同，其最重要的特点是当工作容量到达213213字节时开始大幅度增长。这就是TLB缓存溢出的时候。我们能计算出一个64字节大小的元素的TLB缓存有64个输入。成本不会受页面错误影响，因为程序锁定了存储器以防止内存被换出。

可以看出，计算物理地址并把它存储在TLB中所花费的周期数量级是非常高的。图3.12的表格显示了一个极端的例子，但从中可以清楚的得到：TLB缓存效率降低的一个重要因素是NPAD值，效率随着NPAD值增大而降低。由于物理地址必须在cache line能被`L2`或主存读取之前计算出来，地址转换这个不利因素就增加了内存访问时间。这一点部分解释了为什么`NPAD=31`时每个列表元素的总花费比理论上的RAM访问时间要高。

![img](https://lrita.github.io/images/posts/memory/cpumemory.24.png)

图3.13 `NPAD=1`时的顺序读和写

通过查看链表元素被修改时测试数据的运行情况，我们可以窥见一些更详细的预取实现细节。图3.13显示了三条曲线。所有情况下元素宽度都为16个字节。第一条曲线“Follow”是熟悉的链表遍历在这里作为基线。第二条曲线，标记为“Inc”，仅仅在当前元素进入下一个前给其增加`pad[0]`成员。第三条曲线，标记为”Addnext0”，取出下一个元素的`pad[0]`链表元素并把它加和到当前链表元素的`pad[0]`成员。

在没运行时，大家可能会以为”Addnext0”更慢，因为它要做的事情更多：在没前进到下个元素之前就需要加载它的值。但实际的运行结果令人惊讶——在某些小工作集下，”Addnext0”比”Inc”更快。这是为什么呢？原因在于，系统一般会对下一个元素进行强制性**预取**。当程序前进到下个元素时，这个元素其实早已被预取在`L1d`里。因此，只要工作集比`L2`小，”Addnext0”的性能基本就能与”Follow”测试媲美。

但是，”Addnext0”比”Inc”更快超出`L2`，这是因为它需要从主存装载更多的数据。而在工作集达到221221字节时，”Addnext0”的耗时达到了28个周期，是同期”Follow”14周期的两倍。这个两倍也很好解释。”Addnext0”和”Inc”涉及对内存的修改，因此`L2`的逐出操作不能简单地把数据一扔了事，而必须将它们回写入内存。因此FSB的可用带宽变成了一半，传输等量数据的耗时也就变成了原来的两倍。

![img](https://lrita.github.io/images/posts/memory/cpumemory.25.png)

图3.14: 更大L2/L3缓存的优势

决定顺序式缓存处理性能的另一个重要因素是缓存容量。虽然这一点比较明显，但还是值得一说。图3.14展示了128字节长元素的测试结果(64位机，`NPAD=15`)。这次我们比较三台不同计算机的曲线，两台P4，一台Core 2。两台P4的区别是缓存容量不同，一台是32k的`L1d`和1M的`L2`，一台是16K的`L1d`、512k的`L2`和2M的`L3`。Core 2那台则是32k的`L1d`和4M的`L2`。

图中最有趣的地方，并不是Core 2如何大胜两台P4，而是工作集开始增长到连末级缓存也放不下，需要主存被大量调用时部分（图中最右侧部分）。

![img](https://lrita.github.io/images/posts/memory/cpumem-table-3.2.png)

表3.2: 顺序访问与随机访问时L2命中与未命中的情况，NPAD=0

与我们预计的相似，最末级缓存越大，曲线停留在`L2`访问耗时区的时间越长。在220220字节的工作集时，第二台P4(更老一些)比第一台P4要快上一倍，这要完全归功于更大的末级缓存。而Core 2拜它巨大的4M `L2`所赐，表现更为卓越。

对于随机的工作负荷而言，可能没有这么惊人的效果，但是，如果我们能将工作负荷进行一些裁剪，让它匹配末级缓存的容量，就完全可以得到非常大的性能提升。也是由于这个原因，有时候我们需要多花一些钱，买一个拥有更大缓存的处理器。

#### 单线程随机访问模式的测量

前面我们已经看到，处理器能够利用`L1d`到`L2`之间的预取消除访问主存、甚至是访问`L2`的时延。

![img](https://lrita.github.io/images/posts/memory/cpumemory.26.png)

图3.15: 顺序读取vs随机读取，NPAD=0

但是，如果换成随机访问或者不可预测的访问，情况就大不相同了。图3.15比较了顺序读取与随机读取的耗时情况。换成随机之后，处理器无法再有效地预取数据，只有少数情况下靠运气刚好碰到先后访问的两个元素挨在一起的情形。

图3.15中有两个需要关注的地方。首先，在大的工作集下需要非常多的周期。这台机器访问主存的时间大约为200-300个周期，但图中的耗时甚至超过了450个周期。我们前面已经观察到过类似现象(对比图3.11)。这说明，处理器的自动预取在这里起到了反效果。

其次，代表随机访问的曲线在各个阶段不像顺序访问那样保持平坦，而是不断攀升。为了解释这个问题，我们测量了程序在不同工作集下对`L2`的访问情况。结果如图3.16和表3.2。

从图中可以看出，当工作集大小超过`L2`时，未命中率(`L2`未命中次数/`L2`访问次数)开始上升。整条曲线的走向与图3.15有些相似: 先急速爬升，随后缓缓下滑，最后再度爬升。它与耗时图有紧密的关联。`L2`未命中率会一直爬升到100%为止。只要工作集足够大(并且内存也足够大)，就可以将L2中cache line随机选取或加载的可能性降到非常低。

缓存未命中率的攀升已经可以解释一部分的开销。除此以外，还有一个因素。观察表3.2的`L2/#Iter`列，可以看到每个循环对`L2`的使用次数在增长。由于工作集每次为上一次的两倍，如果没有缓存的话，内存的访问次数也将是上一次的两倍。在按顺序访问时，由于缓存的帮助及完美的预见性，对`L2`使用的增长比较平缓，完全取决于工作集的增长速度。

![img](https://lrita.github.io/images/posts/memory/cpumemory.32.png)

图3.16: L2d未命中率

![img](https://lrita.github.io/images/posts/memory/cpumemory.67.png)

图3.17: 页意义上(Page-Wise)的随机化，NPAD=7

而换成随机访问后，单位耗时的增长超过了工作集的增长，根源是TLB未命中率的上升。图3.17描绘的是`NPAD=7`时随机访问的耗时情况。这一次，我们修改了随机访问的方式。正常情况下是把整个列表作为一个块进行随机(以∞表示)，而其它11条线则是在小一些的块里进行随机。例如，标签为’60’的线表示以60页(245760字节)为单位进行随机。先遍历完这个块里的所有元素，再访问另一个块。这样一来，可以保证任意时刻使用的TLB条目数都是有限的。

`NPAD=7`对应于64字节，正好等于cache line的长度。由于元素顺序随机，硬件预取不可能有任何效果，特别是在元素较多的情况下。这意味着，分块随机时的`L2`未命中率与整个列表随机时的未命中率没有本质的差别。随着块的增大，曲线逐渐逼近整个列表随机对应的曲线。这说明，在这个测试里，性能受到TLB命中率的影响很大，如果我们能提高TLB命中率，就能大幅度地提升性能(在后面的一个例子里，性能提升了38%之多)。

### 3.3.3 写入时的行为

在我们开始研究多个线程或进程同时使用相同内存之前，先来看一下缓存实现的一些细节。我们要求缓存是一致的，而且这种一致性必须对用户级代码完全透明。而内核代码则有所不同，它有时候需要对缓存进行转储(flush)。

这意味着，如果对cache line进行了修改，那么在这个时间点之后，系统的结果应该是与没有缓存的情况下是相同的，即主存的对应位置也已经被修改的状态。这种要求可以通过两种方式或策略实现：

- 写穿透(write-through)
- 回写(write-back)

写穿透比较简单。当修改cache line时，处理器立即将它写入主存。这样可以保证主存与缓存的内容永远保持一致。当cache line被替代时，只需要简单地将它丢弃即可。这种策略很简单，但是速度比较慢。如果某个程序反复修改一个本地变量，可能导致FSB上产生大量数据流，而不管这个变量是不是有人在用，或者是不是短期变量。

回写比较复杂。当修改cache line时，处理器不再马上将它写入主存，而是打上已弄脏(dirty)的标记。当以后某个时间点cache line被丢弃时，这个已弄脏标记会通知处理器把数据回写到主存中，而不是简单地扔掉。

回写有时候会有非常不错的性能，因此较好的系统大多采用这种方式。采用回写时，处理器们甚至可以利用FSB的空闲容量来存储cache line。这样一来，当需要缓存空间时，处理器只需清除脏标记，丢弃cache line即可。

但回写也有一个很大的问题。当有多个处理器(或核心、超线程)访问同一块内存时，必须确保它们在任何时候看到的都是相同的内容。如果cache line在其中一个处理器上弄脏了(修改了，但还没回写主存)，而第二个处理器刚好要读取同一个内存地址，那么这个读操作不能去读主存，而需要读第一个处理器的cache line。在下一节中，我们将研究如何实现这种需求。

在此之前，还有其它两种缓存策略需要提一下:

- 写入合并
- 不可缓存

这两种策略用于真实内存不支持的特殊地址区，内核为地址区设置这些策略(x86处理器利用内存类型范围寄存器MTRR)，余下的部分自动进行。MTRR还可用于写穿透和回写策略的选择。

写入合并是一种有限的缓存优化策略，更多地用于显卡等设备之上的内存。由于设备的传输开销比本地内存要高的多，因此避免进行过多的传输显得尤为重要。如果仅仅因为修改了cache line上的一个字，就传输整条cache line，而下个操作刚好是修改cache line上的下一个字，那么这次传输就过于浪费了。而这恰恰对于显卡来说是比较常见的情形——屏幕上水平邻接的像素往往在内存中也是靠在一起的。顾名思义，写入合并是在写出cache line前，先将多个写入访问合并起来。在理想的情况下，cache line被逐字逐字地修改，只有当写入最后一个字时，才将整条cache line写入内存，从而极大地加速内存的访问。

最后来讲一下不可缓存的内存。一般指的是不被RAM支持的内存位置，它可以是硬编码的特殊地址，承担CPU以外的某些功能。对于商用硬件来说，比较常见的是映射到外部卡或设备的地址。在嵌入式主板上，有时也有类似的地址，用来开关LED。对这些地址进行缓存显然没有什么意义。比如上述的LED，一般是用来调试或报告状态，显然应该尽快点亮或关闭。而对于那些PCI卡上的内存，由于不需要CPU的干涉即可更改，也不该缓存。

### 3.3.4 多处理器支持

在上节中我们已经指出当多处理器开始发挥作用的时候所遇到的问题。甚至对于那些不共享的高速级别的缓存（至少在`L1d`级别）的多核处理器也有问题。

直接提供从一个处理器到另一处理器的高速访问，这是完全不切实际的。从一开始，连接速度根本就不够快。实际的选择是，在其需要的情况下，转移cache中的内容到其他处理器。需要注意的是，这同样应用在相同处理器上无需共享的高速缓存。

现在的问题是，当该cache line转移的时候会发生什么？这个问题回答起来相当容易：当一个处理器需要在另一个处理器的高速缓存中读或者写的脏的cache line的时候。但怎样处理器怎样确定在另一个处理器的缓存中的cache line是脏的？假设它仅仅是因为一个cache line被另一个处理器加载将是次优的（最好的）。通常情况下，大多数的内存访问是只读cache line，从而并不弄脏cache line。在cache line上处理器的操作非常频繁，因此意味着每一次写访问后，要广播关于cache line的改变是不切实际的。

多年来，人们开发除了MESI缓存一致性协议(MESI=Modified, Exclusive, Shared, Invalid，变更的、独占的、共享的、无效的)。协议的名称来自协议中cache line可以进入的四种状态:

- **变更的**: 本地处理器修改了cache line。同时暗示，它是所有缓存中唯一的拷贝。
- **独占的**: cache line没有被修改，而且没有被装入其它处理器的缓存。
- **共享的**: cache line没有被修改，但可能已被装入其它处理器的缓存。
- **无效的**: cache line无效，即，未被使用。

MESI协议开发了很多年，最初的版本比较简单，但是效率也比较差。现在的版本通过以上4个状态可以有效地实现回写式缓存，同时支持不同处理器对只读数据的并发访问。

![img](https://lrita.github.io/images/posts/memory/cpumemory.13.png)

图3.18: MESI协议的状态跃迁图

在协议中，通过处理器监听其它处理器的活动，不需太多努力即可实现状态变更。处理器将操作发布在外部引脚上，使外部可以了解到处理过程。目标的cache line地址则可以在地址总线上看到。在下文讲述状态时，我们将介绍总线参与的时机（图3.18）。

一开始，所有cache line都是空的，缓存为无效(Invalid)状态。当有数据装进缓存供写入时，缓存变为变更(Modified)状态。如果有数据装进缓存供读取，那么新状态取决于其它处理器是否已经加载了同一条cache line。如果是，那么新状态变成共享(Shared)状态，否则变成独占(Exclusive)状态。

如果本地处理器对某条Modified状态的cache line进行读写，那么直接使用缓存内容，状态保持不变。如果第二个处理器希望读它，那么第一个处理器将内容发给第二个处理器，然后可以将缓存状态置为Shared。而发给第二个处理器的数据由内存控制器接收，并回写入内存。如果这一步没有发生，就不能将这条线置为Shared。如果第二个处理器希望的是写，那么第一个处理器将内容发给它后，将缓存置为Invalid。这就是臭名昭著的”请求所有权(Request For Ownership,RFO)”操作。在末级缓存执行RFO操作的代价比较高。如果是写穿透缓存，还要加上将内容写入上一层缓存或主存的时间，进一步提升了代价。

对于Shared状态的cache line，本地处理器的读取操作并不需要修改状态，而且可以直接从缓存满足。而本地处理器的写入操作则需要将状态置为Modified，而且需要将cache line在其它处理器的所有拷贝置为Invalid。因此，这个写入操作需要通过RFO消息发通知其它处理器。如果第二个处理器请求读取，无事发生。因为主存已经包含了当前数据，而且状态已经为Shared。如果第二个处理器需要写入，则将cache line置为Invalid。不需要总线操作。

Exclusive状态与Shared状态很像，只有一个不同之处: 在Exclusive状态时，本地写入操作不需要在总线上通知，因为本地的缓存是系统中唯一的拷贝。这是一个巨大的优势，所以处理器会尽量将cache line保留在Exclusive状态，而不是Shared状态。只有在信息不可用时，才退而求其次选择shared。放弃Exclusive不会引起任何功能缺失，但会导致性能下降，因为E→M要远远快于S→M。

从以上的描述中应该已经可以看出，在多处理器环境下，哪一步的代价比较大了。填充缓存的代价当然还是很高，但我们还需要留意RFO消息。一旦涉及RFO，操作就快不起来了。

RFO消息在两种情况下是必需的:

- 线程从一个处理器迁移到另一个处理器，需要将所有cache line移到新处理器。
- 某条缓存线确实需要被两个处理器使用。{对于同一处理器的两个核心，也有同样的情况，只是代价稍低。RFO消息可能会被发送多次。}

多线程或多进程的程序总是需要同步，而这种同步依赖内存来实现。因此，有些RFO消息是合理的，但仍然需要尽量降低发送频率。除此以外，还有其它来源的RFO。在第6节中，我们将解释这些场景。缓存一致性协议的消息必须发给系统中所有处理器。只有当协议确定已经给过所有处理器响应机会之后，才能进行状态跃迁。也就是说，协议的速度取决于最长响应时间。*这也是现在能看到三插槽AMD Opteron系统的原因。这类系统只有三个超级链路(hyperlink)，其中一个连接南桥，每个处理器之间都只有一跳的距离*。总线上可能会发生冲突，NUMA系统的延时很大，突发的流量会拖慢通信。这些都是让我们避免无谓流量的充足理由。

此外，关于多处理器还有一个问题。虽然它的影响与具体机器密切相关，但根源是唯一的：FSB是共享的。在大多数情况下，所有处理器通过唯一的总线连接到内存控制器(参见图2.1)。如果一个处理器就能占满总线(十分常见)，那么共享总线的两个或四个处理器显然只会得到更有限的带宽。

即使每个处理器有自己连接内存控制器的总线，如图2.2，但还需要通往内存模块的总线。一般情况下，这种总线只有一条。退一步说，即使像图2.2那样不止一条，对同一个内存模块的并发访问也会限制它的带宽。

对于每个处理器拥有本地内存的AMD模型来说，也是同样的问题。的确，所有处理器可以非常快速地同时访问它们自己的内存。但是，多线程呢？多进程呢？它们仍然需要通过访问同一块内存来进行同步。

对同步来说，有限的带宽严重地制约着并发度。程序需要更加谨慎的设计，将不同处理器访问同一块内存的机会降到最低。以下的测试展示了这一点，还展示了与多线程代码相关的其它效果。

#### 多线程测量

为了帮助大家理解问题的严重性，我们来看一些曲线图，主角也是前文的那个程序。只不过这一次，我们运行多个线程，并测量这些线程中最快那个的运行时间。也就是说，等它们全部运行完是需要更长时间的。我们用的机器有4个处理器，而测试是做多跑4个线程。所有处理器共享同一条通往内存控制器的总线，另外，通往内存模块的总线也只有一条。

![img](https://lrita.github.io/images/posts/memory/cpumemory.30.png)

图3.19: 顺序读操作，多线程

图3.19展示了顺序读访问时的性能，元素为128字节长(64位计算机，NPAD=15)。对于单线程的曲线，我们预计是与图3.11相似，只不过是换了一台机器，所以实际的数字会有些小差别。

更重要的部分当然是多线程的环节。由于是只读，不会去修改内存，不会尝试同步。但即使不需要RFO，而且所有缓存线都可共享，性能仍然分别下降了18%(双线程)和34%(四线程)。由于不需要在处理器之间传输缓存，因此这里的性能下降完全由以下两个瓶颈之一或同时引起: 一是从处理器到内存控制器的共享总线，二是从内存控制器到内存模块的共享总线。当工作集超过L3后，三种情况下都要预取新元素，而即使是双线程，可用的带宽也无法满足线性扩展(无惩罚)。

当加入修改之后，场面更加难看了。图3.20展示了顺序递增测试的结果。

![img](https://lrita.github.io/images/posts/memory/cpumemory.29.png)

图3.20: 顺序递增，多线程

图中Y轴采用的是对数刻度，不要被看起来很小的差值欺骗了。现在，双线程的性能惩罚仍然是18%，但四线程的惩罚飙升到了93%！原因在于，采用四线程时，预取的流量与回写的流量加在一起，占满了整个总线。

我们用对数刻度(Y轴)来展示`L1d`范围的结果。可以发现，当超过一个线程后，`L1d`就无力了。单线程时，仅当工作集超过`L1d`时访问时间才会超过20个周期，而多线程时，即使在很小的工作集情况下，访问时间也达到了那个水平。

这里并没有揭示问题的另一方面，主要是用这个程序很难进行测量。问题是这样的，我们的测试程序修改了内存，所以本应看到RFO的影响，但在结果中，我们并没有在`L2`阶段看到更大的开销。原因在于，要看到RFO的影响，程序必须使用大量内存，而且所有线程必须同时访问同一块内存。如果没有大量的同步，这是很难实现的，而如果加入同步，则会占满执行时间。

![img](https://lrita.github.io/images/posts/memory/cpumemory.31.png)

图3.21: 随机的Addnextlast，多线程

最后，在图3.21中，我们展示了随机访问的Addnextlast测试的结果。这里主要是为了让大家感受一下这些巨大到爆的数字。极端情况下，甚至用了1500个周期才处理完一个元素。如果加入更多线程，真是不可想象哪。我们把多线程的效能总结了一下:

![img](https://lrita.github.io/images/posts/memory/cpumem-table-3.3.png)

表3.3: 多线程的效能

这个表展示了图3.21中多线程运行大工作集时的效能。表中的数字表示测试程序在使用多线程处理大工作集时可能达到的最大加速倍数。双线程和四线程的理论最大加速倍数分别是2和4。从表中数据来看，双线程的结果还能接受，但四线程的结果表明，扩展到双线程以上是没有什么意义的，带来的收益可以忽略不计。只要我们把图3.21换个方式呈现，就可以很容易看清这一点。

![img](https://lrita.github.io/images/posts/memory/cpumemory.28.png)

图3.22: 通过并行化实现的加速倍数

图3.22中的曲线展示了加速倍数，即多线程相对于单线程所能获取的性能加成值。测量值的精确度有限，因此我们需要忽略比较小的那些数字。可以看到，在`L2`与`L3`范围内，多线程基本可以做到线性加速，双线程和四线程分别达到了2和4的加速倍数。但是，一旦工作集的大小超出`L3`，曲线就崩塌了，双线程和四线程降到了基本相同的数值(参见表3.3中第4列)。也是部分由于这个原因，我们很少看到4CPU以上的主板共享同一个内存控制器。如果需要配置更多处理器，我们只能选择其它的实现方式(参见第5节)。

可惜，上图中的数据并不是普遍情况。在某些情况下，即使工作集能够放入末级缓存，也无法实现线性加速。实际上，这反而是正常的，因为普通的线程都有一定的耦合关系，不会像我们的测试程序这样完全独立。而反过来说，即使是很大的工作集，即使是两个以上的线程，也是可以通过并行化受益的，但是需要程序员的聪明才智。我们会在第6节进行一些介绍。

#### 特例: 超线程

由CPU实现的超线程(有时又叫对称多线程，SMT)是一种比较特殊的情况，每个线程并不能真正并发地运行。它们共享着除寄存器外的绝大多数处理资源。每个核心和CPU仍然是并行工作的，但核心上的线程则受到这个限制。理论上，每个核心可以有大量线程，不过到目前为止，Intel的CPU最多只有两个线程。CPU负责对各线程进行时分复用，但这种复用本身并没有多少厉害。它真正的优势在于，CPU可以在当前运行的超线程发生延迟时，调度另一个线程。这种延迟一般由内存访问引起。

如果两个线程运行在一个超线程核心上，那么只有当两个线程合起来的运行时间少于单线程运行时间时，效率才会比较高。我们可以将通常先后发生的内存访问叠合在一起，以实现这个目标。有一个简单的计算公式，可以帮助我们计算如果需要某个加速倍数，最少需要多少的缓存命中率。

程序的执行时间可以通过一个只有一级缓存的简单模型来进行估算(参见[htimpact]):

Texe=N[(1−Fmem)Tproc+Fmem(GhitTcache+(1−Ghit)Tmiss)]Texe=N[(1−Fmem)Tproc+Fmem(GhitTcache+(1−Ghit)Tmiss)]

各变量的含义如下:

N=指令数Fmem=N中访问内存的比例Ghit=命中缓存的比例Tproc=每条指令所用的周期数Tcache=缓存命中所用的周期数Tmiss=缓冲未命中所用的周期数Texe=程序的执行时间N=指令数Fmem=N中访问内存的比例Ghit=命中缓存的比例Tproc=每条指令所用的周期数Tcache=缓存命中所用的周期数Tmiss=缓冲未命中所用的周期数Texe=程序的执行时间

对于使用两个线程的任何意义来说，两个线程之中任一线程的执行时间最多为单线程指令的一半。两者都有一个唯一的变量缓存命中数。如果我们要解决最小缓存命中率相等的问题，需要使我们获得的线程的执行率不少于50%或更多，如图 3.23.

![img](https://lrita.github.io/images/posts/memory/cpumemory.14.png)

图 3.23: 最小缓存命中率-加速

X轴表示单线程指令的缓存命中率GhitGhit，Y轴表示多线程指令所需的缓存命中率。这个值永远不能高于单线程命中率，否则，单线程指令也会使用改良的指令。为了使单线程的命中率在低于55%的所有情况下优于使用多线程，cpu要或多或少的足够空闲(延迟)，因为缓存丢失会运行另外一个超线程。

绿色区域是我们的目标。如果线程的速度没有慢过50%，而每个线程的工作量只有原来的一半，那么它们合起来的耗时应该会少于单线程的耗时。对我们用的示例系统来说(使用超线程的P4机器)，如果单线程代码的命中率为60%，那么多线程代码至少要达到10%才能获得收益。这个要求一般来说还是可以做到的。但是，如果单线程代码的命中率达到了95%，那么多线程代码要做到80%才行。这就很难了。而且，这里还涉及到超线程，在两个超线程的情况下，每个超线程只能分到一半的有效缓存。因为所有超线程是使用同一个缓存来装载数据的，如果两个超线程的工作集没有重叠，那么原始的95%也会被打对折——47%，远低于80%。

因此，超线程只在某些情况下才比较有用：单线程代码的缓存命中率必须低到一定程度，从而使缓存容量变小时新的命中率仍能满足要求。只有在这种情况下，超线程才是有意义的。在实践中，采用超线程能否获得更快的结果，取决于处理器能否有效地将每个进程的等待时间与其它进程的执行时间重叠在一起。并行化也需要一定的开销，需要加到总的运行时间里，这个开销往往是不能忽略的。

在6.3.4节中，我们会介绍一种技术，它将多个线程通过公用缓存紧密地耦合起来。这种技术适用于许多场合，前提是程序员们乐意花费时间和精力扩展自己的代码。

应该明确一点，如果两个超线程执行完全不同的代码(两个线程就像被当成两个处理器，分别执行不同进程)，那么缓存容量就真的会降为一半，导致缓冲未命中率大为攀升。这样的调度机制是很有问题的，除非你的缓存足够大。所以，除非程序的工作集设计得比较合理，能够确实从超线程获益，否则还是建议在BIOS中把超线程功能关掉。{我们可能会因为另一个原因开启超线程，那就是调试，因为SMT在查找并行代码的问题方面真的非常好用。}

### 3.3.5 其它细节

我们已经介绍了地址的组成，即标签、集合索引和偏移三个部分。那么，实际会用到什么样的地址呢？目前，处理器一般都向进程提供虚拟地址空间，意味着我们有两种不同的地址: 虚拟地址和物理地址。

虚拟地址有个问题：并不唯一。随着时间的变化，虚拟地址可以变化，指向不同的物理地址。同一个地址在不同的进程里也可以表示不同的物理地址。那么，是不是用物理地址会比较好呢？

问题是，处理器指令用的虚拟地址，而且需要在内存管理单元(MMU)的协助下将它们翻译成物理地址。这并不是一个很小的操作。在执行指令的管线(pipeline)中，物理地址只能在很后面的阶段才能得到。这意味着，缓存逻辑需要在很短的时间里判断地址是否已被缓存过。而如果可以使用虚拟地址，缓存查找操作就可以更早地发生，一旦命中，就可以马上使用内存的内容。结果就是，使用虚拟内存后，可以让管线把更多内存访问的开销隐藏起来。

处理器的设计人员们现在使用虚拟地址来标记第一级缓存。这些缓存很小，很容易被清空。在进程页表树发生变更的情况下，至少是需要清空部分缓存的。如果处理器拥有指定变更地址范围的指令，那么可以避免缓存的完全刷新。由于一级缓存`L1i`及`L1d`的时延都很小(~3周期)，基本上必须使用虚拟地址。

对于更大的缓存，包括`L2`和`L3`等，则需要以物理地址作为标签。因为这些缓存的时延比较大，虚拟到物理地址的映射可以在允许的时间里完成，而且由于主存时延的存在，重新填充这些缓存会消耗比较长的时间，刷新的代价比较昂贵。

一般来说，我们并不需要了解这些缓存处理地址的细节。我们不能更改它们，而且那些可能影响性能的因素，要么是应该避免的，要么是有很高代价的。填满缓存是不好的行为，cache line都落入同一个集合，也会让缓存早早地出问题。对于后一个问题，可以通过缓存虚拟地址来避免，但作为一个用户级程序，是不可能避免缓存物理地址的。我们唯一可以做的，是尽最大努力不要在同一个进程里用多个虚拟地址映射同一个物理地址。

另一个细节对程序员们来说比较乏味，那就是缓存的替换策略。大多数缓存会优先逐出最近最少使用(Least Recently Used,LRU)的元素。这往往是一个效果比较好的策略。在关联性很大的情况下(随着以后核心数的增加，关联性势必会变得越来越大)，维护LRU列表变得越来越昂贵，于是我们开始看到其它的一些策略。

在缓存的替换策略方面，程序员可以做的事情不多。如果缓存使用物理地址作为标签，我们是无法找出虚拟地址与缓存集之间关联的。有可能会出现这样的情形: 所有逻辑页中的缓存线都映射到同一个缓存集，而其它大部分缓存却空闲着。即使有这种情况，也只能依靠OS进行合理安排，避免频繁出现。

虚拟化的出现使得这一切变得更加复杂。现在不仅操作系统可以控制物理内存的分配。虚拟机监视器（VMM，也称为 hypervisor）也负责分配内存。

对程序员来说，最好 a) 完全使用逻辑内存页面 b) 在有意义的情况下，使用尽可能大的页面大小来分散物理地址。更大的页面大小也有其他好处，不过这是另一个话题（见第4节）。

## 3.4 指令缓存

其实，不光处理器使用的数据被缓存，它们执行的指令也是被缓存的。只不过，指令缓存的问题相对来说要少得多，因为:

- 执行的代码量取决于代码大小。而代码大小通常取决于问题复杂度。问题复杂度则是固定的。
- 程序的数据处理逻辑是程序员设计的，而程序的指令却是编译器生成的。编译器的作者知道如何生成优良的代码。
- 程序的流向比数据访问模式更容易预测。现如今的CPU很擅长模式检测，对预取很有利。
- 代码永远都有良好的时间局部性和空间局部性。

有一些准则是需要程序员们遵守的，但大都是关于如何使用工具的，我们会在第6节介绍它们。而在这里我们只介绍一下指令缓存的技术细节。

随着CPU的核心频率大幅上升，缓存与核心的速度差越拉越大，CPU的处理开始管线化。也就是说，指令的执行分成若干阶段。首先，对指令进行解码，随后，准备参数，最后，执行它。这样的管线可以很长(例如，Intel的Netburst架构超过了20个阶段)。在管线很长的情况下，一旦发生延误(即指令流中断)，需要很长时间才能恢复速度。管线延误发生在这样的情况下: 下一条指令未能正确预测，或者装载下一条指令耗时过长(例如，需要从内存读取时)。

为了解决这个问题，CPU的设计人员们在分支预测上投入大量时间和芯片资产(chip real estate)，以降低管线延误的出现频率。

在CISC处理器上，指令的解码阶段也需要一些时间。x86及x86-64处理器尤为严重。近年来，这些处理器不再将指令的原始字节序列存入`L1i`，而是缓存解码后的版本。这样的`L1i`被叫做“追踪缓存(trace cache)”。追踪缓存可以在命中的情况下让处理器跳过管线最初的几个阶段，在管线发生延时时尤其有用。

前面说过，`L2`以上的缓存是统一缓存，既保存代码，也保存数据。显然，这里保存的代码是原始字节序列，而不是解码后的形式。

在提高性能方面，与指令缓存相关的只有很少的几条准则:

- 生成尽量少的代码。也有一些例外，如出于管线化的目的需要更多的代码，或使用小代码会带来过高的额外开销。
- 尽量帮助处理器作出良好的预取决策，可以通过代码布局或显式预取来实现。

这些准则一般会由编译器的代码生成阶段强制执行。至于程序员可以参与的部分，我们会在第6节介绍。

### 3.4.1 自修改的代码

在计算机的早期岁月里，内存十分昂贵。人们想尽千方百计，只为了尽量压缩程序容量，给数据多留一些空间。其中，有一种方法是修改程序自身，称为自修改代码(SMC)。现在，有时候我们还能看到它，一般是出于提高性能的目的，也有的是为了攻击安全漏洞。

一般情况下，应该避免SMC。虽然一般情况下没有问题，但有时会由于执行错误而出现性能问题。显然，发生改变的代码是无法放入追踪缓存(追踪缓存放的是解码后的指令)的。即使没有使用追踪缓存(代码还没被执行或有段时间没执行)，处理器也可能会遇到问题。如果某个进入管线的指令发生了变化，处理器只能扔掉目前的结果，重新开始。在某些情况下，甚至需要丢弃处理器的大部分状态。

最后，由于处理器认为代码页是不可修改的(这是出于简单化的考虑，而且在99.9999999%情况下确实是正确的)，`L1i`用到并不是MESI协议，而是一种简化后的SI协议。这样一来，如果万一检测到修改的情况，就需要作出大量悲观的假设。

因此，对于SMC，强烈建议能不用就不用。现在内存已经不再是一种那么稀缺的资源了。最好是写多个函数，而不要根据需要把一个函数改来改去。也许有一天可以把SMC变成可选项，我们就能通过这种方式检测入侵代码。如果一定要用SMC，应该让写操作越过缓存，以免由于`L1i`需要`L1d`里的数据而产生问题。更多细节，请参见6.1节。

在Linux上，判断程序是否包含SMC是很容易的。利用正常工具链(toolchain)构建的程序代码都是写保护(write-protected)的。程序员需要在链接时施展某些关键的魔术才能生成可写的代码页。现代的Intel x86和x86-64处理器都有统计SMC使用情况的专用计数器。通过这些计数器，我们可以很容易判断程序是否包含SMC，即使它被准许运行。

## 3.5 缓存未命中的因素

我们已经看过内存访问没有命中缓存时，那陡然猛涨的高昂代价。但是有时候，这种情况又是无法避免的，因此我们需要对真正的代价有所认识，并学习如何缓解这种局面。

### 3.5.1 缓存与内存带宽

为了更好地理解处理器的能力，我们测量了各种理想环境下能够达到的带宽值。由于不同处理器的版本差别很大，所以这个测试比较有趣，也因为如此，这一节都快被测试数据灌满了。我们使用了x86和x86-64处理器的SSE指令来装载和存储数据，每次16字节。工作集则与其它测试一样，从1kB增加到512MB，测量的具体对象是每个周期所处理的字节数。

![img](https://lrita.github.io/images/posts/memory/cpumemory.60.png)

图3.24: P4的带宽

图3.24展示了一颗64位Intel Netburst处理器的性能图表。当工作集能够完全放入`L1d`时，处理器的每个周期可以读取完整的16字节数据，即每个周期执行一条装载指令(moveaps指令，每次移动16字节的数据)。测试程序并不对数据进行任何处理，只是测试读取指令本身。当工作集增大，无法再完全放入`L1d`时，性能开始急剧下降，跌至每周期6字节。在218218工作集处出现的台阶是由于DTLB缓存耗尽，因此需要对每个新页施加额外处理。由于这里的读取是按顺序的，预取机制可以完美地工作，而FSB能以5.3字节/周期的速度传输内容。但预取的数据并不进入`L1d`。当然，真实世界的程序永远无法达到以上的数字，但我们可以将它们看作一系列实际上的极限值。

更令人惊讶的是写操作和复制操作的性能。即使是在很小的工作集下，写操作也始终无法达到4字节/周期的速度。这意味着，Intel为Netburst处理器的`L1d`选择了写穿透(write-through)模式，所以写入性能受到`L2`速度的限制。同时，这也意味着，复制测试的性能不会比写入测试差太多(复制测试是将某块内存的数据拷贝到另一块不重叠的内存区)，因为读操作很快，可以与写操作实现部分重叠。最值得关注的地方是，两个操作在工作集无法完全放入`L2`后出现了严重的性能滑坡，降到了0.5字节/周期！比读操作慢了10倍！显然，如果要提高程序性能，优化这两个操作更为重要。

再来看图3.25，它来自同一颗处理器，只是运行双线程，每个线程分别运行在处理器的一个超线程上。

![img](https://lrita.github.io/images/posts/memory/cpumemory.61.png)

图3.25: P4开启两个超线程时的带宽表现

图3.25采用了与图3.24相同的刻度，以方便比较两者的差异。图3.25中的曲线抖动更多，是由于采用双线程的缘故。结果正如我们预期，由于超线程共享着几乎所有资源(仅除寄存器外)，所以每个超线程只能得到一半的缓存和带宽。所以，即使每个线程都要花上许多时间等待内存，从而把执行时间让给另一个线程，也是无济于事——因为另一个线程也同样需要等待。这里恰恰展示了使用超线程时可能出现的最坏情况。

![img](https://lrita.github.io/images/posts/memory/cpumemory.62.png)

图3.26: Core 2的带宽表现

再来看Core 2处理器的情况。看看图3.26和图3.27，再对比下P4的图3.24和3.25，可以看出不小的差异。Core 2是一颗双核处理器，有着共享的`L2`，容量是P4`L2`的4倍。但更大的`L2`只能解释写操作的性能下降出现较晚的现象。

当然还有更大的不同。可以看到，读操作的性能在整个工作集范围内一直稳定在16字节/周期左右，在220220处的下降同样是由于DTLB的耗尽引起。能够达到这么高的数字，不但表明处理器能够预取数据，并且按时完成传输，而且还意味着，预取的数据是被装入`L1d`的。

写/复制操作的性能与P4相比，也有很大差异。处理器没有采用写穿透策略，写入的数据留在`L1d`中，只在必要时才剔除。这使得写操作的速度可以逼近16字节/周期。一旦工作集超过`L1d`，性能即飞速下降。由于Core 2读操作的性能非常好，所以两者的差值显得特别大。当工作集超过`L2`时，两者的差值甚至超过20倍！但这并不表示Core 2的性能不好，相反，Core 2永远都比Netburst强。

![img](https://lrita.github.io/images/posts/memory/cpumemory.63.png)

图3.27: Core 2运行双线程时的带宽表现

在图3.27中，启动双线程，各自运行在Core 2的一个核心上。它们访问相同的内存，但不需要完美同步。从结果上看，读操作的性能与单线程并无区别，只是多了一些多线程情况下常见的抖动。

有趣的地方来了——当工作集小于`L1d`时，写操作与复制操作的性能很差，就好像数据需要从内存读取一样。两个线程彼此竞争着同一个内存位置，于是不得不频频发送RFO消息。问题的根源在于，虽然两个核心共享着`L2`，但无法以`L2`的速度处理RFO请求。而当工作集超过`L1d`后，性能出现了迅猛提升。这是因为，由于`L1d`容量不足，于是将被修改的条目刷新到共享的`L2`。由于`L1d`的未命中可以由`L2`满足，只有那些尚未刷新的数据才需要RFO，所以出现了这样的现象。这也是这些工作集情况下速度下降一半的原因。这种渐进式的行为也与我们期待的一致: 由于每个核心共享着同一条FSB，每个核心只能得到一半的FSB带宽，因此对于较大的工作集来说，每个线程的性能大致相当于单线程时的一半。

由于同一个厂商的不同处理器之间都存在着巨大差异，我们没有理由不去研究一下其它厂商处理器的性能。图3.28展示了AMD家族10h Opteron处理器的性能。这颗处理器有64kB的`L1d`、512kB的`L2`和2MB的`L3`，其中`L3`缓存由所有核心所共享。

![img](https://lrita.github.io/images/posts/memory/cpumemory.64.png)

图3.28: AMD家族10h Opteron的带宽表现

大家首先应该会注意到，在`L1d`缓存足够的情况下，这个处理器每个周期能处理两条指令。读操作的性能超过了32字节/周期，写操作也达到了18.7字节/周期。但是，不久，读操作的曲线就急速下降，跌到2.3字节/周期，非常差。处理器在这个测试中并没有预取数据，或者说，没有有效地预取数据。

另一方面，写操作的曲线随几级缓存的容量而变化。在`L1d`阶段达到最高性能，随后在`L2`阶段下降到6字节/周期，在L3阶段进一步下降到2.8字节/周期，最后，在工作集超过`L3`后，降到0.5字节/周期。它在`L1d`阶段超过了Core 2，在`L2`阶段基本相当(Core 2的`L2`更大一些)，在`L3`及主存阶段比Core 2慢。

复制的性能既无法超越读操作的性能，也无法超越写操作的性能。因此，它的曲线先是被读性能压制，随后又被写性能压制。

图3.29显示的是Opteron处理器在多线程时的性能表现。

![img](https://lrita.github.io/images/posts/memory/cpumemory.65.png)

图3.29: AMD Fam 10h在双线程时的带宽表现

读操作的性能没有受到很大的影响。每个线程的`L1d`和`L2`表现与单线程下相仿，`L3`的预取也依然表现不佳。两个线程并没有过渡争抢`L3`。问题比较大的是写操作的性能。两个线程共享的所有数据都需要经过`L3`，而这种共享看起来却效率很差。即使是在`L3`足够容纳整个工作集的情况下，所需要的开销仍然远高于`L3`的访问时间。再来看图3.27，可以发现，在一定的工作集范围内，Core 2处理器能以共享的`L2`缓存的速度进行处理。而Opteron处理器只能在很小的一个范围内实现相似的性能，而且，它仅仅只能达到`L3`的速度，无法与Core 2的`L2`相比。

### 3.5.2 关键字加载

内存以比cache line还小的块从主存储器向缓存传送。如今64位可一次性传送，cache line的大小为64或128比特。这意味着每个cache line需要8或16次传送。

DRAM芯片可以以触发模式传送这些64位的块。这使得不需要内存控制器的进一步指令和可能伴随的延迟，就可以将cache line充满。如果处理器预取了缓存，这有可能是最好的操作方式。

如果程序在访问数据或指令缓存时没有命中(这可能是强制性未命中或容量性未命中，前者是由于数据第一次被使用，后者是由于容量限制而将cache line逐出)，情况就不一样了。程序需要的并不总是cache line中的第一个字，而数据块的到达是有先后顺序的，即使是在突发模式和双倍传输率下，也会有明显的时间差，一半在4个CPU周期以上。举例来说，如果程序需要cache line中的第8个字，那么在首字抵达后它还需要额外等待30个周期以上。

当然，这样的等待并不是必需的。事实上，内存控制器可以按不同顺序去请求cache line中的字。当处理器告诉它，程序需要缓存中具体某个字，即「关键字(critical word)」时，内存控制器就会先请求这个字。一旦请求的字抵达，虽然cache line的剩余部分还在传输中，缓存的状态还没有达成一致，但程序已经可以继续运行。这种技术叫做关键字优先及较早重启(Critical Word First & Early Restart)。

现在的处理器都已经实现了这一技术，但有时无法运用。比如，预取操作的时候，并不知道哪个是关键字。如果在预取的中途请求某条cache line，处理器只能等待，并不能更改请求的顺序。

![img](https://lrita.github.io/images/posts/memory/cpumemory.72.png)

图3.30: 关键字位于cache line尾时的表现

在关键字优先技术生效的情况下，关键字的位置也会影响结果。图3.30展示了下一个测试的结果，图中表示的是关键字分别在cache line首和cache line尾时的性能对比情况。元素大小为64字节，等于cache line的长度。图中的噪声比较多，但仍然可以看出，当工作集超过`L2`后，关键字处于cache line尾情况下的性能要比cache line首情况下低0.7%左右。而顺序访问时受到的影响更大一些。这与我们前面提到的预取下条cache line时可能遇到的问题是相符的。

### 3.5.3 缓存设定

缓存放置的位置与超线程，内核和处理器之间的关系，不在程序员的控制范围之内。但是程序员可以决定线程执行的位置，接着高速缓存与使用的CPU的关系将变得非常重要。

这里我们将不会深入（探讨）什么时候选择什么样的内核以运行线程的细节。我们仅仅描述了在设置关联线程的时候，程序员需要考虑的系统结构的细节。

超线程，通过定义，共享除去寄存器集以外的所有数据。包括`L1`缓存。这里没有什么可以多说的。多核处理器的独立核心带来了一些乐趣。每个核心都至少拥有自己的`L1`缓存。除此之外，下面列出了一些不同的特性：

- 早期多核心处理器有独立的`L2`缓存且没有更高层级的缓存。
- 之后英特尔的双核心处理器模型拥有共享的`L2`缓存。对四核处理器，则分对拥有独立的`L2`缓存，且没有更高层级的缓存。
- AMD 家族的 10h 处理器有独立的`L2`缓存以及一个统一的`L3`缓存。

关于各种处理器模型的优点，已经在它们各自的宣传手册里写得够多了。在每个核心的工作集互不重叠的情况下，独立的`L2`拥有一定的优势，单线程的程序可以表现优良。考虑到目前实际环境中仍然存在大量类似的情况，这种方法的表现并不会太差。不过，不管怎样，我们总会遇到工作集重叠的情况。如果每个缓存都保存着某些通用运行库的常用部分，那么很显然是一种浪费。

如果像Intel的双核处理器那样，共享除`L1`外的所有缓存，则会有一个很大的优点。如果两个核心的工作集重叠的部分较多，那么综合起来的可用缓存容量会变大，从而允许容纳更大的工作集而不导致性能的下降。如果两者的工作集并不重叠，那么则是由Intel的高级智能缓存管理(Advanced Smart Cache management)发挥功用，防止其中一个核心垄断整个缓存。

即使每个核心只使用一半的缓存，也会有一些摩擦。缓存需要不断衡量每个核心的用量，在进行逐出操作时可能会作出一些比较差的决定。我们来看另一个测试程序的结果。

![img](https://lrita.github.io/images/posts/memory/cpumemory.74.png)

这次，测试程序两个进程，第一个进程不断用SSE指令读/写2MB的内存数据块。选择2MB，是因为它正好是Core 2处理器`L2`缓存的一半，第二个进程则是读/写大小变化的内存区域，我们把这两个进程分别固定在处理器的两个核心上。图中显示的是每个周期读/写的字节数，共有4条曲线，分别表示不同的读写搭配情况。例如，标记为读/写(read/write)的曲线代表的是后台进程进行写操作(固定2MB工作集)，而被测量进程进行读操作(工作集从小到大)。

图中最有趣的是220220到223223之间的部分。如果两个核心的`L2`是完全独立的，那么所有4种情况下的性能下降均应发生在221221到222222之间，也就是`L2`缓存耗尽的时候。但从图上来看，实际情况并不是这样，特别是背景进程进行写操作时尤为明显。当工作集达到1MB(220220)时，性能即出现恶化，两个进程并没有共享内存，因此并不会产生RFO消息。所以，完全是缓存逐出操作引起的问题。目前这种智能的缓存处理机制有一个问题，每个核心能实际用到的缓存更接近1MB，而不是理论上的2MB。如果未来的处理器仍然保留这种多核共享缓存模式的话，我们唯有希望厂商会把这个问题解决掉。

推出拥有双`L2`缓存的4核处理器仅仅只是一种临时措施，是开发更高级缓存之前的替代方案。与独立插槽及双核处理器相比，这种设计并没有带来多少性能提升。两个核心是通过同一条总线(被外界看作FSB)进行通信，并没有什么特别快的数据交换通道。

未来，针对多核处理器的缓存将会包含更多层次。AMD的10h家族是一个开始，至于会不会有更低级共享缓存的出现，还需要我们拭目以待。我们有必要引入更多级别的缓存，因为频繁使用的高速缓存不可能被许多核心共用，否则会对性能造成很大的影响。我们也需要更大的高关联性缓存，它们的数量、容量和关联性都应该随着共享核心数的增长而增长。巨大的`L3`和适度的`L2`应该是一种比较合理的选择。`L3`虽然速度较慢，但也较少使用。

对于程序员来说，不同的缓存设计就意味着调度决策时的复杂性。为了达到最高的性能，我们必须掌握工作负载的情况，必须了解机器架构的细节。好在我们在判断机器架构时还是有一些支援力量的，我们会在后面的章节介绍这些接口。

### 3.5.4 FSB的影响

FSB在性能中扮演了核心角色。缓存数据的存取速度受制于内存通道的速度。我们做一个测试，在两台机器上分别跑同一个程序，这两台机器除了内存模块的速度有所差异，其它完全相同。图3.32展示了Addnext0测试(将下一个元素的pad[0]加到当前元素的pad[0]上)在这两台机器上的结果(`NPAD=7`，64位机器)。两台机器都采用Core 2处理器，一台使用667MHz的DDR2内存，另一台使用800MHz的DDR2内存(比前一台增长20%)。

![img](https://lrita.github.io/images/posts/memory/cpumemory.27.png)

图3.32: FSB速度的影响

图上的数字表明，当工作集大到对FSB造成压力的程度时，高速FSB确实会带来巨大的优势。在我们的测试中，性能的提升达到了18.5%，接近理论上的极限。而当工作集比较小，可以完全纳入缓存时，FSB的作用并不大。当然，这里我们只测试了一个程序的情况，在实际环境中，系统往往运行多个进程，工作集是很容易超过缓存容量的。

如今，一些英特尔的处理器，支持前端总线(FSB)的速度高达1333MHz，这意味着速度有另外60％的提升。将来还会出现更高的速度。速度是很重要的，工作集会更大，快速的RAM和高FSB速度的内存肯定是值得投资的。我们必须小心使用它，因为即使处理器可以支持更高的前端总线速度，但是主板的北桥芯片可能不会。使用时，检查它的规范是至关重要的。

## 参考

1. [What-Every-Programmer-Should-Know-About-Memory](https://lrita.github.io/images/posts/memory/What-Every-Programmer-Should-Know-About-Memory.pdf) [↩](https://lrita.github.io/2018/06/30/programmer-should-know-about-memory-1/#fnref:1)
2. [每个程序员都应该了解的 CPU 高速缓存](https://www.oschina.net/translate/what-every-programmer-should-know-about-cpu-cache-part2) [↩](https://lrita.github.io/2018/06/30/programmer-should-know-about-memory-1/#fnref:2)
3. [Memory part 2: CPU caches](https://lwn.net/Articles/252125/) [↩](https://lrita.github.io/2018/06/30/programmer-should-know-about-memory-1/#fnref:3)

# 4 虚拟内存(Virtual Memory)[1](https://lrita.github.io/2022/04/01/programmer-should-know-about-memory-2/#fn:1)[2](https://lrita.github.io/2022/04/01/programmer-should-know-about-memory-2/#fn:2)

> 注3
> 看这篇文章之前，可以先简单阅读一下[《KVM 虚拟化详解》](https://zhuanlan.zhihu.com/p/105499858)，可以帮助理解以下的内容。

处理器的虚拟内存子系统为每个进程提供了虚拟地址空间的实现。这使每个进程都认为它在系统中是独立的。虚拟内存的优势在其他地方有详细描述，因此这里不再复述。相反，本节集中讨论虚拟内存子系统的实现细节和相关代价。

虚拟地址空间由CPU的内存管理单元（MMU）实现。操作系统必须填充页表数据结构（page table data structures），但大多数CPU自己完成其余工作。这实际上是一个相当复杂的机制；理解它的最佳方式是介绍用于描述虚拟地址空间的数据结构。

通常（程序）输入一个虚拟地址让MMU进行翻译（获得对应的物理地址）。通常对其价值几乎没有限制。虚拟地址在32位系统上是32-bit，在64位系统上是64-bit。在某些系统上，例如x86和x86-64，使用的地址实际上包含了另一个层次的间接寻址（内存分段）：这些体系结构使用的分段内存寻址，其会向每个逻辑地址添加一个偏移量。我们可以忽略这一部分，它是微不足道的，程序员不必关心内存处理的性能。{x86上的段限制与性能有关，但这是另一回事。}

## 4.1 最简单的地址转换

有趣的部分是虚拟地址到物理地址的转换。MMU可以逐页重新映射地址。就像地址缓存排列的时候，虚拟地址被分割为不同的部分。这些部分被用来做多个表的索引，而这些表是被用来创建最终物理地址用的。对于最简单的模型，我们只有一级表。

![img](https://lrita.github.io/images/posts/memory/cpumemory.18.png)

```
图 4.1: 一层表结构的地址转换
```

图4.1显示了如何使用虚拟地址的不同部分。顶部用于选择页面目录中的条目；该目录中的每个条目都可以由操作系统单独设置。页面目录条目确定物理内存页面的地址；页面目录中的多个不同条目可以指向同一块物理地址。存储单元的完整物理地址是通过将页面目录中的页面地址与虚拟地址中的低地址相结合来确定的。页面目录条目还包含一些关于页面的附加信息，例如访问权限。

页面目录的数据结构存储在内存中。操作系统必须分配连续的物理内存，并将该内存区域的基址存储在一个特殊的寄存器中。然后，虚拟地址的适当位被用作页面目录的索引，页面目录实际上是一个目录项数组。

举个具体的例子，这是x86机器上4MB页面的布局。虚拟地址的偏移部分大小为22位，足以寻址4MB页面中的每个字节。虚拟地址的剩余10位选择页面目录中的1024个条目之一。每个条目都包含一个4MB页面的10位基址，该基址与偏移量结合形成一个完整的32位地址。

## 4.2 多级页表

4MB页大小不是标准，它们会浪费大量内存，因为操作系统必须执行的许多操作都需要与内存页面对齐。对于4kB页面（32位机器上的标准，而且通常也是64位机器的标准），虚拟地址的偏移部分大小只有12bit，这将留下20bit作为页表目录的选择器。有2^20个条目的页表是不实用的。即使每个条目只有4个字节，表的大小也将是4MB。由于每个进程都可能有自己独立的页表目录，(那么)系统的大部分物理内存都会被这些页表目录占用。

解决方案是使用多级页表。然后它可以表示一个稀疏的巨大页表目录，在这个目录中，没有实际使用的区域不需要分配内存。因此，这种表示方式更加紧凑，使得在内存中有许多进程的页表而不会对性能造成太大影响。

如今，最复杂的页表结构包括四个层。图4.2显示了这种实现的示意图。

![img](https://lrita.github.io/images/posts/memory/cpumemory.19-sm.png) `Figure 4.2: 4-层地址转换`

在本例中，虚拟地址至少分为五个部分。其中四个部分是各种目录的索引。CPU使用专用寄存器指向第4级目录。4级到2级目录的内容是对下一级目录的引用。如果一个目录条目被标记为空，它显然不需要指向任何较低的目录。这样，页表树就可以稀疏而紧凑。与图4.1一样，1级目录的条目包括部分物理地址，以及访问权限等辅助数据。

为了确定与虚拟地址对应的物理地址，处理器首先确定最高级别目录的地址。该地址通常存储在寄存器中。然后，CPU获取与该目录对应的虚拟地址的索引部分，并使用该索引选择适当的条目。此条目是下一个目录的地址，该目录使用虚拟地址的下一部分编制索引。这个过程一直持续到到达1级目录，此时目录项的值是物理地址的较高部分。物理地址通过从虚拟地址添加页偏移位来完成。这个过程称为页表树遍历。一些处理器（如x86和x86-64）在硬件上执行此操作，其他处理器则需要操作系统的帮助。

系统上运行的每个进程可能都需要自己的页表树。可以部分共享树，但这是个例外。因此，如果页表树所需的内存尽可能小，就有利于性能和可伸缩性。理想的情况是将使用过的内存紧密地放在虚拟地址空间中；实际使用的物理地址(是否紧凑/连续)并不重要。一个小程序可能只需要在2级、3级和4级各使用一个目录，以及几个1级目录。在具有4kB页面和每个目录512个条目的x86-64上，这允许寻址2MB，总共有4个目录（每个级别一个目录）。1GB的连续内存可以通过一个目录寻址，用于级别2到4，512个目录用于级别1。

不过，假设所有内存都可以连续分配，那就太简单了。大多数情况下，出于灵活性的原因，进程的堆栈和堆区域被分配在地址空间几乎相反的一端。这使得任何一个区域都可以在需要时尽可能地增长。这意味着很可能需要两个级别2的目录，相应地，需要更多级别较低的目录。

但即便如此，这也并不总是符合当前的做法。出于安全原因，可执行文件的各个部分（代码、数据、堆、堆栈、DSO（又名共享库））映射到随机地址。随机化延伸到各个部分的相对位置；这意味着进程中使用的各种内存区域在整个虚拟地址空间中都很分布的很广(不会集中到某些地址范围内)。通过对随机地址的位数施加一些限制，可以限制范围，但在大多数情况下，它肯定不允许进程在2级和3级仅使用一个或两个目录运行。

如果性能真的比安全性重要得多，那么可以关闭随机化。操作系统通常至少会在虚拟内存中连续加载所有DSO。

## 4.3 优化页表访问

页表的所有数据结构都保存在内存中。创建进程或更改页表时，会通知CPU。页表用于使用将每个虚拟地址解析为物理地址。更重要的是：在解析虚拟地址的过程中，每个级别至少使用一个目录。这需要最多四次内存访问（对于正在运行的进程的一次访问），这将导致速度很慢。可以将这些目录表条目视为普通数据，并将它们缓存在L1d、L2等中，但这仍然太慢。

从虚拟内存诞生之初，CPU设计师就采用了特殊的优化方法。一个简单的计算可以表明，只有将目录表条目保留在L1d和更高的缓存中，才会导致糟糕的性能。每个绝对地址计算都需要与页表深度对应的多个L1d访问。这些访问无法并行化，因为它们依赖于前一次查找的结果。在具有四级页表的机器上，仅此一项就至少需要12个周期。再加上L1d未命中的概率，结果是指令管道无法隐藏任何信息。额外的L1d访问还窃取了缓存的宝贵带宽。

因此，不只是缓存页表条目，而是缓存物理页地址的完整计算。出于代码和数据缓存工作的相同原因，这种缓存地址计算是有效的。由于虚拟地址的页偏移部分（页内偏移量）在物理页地址的计算中不起任何作用，因此只有虚拟地址的其余部分用作缓存的标记。根据页面大小，这意味着数百或数千条指令或数据对象共享同一标记，因此物理地址前缀相同。

存储计算值的高速缓存称为`Translation Look-Aside Buffer`（TLB）。它通常是一个小缓存，但是它必须非常快。与其他缓存一样，现代CPU提供多级TLB缓存；更高级别的缓存更大，速度也更慢。L1TLB的小尺寸通常通过使缓存与LRU逐出策略完全关联来弥补。

如上所述，用于访问TLB的标签(tag，即查找缓存的key)是虚拟地址的一部分。如果标记在缓存中有匹配项，则通过将虚拟地址的页偏移量添加到(读取到的)缓存值来(加合得到)最终的物理地址。这是一个非常快速的过程；必须这样做，因为CPU每条指令需要依赖于内存的物理地址，在某些情况下，还必须使用物理地址作为key进行缓存L2的查询。如果TLB查找未命中，处理器必须执行页表遍历；这可能相当昂贵。

如果地址在另一页上，通过软件或硬件预取代码或数据可能会隐式预取TLB的条目。这不能用于硬件预取，因为硬件可能会启动无效的页表遍历。因此，程序员不能依靠硬件预取来预取TLB条目。因此必须显示使用预取指令(如果需要的时候)。TLB就像数据和指令缓存一样，可以出现在多个级别。与数据缓存一样，TLB通常以两种形式出现：指令TLB（ITLB）和数据TLB（DTLB）。与其他缓存一样，L2TLB等更高级别的TLB通常是统一的。

### 4.3.1 使用TLB的警告

TLB是处理器核心的全局资源。在处理器核心上执行的所有线程和进程都使用相同的TLB。由于虚拟地址到物理地址的转换取决于安装了哪个页表树，因此如果页表发生更改，CPU不能盲目地重用缓存的条目。每个进程都有不同的页表树（但不是同一进程中的线程），内核和VMM（虚拟机监控程序）也有不同的页表树（如果存在的话）。进程的地址空间布局也可能发生变化。有两种方法可以解决这个问题：

- 每当更改页表树时，TLB都会刷新。
- TLB条目的标记被扩展，以额外且唯一地标识它们所引用的页表树。

在第一种情况下，只要执行上下文切换，就会刷新TLB。由于在大多数操作系统中，从一个线程/进程切换到另一个线程/进程需要执行一些内核代码，因此TLB刷新仅限于进入和离开内核地址空间。在虚拟化系统上，当内核调用VMM并返回时，也会发生这种情况(指TLB刷新)。

刷新TLB有效但代价高。例如，在执行系统调用时，内核代码可能会被限制为几千条指令，这些指令可能会涉及几个新page（或者一个hugepage，就像某些体系结构上的Linux那样）。这项工作只会在触摸页面时替换尽可能多的TLB条目。对于拥有128个ITLB和256个DTLB条目的Intel Core2体系结构，刷新TLB意味着不必要地刷新了100多个条目和200多个条目。当系统调用返回到同一进程时，所有被刷新的TLB条目都可能被再次使用，但它们将不在TLB了。内核或VMM中经常使用的代码也是如此。在进入内核的每个条目上，必须从头开始填充TLB，即使内核和VMM的页表通常不会更改，因此，理论上，TLB条目可以保留很长时间。这也解释了为什么当今处理器中的TLB缓存并不更大：程序很可能不会运行足够长的时间来填充所有这些条目。

当然，这一事实并没有逃过CPU架构师的眼睛。优化缓存刷新的一种可能性是独立分别使TLB条目无效。例如，如果内核代码和数据属于特定的地址范围，则只有属于该地址范围的页面必须从TLB中移出。这只需要比较标签，因此并不十分昂贵。如果地址空间的一部分发生更改，例如通过调用munmap，此方法也很有用。

更好的解决方案是扩展用于TLB访问的标签。如果除了虚拟地址的一部分之外，还为每个页表树（即进程的地址空间）添加了一个唯一标识符，那么TLB根本不需要完全刷新。内核、VMM和各个进程都可以有唯一的标识符。该方案的唯一问题是，TLB标签的可用位数受到严重限制，而地址空间的数量则不受限制。这意味着一些标识符重用是必要的。发生这种情况时，必须部分刷新TLB（如果可能的话）。所有具有重用标识符的条目都必须刷新，但希望这是一个小得多的集合。

当系统上运行多个进程时，这种扩展的TLB标记具有普遍优势。如果每个可运行进程的内存使用（以及TLB项使用）都是有限的，那么当进程再次被调度时，最近使用的TLB项很有可能仍在TLB中。但还有两个额外的优势：

- 特殊的地址空间，比如内核和VMM使用的地址空间，通常只进入使用很短的时间；之后，控制权通常会返回到发起调用的地址空间。如果没有标签(tag)，将执行两次TLB刷新。使用标记时，调用地址空间的缓存翻译会被保留，而且由于内核和VMM地址空间根本不经常更改TLB条目，因此以前的系统调用等的翻译仍然可以使用。
- 在同一进程的两个线程之间切换时，根本不需要TLB刷新。如果没有扩展的TLB标签，进入内核切换时会破坏另一个线程的TLB。

一段时间以来，一些处理器已经实现了这些扩展标记。AMD推出了带有Pacifica虚拟化扩展的1-bit标签扩展。在虚拟化环境中，此`1-bit Address Space ID（ASID）`用于区分VMM的地址空间和guest域的地址空间。这允许操作系统避免在每次输入VMM（例如，为了处理页面错误）时刷新guest的TLB条目，或者在控制返回给guest时刷新VMM的TLB条目。该体系结构将允许在未来使用更多比特。其他主流处理器可能也会效仿并支持这一功能。

### 4.3.2 影响TLB性能的因素

有几个因素会影响TLB性能。首先是页面的大小。显然，一个页面越大，它容纳的指令或数据对象就越多。因此，较大的页面大小会减少所需的地址转换总数，这意味着TLB缓存中需要的条目更少。大多数架构允许使用多种不同的页面大小；有些尺寸可以同时使用。例如，x86/x86-64处理器的正常页面大小为4kB，但它们也可以分别使用4MB和2MB页面。IA-64和PowerPC允许64kB这样的大小作为基本页大小。

不过，使用大页面会带来一些问题。用于大页面的内存区域在物理内存中必须是连续的。如果管理物理内存的单元大小提高到虚拟内存页的大小，则浪费的内存量将增加。所有类型的内存操作（如加载可执行文件）都需要与页面边界对齐。这意味着，平均而言，每个映射在物理内存中浪费的页面大小是每个映射的一半。这种浪费很容易累积起来；因此，它为物理内存分配的合理单元大小设置了上限。

将单元大小增加到2MB以适应x86-64上的大页面肯定是不现实的。这个尺寸太大了。但这反过来意味着每个大页面必须由许多小页面组成。这些小页面在物理内存中必须是连续的。分配单位页面大小为4kB的2MB连续物理内存可能是一项挑战。它需要找到一个包含512个连续页面的空闲区域。在系统运行一段时间后，物理内存变得碎片化，这可能非常困难（或不可能）。

因此，在Linux上，有必要在系统启动时使用特殊的hugetlbfs文件系统预先分配这些大页面。保留固定数量的物理页面作为大型虚拟页面专用。这会限制可能并不总是被使用的资源。它也是一个有限的池；增加它通常意味着重新启动系统。尽管如此，在追求性能优越、资源充足，hugepage仍然是一种选择。数据库服务器就是一个例子。

增加最小虚拟页面大小（相对于可选的大页面）也有问题。内存映射操作（例如加载应用程序）必须符合这些页面大小。对于大多数架构来说，可执行文件的各个部分的位置具有固定的关系。如果页面大小的增加超出了构建可执行文件或DSO时考虑的范围，则无法执行加载操作。记住这个限制很重要。图4.3显示了如何确定ELF二进制文件的对齐要求。它被编码在ELF程序头中。

```
$ eu-readelf -l /bin/ls
Program Headers:
  Type   Offset   VirtAddr           PhysAddr           FileSiz  MemSiz   Flg Align
...
  LOAD   0x000000 0x0000000000400000 0x0000000000400000 0x0132ac 0x0132ac R E 0x200000
  LOAD   0x0132b0 0x00000000006132b0 0x00000000006132b0 0x001a71 0x001a71 RW  0x200000
...
```

Figure 4.3: ELF Program 头部指示对齐要求

在本例中，一个x86-64二进制文件的值为0x200000=2097152=2MB，对应于处理器支持的最大页面大小。

使用较大的页面大小还有第二个效果：页表树的级别数减少。由于虚拟地址中与页偏移量相对应的部分增加，因此不需要通过页目录处理多少位。这意味着，在TLB miss的情况下，需要完成的工作量会减少。

除了使用较大的页面大小，还可以通过将同时使用的数据移动到较少的页面来减少所需的TLB条目数量。这类似于我们上面讨论的缓存使用的一些优化。只是现在，所需的对齐量很大。鉴于TLB条目的数量非常小，这可能是一个重要的优化。

## 4.4 虚拟化的影响

操作系统映像的虚拟化将变得越来越普遍；这意味着图片中添加了另一层内存处理。进程（基本上是监狱）或操作系统容器的虚拟化不属于这一类，因为只涉及一个操作系统。Xen或KVM等技术可以在有或没有处理器帮助的情况下执行独立的操作系统映像。在这些情况下，只有一个软件可以直接控制对物理内存的访问。

![img](https://lrita.github.io/images/posts/memory/cpumemory.34.png)

Figure 4.4: Xen虚拟化模型

在Xen的情况下（见图4.4），Xen VMM就是该软件，不过VMM本身并没有实现许多其他硬件控件。与其他早期系统（以及Xen VMM的第一个版本）上的VMM不同，内存和处理器之外的硬件由特权Dom0域控制。目前，这与非特权DomU内核基本相同，就内存处理而言，它们没有区别。这里很重要的一点是，VMM将物理内存分配给Dom0和DomU内核，这些内核本身实现了通常的内存处理，就像它们直接在处理器上运行一样。

为了实现虚拟化完成所需的域分离，Dom0和DomU内核中的内存处理不具有对物理内存的无限制访问。VMM不会通过分发单独的物理页并让guest操作系统处理寻址操作；这不会对错误或恶意的guest域提供任何保护。相反，VMM为每个guest域创建自己的页表树，并使用这些数据结构寻址内存。好处是可以控制对页表树的管理信息的访问。如果代码没有适当的权限，它将无法执行任何操作。

这种访问控制在Xen提供的虚拟化中得到利用，无论是使用准虚拟化还是硬件（也称为完全）虚拟化。guest域为每个进程构建页面表树的方式故意与准虚拟化和硬件虚拟化非常相似。每当guest操作系统修改其页表时，就会调用VMM。VMM然后使用guest域中更新的信息来更新自己的影子页表。这些（影子页表）是硬件实际使用的页面表。显然，这个过程非常昂贵：页面表树的每次修改都需要调用VMM。虽然没有虚拟化，对内存映射的更改并不高效，但现在它们变得更加代价高。

考虑到从guest操作系统到切换到VMM，再切换回本身的更改已经非常代价高，额外的成本可能非常大。这就是为什么处理器开始有附加功能来避免创建影子页表。这不仅是因为速度问题，而且还减少了VMM的内存消耗。英特尔有`Extended Page Tables（EPT）`，AMD有`Nested Page Tables（NPT）`技术。本质上，这两种技术都有客户操作系统生成虚拟物理地址(注意，不是虚拟地址)的页表，然后，必须使用的EPT/NPT树将这些虚拟物理地址进一步转换为实际的物理地址。这将允许以几乎与无虚拟化情况相同的速度处理内存。它还减少了VMM的内存使用，因为现在每个域只需要维护一个页表树（与进程相反）。

附加功能的地址转换步骤的结果也存储在TLB中。这意味着TLB不存储虚拟物理地址，而是存储查找完的结果。已经有人解释说，AMD的Pacifica扩展引入ASID是为了避免每次进入时TLB刷新。ASID的bit数在处理器扩展的初始版本中为1位；这足以区分VMM和guest操作系统。英特尔的则是虚拟处理器ID（VPID），它们的用途相同，只是数量更多。但是，VPID对于每个guest域都是固定的，因此它不能用于标记单独的进程，也不能避免该级别的TLB刷新。

每个地址空间修改所需的工作量大是虚拟化操作系统的一个问题。不过，基于VMM的虚拟化还存在另一个固有问题：没有办法实现两层内存处理。但内存处理很难（特别是在考虑NUMA等复杂因素时，请参见第5节）。使用单独VMM的Xen方法使得优化（甚至是良好的）处理变得困难，因为内存管理实现的所有复杂性，包括内存区域发现等“琐碎”事情，都必须在VMM中复制。操作系统拥有成熟且优化的实现；人们真的希望避免重复它们。

![img](https://lrita.github.io/images/posts/memory/cpumemory.35.png) Figure 4.5: KVM虚拟化模型

这就是为什么总结VMM/Dom0模型是如此有吸引力的选择。图4.5显示了KVM Linux内核扩展如何解决这个问题。没有单独的VMM直接在硬件上运行并控制所有guest；相反，一个普通的Linux内核接管了这个功能。这意味着Linux内核中完整而复杂的内存处理功能用于管理(虚拟化)系统的内存。guest域与正常的用户级进程一起运行，创建者称之为“guest mode”。虚拟化（准虚拟化或完全虚拟化）由另一个用户级进程(KVM VMM)控制。这只是另一个使用内核实现的特殊KVM设备控制来guest的过程。

与Xen模型的单独VMM相比，该模型的好处在于，即使在使用guest操作系统时仍有两个内存转换机制在工作，但只需要一个实现，即Linux内核中的实现。无需在另一段代码（如Xen VMM）中复制相同的功能。这会导致更少的工作、更少的错误，并且可能会减少两个内存处理程序接触的摩擦，因为Linux客户机中的内存处理程序与运行在裸硬件上的外部Linux内核中的内存处理程序做出相同的假设。

总的来说，程序员必须意识到，使用虚拟化后，内存操作的成本至少比不使用虚拟化时要高。任何减少这项工作的优化都将在虚拟化环境中获得更大的回报。随着时间的推移，处理器设计师将通过EPT和NPT等技术越来越多地减少差异，但这种差异永远不会完全消失。

# 参考

1. [What-Every-Programmer-Should-Know-About-Memory](https://lrita.github.io/images/posts/memory/What-Every-Programmer-Should-Know-About-Memory.pdf) [↩](https://lrita.github.io/2022/04/01/programmer-should-know-about-memory-2/#fnref:1)
2. [Memory part 3: Virtual Memory](https://lwn.net/Articles/253361/) [↩](https://lrita.github.io/2022/04/01/programmer-should-know-about-memory-2/#fnref:2)

# 6 What Programmers Can Do

在前面几节的描述之后，很明显，无论是正面的还是负面的，程序员有很多很多机会影响程序的性能。这仅用于与内存相关的操作。我们将从最底层的物理 RAM 访问和 L1 缓存开始，一直到影响内存处理的操作系统功能。

## 6.1 绕过缓存

==如果不（立即）消耗刚产生的数据==，内存**存储**操作首先读取完整的缓存行，然后修改缓存数据的事实，并不有利于性能。对于那些不会很快使用的数据，该操作可能会把后续还好需要的数据先从缓存中推出，对于大型数据结构尤其如此，例如矩阵，它们被填充后再使用。在矩阵的最后一个元素被填满之前，庞大的规模将会导致第一个元素已被驱逐，从而使写入缓存无效。

对于这种情况以及类似的情况，处理器提供对**非时态**写操作的支持。在此上下文中，**非时态**意味着数据不会很快被重用，因此没有理由缓存它。这些**非时态**写操作不会先读取缓存线，然后再修改它；相反，新内容直接写入内存。

这听起来可能很昂贵，但并非必须如此。处理器将尝试使用写合并（参见第 3.3.3 节）来填充整个高速缓存行。如果成功，则根本不需要内存读取操作。对于 x86 和 x86-64 架构，gcc 提供了许多内置函数：

```c
#include <emmintrin.h>
void _mm_stream_si32(int *p, int a);
void _mm_stream_si128(int *p, __m128i a);
void _mm_stream_pd(double *p, __m128d a);

#include <xmmintrin.h>
void _mm_stream_pi(__m64 *p, __m64 a);
void _mm_stream_ps(float *p, __m128 a);

#include <ammintrin.h>
void _mm_stream_sd(double *p, __m128d a);
void _mm_stream_ss(float *p, __m128 a);
```

如果一次性处理大量数据，这些指令的使用效率最高。数据从内存中加载，经过一个或多个步骤进行处理，然后写回内存。 数据“流”过处理器，这是内置函数如此命名的原因。

内存地址必须分别对齐到 8 或 16 字节。在使用多媒体扩展的代码中，可以用这些**非时态**版本替换普通的 `_mm_store_*` 内置函数。在第 9.1 节的矩阵乘法代码中，我们没有这样做，因为写入的值会在很短的时间内重复使用。这是一个使用流指令没有用的示例。有关此代码的更多信息，请参见第 6.2.1 节。

处理器的**==写入合并缓冲区==**可以将部分写到缓存行的请求保留一定的时间。通常需要一个接一个地发出所有修改单个缓存行的指令，这样写合并才能真正进行。下面是一个示例:

```c
#include <emmintrin.h>
void setbytes(char *p, int c) {
    __m128i i = _mm_set_epi8(c, c, c, c,
                             c, c, c, c,
                             c, c, c, c,
                             c, c, c, c);
    _mm_stream_si128((__m128i *)&p[0], i);
    _mm_stream_si128((__m128i *)&p[16], i);
    _mm_stream_si128((__m128i *)&p[32], i);
    _mm_stream_si128((__m128i *)&p[48], i);
}
```

假设指针 `p` 正确对齐，调用此函数会将缓存行所在地址的所有字节设置为 `c`。写入合并逻辑将看到四个生成的 `movntdq` 指令，并且只有在执行最后一条指令之后才对内存发出写命令。总而言之，这个代码序列不仅避免了在写入之前读取缓存行，还避免了可能很快不需要的数据污染缓存。<u>==在某些情况下，这可以带来巨大的好处。使用这种技术的日常代码的一个例子是 C 运行时库中的 `memset` 函数，它应该使用像上面这样的代码序列来处理大块内存==</u>。

一些架构提供专门的解决方案。 PowerPC 架构定义了 `dcbz` 指令，可用于清除整个高速缓存行。该指令并没有真正绕过高速缓存，因为需要为结果分配高速缓存行，但没有从内存中读取数据。它比非时态存储指令更受限制，因为0缓存线只能设置为全 0，并且它会污染高速缓存(在数据是非时态的情况下)，但不需要写入合并逻辑。

为了了解非时态指令的实际操作，我们来做一个新的测试，用于测量向一个二维数组的矩阵写入。编译器在内存中布置矩阵，以便最左边的索引（`i`）在内存中按顺序布置所有元素的行，右边的索引（`j`）寻址一行中的元素。测试程序以两种方式迭代矩阵：首先增加内部循环中的列号，然后增加内部循环中的行索引。我们得到了如图 6.1 所示的行为。

<p align="center">
<img src="https://static.lwn.net/images/cpumemory/cpumemory.36.png"/>
Figure 6.1: Matrix Access Pattern
</p>

我们测量初始化一个 3000×3000 矩阵所需的时间。为了了解内存的行为，我们使用不使用缓存的**存储**指令。在 IA-32 处理器上，**非时态**提示用于此目的。为了比较，我们还测量了普通的**存储**指令。结果见表 6.1。

|              | Inner Loop Increment |        |
| ------------ | -------------------- | ------ |
|              | Row                  | Column |
| Normal       | 0.048s               | 0.127s |
| Non-Temporal | 0.048s               | 0.160s |

**Table 6.1: Timing Matrix Initialization**

对于确实使用缓存的正常写入，我们看到了预期的结果：如果按顺序使用内存，我们会得到更好的结果，整个操作的 0.048 秒转换为大约 750MB/s，相比之下，或多或少的随机访问需要0.127秒(约280MB/s)。矩阵足够大，缓存基本上是无效。

我们这里主要感兴趣的部分是绕过缓存的写入。令人惊讶的是，这里的顺序访问与使用缓存的情况一样快。这种行为的原因是处理器正在执行写合并，正如上面解释的那样。此外，非时态写的内存排序规则是放宽了的：程序需要显式插入内存屏障（x86 和 x86-64 处理器的 `sfence` 指令）。这意味着处理器可以更自由地写回数据，从而尽可能地使用可用带宽。

在内部循环中按列访问的情况就不同了。结果明显慢于有缓存的情况（0.16s，大约 225MB/s）。这里我们可以看到，写合并是不可能的，每个内存单元必须单独寻址。这需要不断地选择 RAM 芯片中的新行，并伴随着所有相关的延迟。 结果比有缓存的测试差 25%。

在读取端，直到最近，处理器除了使用**非时态访问**（NTA）取指令的弱提示外，还缺乏支持读合并。对于读，没有与写合并相对应的方法，这对于不可缓存的内存（例如内存映射 I/O）尤其不利。Intel 在 SSE4.1 扩展中引入了 **NTA 加载**。通过使用少量流式加载缓冲区而实现；每个缓冲区包含一个高速缓存行。对于给定的缓存行，第一条 `movntdqa` 指令将加载一条缓存行到缓冲区中，可能会替换另一条缓存行。随后对同一高速缓存行的 16 字节对齐访问将从加载缓冲区以很少的成本得到服务。除非有其他原因，否则不会将缓存行加载到缓存中，从而可以在不污染缓存的情况下加载大量内存。 编译器为此指令提供了一个内置函数：

```c
#include <smmintrin.h>
__m128i _mm_stream_load_si128 (__m128i *p);
```

这个内在函数应多次使用，将 16 字节块的地址作为参数传递，直到读取每个缓存行。只有这样才能启动下一个高速缓存行。 由于有一些流式读取缓冲区，因此可以一次从两个内存位置读取。

我们应该从这个实验中得到的是，只要它们是顺序的读写，现代 CPU 非常好地优化了未缓存的写入和（最近的）读取访问。当处理只使用一次的大型数据结构时，这一知识非常有用。其次，缓存可以帮助掩盖一些——但不是全部——随机访问内存的成本。由于 RAM 访问的实现，本示例中的随机访问速度降低了 70%。在改变实现之前，应该尽可能避免随机访问。

在关于预取的那一节中，我们将再次查看**非时态**标志。

## 6.2 Cache Access

程序员可以对缓存做出的最重要的改进是那些影响 L1 缓存的改进。在包括其他层次之前，我们将首先讨论它。显然，对 L1级缓存的所有优化也会影响其他缓存。所有内存访问的主题都是相同的：**改善局部性（空间和时间），并对齐代码和数据**。

### 6.2.1 Optimizing Level 1 Data Cache Access

在第 3.3 节中，我们已经看到有效使用 L1d 缓存可以在多大程度上提高性能。本节，我们将展示哪些类型的代码更改可以帮助提高性能。继续上一节，我们首先关注顺序访问内存的优化。从 3.3 节的数字可以看出，当内存被顺序访问时，处理器会自动预取数据。

示例代码是**矩阵乘法**。我们使用两个 **1000×1000** `double` 元素的方阵。对于那些忘记数学的人，给定两个矩阵 **A** 和 **B** 与元素 **a~ij~** 和 **b~ij~** 与 **0 ≤ i, j < N**，它们的乘积是：
$$
(A B)_{i j}=\sum_{k=0}^{N-1} a_{i k} b_{k j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\cdots+a_{i(N-1)} b_{(N-1) j}
$$

简单的 C 实现可以如下所示：

```c
for (i = 0; i < N; ++i)
  for (j = 0; j < N; ++j)
    for (k = 0; k < N; ++k)
      res[i][j] += mul1[i][k] * mul2[k][j];
```

两个输入矩阵是 `mul1` 和 `mul2`。 假设结果矩阵  `res` 被初始化为全零。这是一个不错且简单的实现。但是很明显，我们遇到了图 6.1 中解释的问题。当 `mul1` 被顺序访问时，内部循环推进 `mul2` 的行号。这意味着  `mul1`  的处理方式类似于图 6.1 中的左侧矩阵，而 `mul2` 的处理方式类似于右侧矩阵。这不可能是好事。

有一个很容易尝试的补救办法。由于矩阵中的每个元素都被多次访问，所以在使用第二个矩阵 `mul2` 之前重新排列（数学术语中的**转置**）。


$$
(A B)_{i j}=\sum_{k=0}^{N-1} a_{i k} b_{j k}^{\mathrm{T}}=a_{i 1} b_{j 1}^{\mathrm{T}}+a_{i 2} b_{j 2}^{\mathrm{T}}+\cdots+a_{i(N-1)} b_{j(N-1)}^{\mathrm{T}}
$$

转置（传统上由上标 **T** 表示）之后，现在依次迭代两个矩阵。 就 C 代码而言，它现在看起来像这样：

```c
double tmp[N][N];
for (i = 0; i < N; ++i)
 for (j = 0; j < N; ++j)
   tmp[i][j] = mul2[j][i];
for (i = 0; i < N; ++i)
 for (j = 0; j < N; ++j)
   for (k = 0; k < N; ++k)
     res[i][j] += mul1[i][k] * tmp[j][k];
```

我们创建一个临时变量来包含转置矩阵。这需要更多的内存，但由于每列 1000 次**非顺序**访问的开销更大（至少在现代硬件上），因此希望能够收回成本。 是时候进行一些性能测试了。 在具有 2666MHz 时钟速度的 Intel Core 2 上的结果是（以时钟周期为单位）：

|          | 原始算法       | 装置算法      |
| -------- | -------------- | ------------- |
| Cycles   | 16,765,297,870 | 3,922,373,010 |
| Relative | 100%           | 23.4%         |

通过矩阵的简单变换，实现了 76.6% 的加速！ 复制操作不是凭空捏造的，1000 次不连续访问真的很伤人。

下一个问题是这是否是我们能做的最好的。 无论如何，我们当然需要一种不需要额外副本的替代方法。我们并不总是能够执行复制：矩阵可能太大或可用内存太小。

The search for an alternative implementation should start with a close examination of the math involved and the operations performed by the original implementation. Trivial math knowledge allows us to see that the order in which the additions for each element of the result matrix are performed is irrelevant as long as each addend appears exactly once. {*We ignore arithmetic effects here which might change the occurrence of overflows, underflows, or rounding.*} This understanding allows us to look for solutions which reorder the additions performed in the inner loop of the original code.

Now let us examine the actual problem in the execution of the original code. The order in which the elements of `mul2` are accessed is: (0,0), (1,0), …, (N-1,0), (0,1), (1,1), …. The elements (0,0) and (0,1) are in the same cache line but, by the time the inner loop completes one round, this cache line has long been evicted. For this example, each round of the inner loop requires, for each of the three matrices, 1000 cache lines (with 64 bytes for the Core 2 processor). This adds up to much more than the 32k of L1d available.

But what if we handle two iterations of the middle loop together while executing the inner loop? In this case we use two `double` values from the cache line which is guaranteed to be in L1d. We cut the L1d miss rate in half. That is certainly an improvement, but, depending on the cache line size, it still might not be as good as we can get it. The Core 2 processor has a L1d cache line size of 64 bytes. The actual value can be queried using

> ```
> sysconf (_SC_LEVEL1_DCACHE_LINESIZE)
> ```

at runtime or using the `getconf` utility from the command line so that the program can be compiled for a specific cache line size. With `sizeof(double)` being 8 this means that, to fully utilize the cache line, we should unroll the middle loop 8 times. Continuing this thought, to effectively use the `res` matrix as well, i.e., to write 8 results at the same time, we should unroll the outer loop 8 times as well. We assume here cache lines of size 64 but the code works also well on systems with 32-byte cache lines since both cache lines are also 100% utilized. In general it is best to hardcode cache line sizes at compile time by using the `getconf` utility as in:

> ```
>   gcc -DCLS=$(getconf LEVEL1_DCACHE_LINESIZE) ...
> ```

If the binaries are supposed to be generic, the largest cache line size should be used. With very small L1ds this might mean that not all the data fits into the cache but such processors are not suitable for high-performance programs anyway. The code we arrive at looks something like this:

```c
#define SM (CLS / sizeof (double))

for (i = 0; i < N; i += SM)
   for (j = 0; j < N; j += SM)
       for (k = 0; k < N; k += SM)
           for (i2 = 0, rres = &res[i][j],
                rmul1 = &mul1[i][k]; i2 < SM;
                ++i2, rres += N, rmul1 += N)
               for (k2 = 0, rmul2 = &mul2[k][j];
                    k2 < SM; ++k2, rmul2 += N)
                   for (j2 = 0; j2 < SM; ++j2)
                       rres[j2] += rmul1[k2] * rmul2[j2];
```

This looks quite scary. To some extent it is but only because it incorporates some tricks. The most visible change is that we now have six nested loops. The outer loops iterate with intervals of `SM` (the cache line size divided by `sizeof(double)`). This divides the multiplication in several smaller problems which can be handled with more cache locality. The inner loops iterate over the missing indexes of the outer loops. There are, once again, three loops. The only tricky part here is that the `k2` and `j2` loops are in a different order. This is done since, in the actual computation, only one expression depends on `k2` but two depend on `j2`.

The rest of the complication here results from the fact that gcc is not very smart when it comes to optimizing array indexing. The introduction of the additional variables `rres`, `rmul1`, and `rmul2` optimizes the code by pulling common expressions out of the inner loops, as far down as possible. The default aliasing rules of the C and C++ languages do not help the compiler making these decisions (unless `restrict` is used, all pointer accesses are potential sources of aliasing). This is why Fortran is still a preferred language for numeric programming: it makes writing fast code easier. {*In theory the `restrict` keyword introduced into the C language in the 1999 revision should solve the problem. Compilers have not caught up yet, though. The reason is mainly that too much incorrect code exists which would mislead the compiler and cause it to generate incorrect object code.*}

How all this work pays off can be seen in Table 6.2.

|          | Original       | Transposed    | Sub-Matrix    | Vectorized    |
| -------- | -------------- | ------------- | ------------- | ------------- |
| Cycles   | 16,765,297,870 | 3,922,373,010 | 2,895,041,480 | 1,588,711,750 |
| Relative | 100%           | 23.4%         | 17.3%         | 9.47%         |

**Table 6.2: Matrix Multiplication Timing**

By avoiding the copying we gain another 6.1% of performance. Plus, we do not need any additional memory. The input matrices can be arbitrarily large as long as the result matrix fits into memory as well. This is a requirement for a general solution which we have now achieved.

There is one more column in Table 6.2 which has not been explained. Most modern processors nowadays include special support for vectorization. Often branded as multi-media extensions, these special instructions allow processing of 2, 4, 8, or more values at the same time. These are often SIMD (Single Instruction, Multiple Data) operations, augmented by others to get the data in the right form. The SSE2 instructions provided by Intel processors can handle two `double` values in one operation. The instruction reference manual lists the intrinsic functions which provide access to these SSE2 instructions. If these intrinsics are used the program runs another 7.3% (relative to the original) faster. The result is a program which runs in 10% of the time of the original code. Translated into numbers which people recognize, we went from 318 MFLOPS to 3.35 GFLOPS. Since we are here only interested in memory effects here, the program code is pushed out into Section 9.1.

It should be noted that, in the last version of the code, we still have some cache problems with `mul2`; prefetching still will not work. But this cannot be solved without transposing the matrix. Maybe the cache prefetching units will get smarter to recognize the patterns, then no additional change would be needed. 3.19 GFLOPS on a 2.66 GHz processor with single-threaded code is not bad, though.

What we optimized in the example of the matrix multiplication is the use of the loaded cache lines. All bytes of a cache line are always used. We just made sure they are used before the cache line is evacuated. This is certainly a special case.

It is much more common to have data structures which fill one or more cache lines where the program uses only a few members at any one time. In Figure 3.11 we have already seen the effects of large structure sizes if only few members are used.

<p align="center">
<img src="https://static.lwn.net/images/cpumemory/cpumemory.37.png"/>
Figure 6.2: Spreading Over Multiple Cache Lines
</p>

Figure 6.2 shows the results of yet another set of benchmarks performed using the by now well-known program. This time two values of the same list element are added. In one case, both elements are in the same cache line; in the other case, one element is in the first cache line of the list element and the second is in the last cache line. The graph shows the slowdown we are experiencing.

Unsurprisingly, in all cases there are no negative effects if the working set fits into L1d. Once L1d is no longer sufficient, penalties are paid by using two cache lines in the process instead of one. The red line shows the data when the list is laid out sequentially in memory. We see the usual two step patterns: about 17% penalty when the L2 cache is sufficient and about 27% penalty when the main memory has to be used.

In the case of random memory accesses the relative data looks a bit different. The slowdown for working sets which fit into L2 is between 25% and 35%. Beyond that it goes down to about 10%. This is not because the penalties get smaller but, instead, because the actual memory accesses get disproportionally more costly. The data also shows that, in some cases, the distance between the elements does matter. The Random 4 CLs curve shows higher penalties because the first and fourth cache lines are used.
An easy way to see the layout of a data structure compared to cache lines is to use the pahole program (see [dwarves]). This program examines the data structures defined in a binary. Take a program containing this definition:

```c
struct foo {
int a;
long fill[7];
int b;
};
```

Compiled on a 64-bit machine, the output of pahole contains (among other things) the information shown in Figure 6.3.

```c
struct foo {
     int                        a;                    /*     0     4 */

     /* XXX 4 bytes hole, try to pack */

     long int                   fill[7];              /*     8    56 */
     /* --- cacheline 1 boundary (64 bytes) --- */
     int                        b;                    /*    64     4 */
}; /* size: 72, cachelines: 2 */
/* sum members: 64, holes: 1, sum holes: 4 */
/* padding: 4 */
/* last cacheline: 8 bytes */
```
**Figure 6.3: Output of pahole Run**

This output tells us a lot. First, it shows that the data structure uses up more than one cache line. The tool assumes the currently-used processor's cache line size, but this value can be overridden using a command line parameter. Especially in cases where the size of the structure is barely over the limit of a cache line, and many objects of this type are allocated, it makes sense to seek a way to compress that structure. Maybe a few elements can have a smaller type, or maybe some fields are actually flags which can be represented using individual bits.

In the case of the example the compression is easy and it is hinted at by the program. The output shows that there is a hole of four bytes after the first element. This hole is caused by the alignment requirement of the structure and the `fill` element. It is easy to see that the element `b`, which has a size of four bytes (indicated by the 4 at the end of the line), fits perfectly into the gap. The result in this case is that the gap no longer exists and that the data structure fits onto one cache line. The pahole tool can perform this optimization itself. If the `—reorganize` parameter is used and the structure name is added at the end of the command line the output of the tool is the optimized structure and the cache line use. Besides moving elements to fill gaps, the tool can also optimize bit fields and combine padding and holes. For more details see [dwarves].

Having a hole which is just large enough for the trailing element is, of course, the ideal situation. For this optimization to be useful it is required that the object itself is aligned to a cache line. We get to that in a bit.

The pahole output also makes it easier to determine whether elements have to be reordered so that those elements which are used together are also stored together. Using the pahole tool, it is easily possible to determine which elements are in the same cache line and when, instead, the elements have to be reshuffled to achieve that. This is not an automatic process but the tool can help quite a bit.

The position of the individual structure elements and the way they are used is important, too. As we have seen in Section 3.5.2 the performance of code with the critical word late in the cache line is worse. This means a programmer should always follow the following two rules:

1. Always move the structure element which is most likely to be the critical word to the beginning of the structure.

2. When accessing the data structures, and the order of access is not dictated by the situation, access the elements in the order in which they are defined in the structure.


For small structures, this means that the programmer should arrange the elements in the order in which they are likely accessed. This must be handled in a flexible way to allow the other optimizations, such as filling holes, to be applied as well. For bigger data structures each cache line-sized block should be arranged to follow the rules.

Reordering elements is not worth the time it takes, though, if the object itself is not aligned. The alignment of an object is determined by the alignment requirement of the data type. Each fundamental type has its own alignment requirement. For structured types the largest alignment requirement of any of its elements determines the alignment of the structure. This is almost always smaller than the cache line size. This means even if the members of a structure are lined up to fit into the same cache line an allocated object might not have an alignment matching the cache line size. There are two ways to ensure that the object has the alignment which was used when designing the layout of the structure:

- the object can be allocated with an explicit alignment requirement. For dynamic allocation a call to `malloc` would only allocate the object with an alignment matching that of the most demanding standard type (usually `long double`). It is possible to use `posix_memalign`, though, to request higher alignments.

  ```c
  #include <stdlib.h>
  int posix_memalign(void **memptr,
                  size_t align,
                  size_t size);
  ```
  
  The function stores the pointer to the newly-allocated memory in the pointer variable pointed to by `memptr`. The memory block is `size` bytes in size and is aligned on a `align`-byte boundary.
  
  For objects allocated by the compiler (in `.data`, `.bss`, etc, and on the stack) a variable attribute can be used:
  
  ```c
  struct strtype variable
    __attribute((aligned(64)));
  ```
  
  In this case the `variable` is aligned at a 64 byte boundary regardless of the alignment requirement of the `strtype` structure. This works for global variables as well as automatic variables.
  
  This method does not work for arrays, though. Only the first element of the array would be aligned unless the size of each array element is a multiple of the alignment value. It also means that every single variable must be annotated appropriately. The use of `posix_memalign` is also not entirely free since the alignment requirements usually lead to fragmentation and/or higher memory consumption.
  
  
  
- the alignment requirement of a type can be changed using a type attribute:

  ```
  struct strtype {
   ...members...
  } __attribute((aligned(64)));
  ```
  
  This will cause the compiler to allocate all objects with the appropriate alignment, including arrays. The programmer has to take care of requesting the appropriate alignment for dynamically allocated objects, though. Here once again `posix_memalign` must be used. It is easy enough to use the `alignof` operator gcc provides and pass the value as the second parameter to `posix_memalign`.

The multimedia extensions previously mentioned in this section almost always require that the memory accesses are aligned. I.e., for 16 byte memory accesses the address is supposed to be 16 byte aligned. The x86 and x86-64 processors have special variants of the memory operations which can handle unaligned accesses but these are slower. This hard alignment requirement is nothing new for most RISC architectures which require full alignment for all memory accesses. Even if an architecture supports unaligned accesses this is sometimes slower than using appropriate alignment, especially if the misalignment causes a load or store to use two cache lines instead of one.

<p align="center">
<img src="https://static.lwn.net/images/cpumemory/cpumemory.48.png"/>
Figure 6.4: Overhead of Unaligned Accesses
</p>

Figure 6.4 shows the effects of unaligned memory accesses. The now well-known tests which increment a data element while visiting memory (sequentially or randomly) are measured, once with aligned list elements and once with deliberately misaligned elements. The graph shows the slowdown the program incurs because of the unaligned accesses. The effects are more dramatic for the sequential access case than for the random case because, in the latter case, the costs of unaligned accesses are partially hidden by the generally higher costs of the memory access. In the sequential case, for working set sizes which do fit into the L2 cache, the slowdown is about 300%. This can be explained by the reduced effectiveness of the L1 cache. Some increment operations now touch two cache lines, and beginning work on a list element now often requires reading of two cache lines. The connection between L1 and L2 is simply too congested.

For very large working set sizes, the effects of the unaligned access are still 20% to 30%—which is a lot given that the aligned access time for those sizes is long. This graph should show that alignment must be taken seriously. Even if the architecture supports unaligned accesses, this must not be taken as “they are as good as aligned accesses”.

There is some fallout from these alignment requirements, though. If an automatic variable has an alignment requirement, the compiler has to ensure that it is met in all situations. This is not trivial since the compiler has no control over the call sites and the way they handle the stack. This problem can be handled in two ways:

1. The generated code actively aligns the stack, inserting gaps if necessary. This requires code to check for alignment, create alignment, and later undo the alignment.

2. Require that all callers have the stack aligned.


All of the commonly used application binary interfaces (ABIs) follow the second route. Programs will likely fail if a caller violates the rule and alignment is needed in the callee. Keeping alignment intact does not come for free, though.

The size of a stack frame used in a function is not necessarily a multiple of the alignment. This means padding is needed if other functions are called from this stack frame. The big difference is that the stack frame size is, in most cases, known to the compiler and, therefore, it knows how to adjust the stack pointer to ensure alignment for any function which is called from that stack frame. In fact, most compilers will simply round the stack frame size up and be done with it.

This simple way to handle alignment is not possible if variable length arrays (VLAs) or `alloca` are used. In that case, the total size of the stack frame is only known at runtime. Active alignment control might be needed in this case, making the generated code (slightly) slower.

On some architectures, only the multimedia extensions require strict alignment; stacks on those architectures are always minimally aligned for the normal data types, usually 4 or 8 byte alignment for 32- and 64-bit architectures respectively. On these systems, enforcing the alignment incurs unnecessary costs. That means that, in this case, we might want to get rid of the strict alignment requirement if we know that it is never depended upon. Tail functions (those which call no other functions) which do no multimedia operations do not need alignment. Neither do functions which only call functions which need no alignment. If a large enough set of functions can be identified, a program might want to relax the alignment requirement. For x86 binaries gcc has support for relaxed stack alignment requirements:

```
-mpreferred-stack-boundary=2
```

If this option is given a value of N, the stack alignment requirement will be set to 2N bytes. So, if a value of 2 is used, the stack alignment requirement is reduced from the default (which is 16 bytes) to just 4 bytes. In most cases this means no additional alignment operation is needed since normal stack push and pop operations work on four-byte boundaries anyway. This machine-specific option can help to reduce code size and also improve execution speed. But it cannot be applied for many other architectures. Even for x86-64 it is generally not applicable since the x86-64 ABI requires that floating-point parameters are passed in an SSE register and the SSE instructions require full 16 byte alignment. Nevertheless, whenever the option is usable it can make a noticeable difference.

Efficient placement of structure elements and alignment are not the only aspects of data structures which influence cache efficiency. If an array of structures is used, the entire structure definition affects performance. Remember the results in Figure 3.11: in this case we had increasing amounts of unused data in the elements of the array. The result was that prefetching was increasingly less effective and the program, for large data sets, became less efficient.

For large working sets it is important to use the available cache as well as possible. To achieve this, it might be necessary to rearrange data structures. While it is easier for the programmer to put all the data which conceptually belongs together in the same data structure, this might not be the best approach for maximum performance. Assume we have a data structure as follows:

```C++
struct order {
  double price;
  bool paid;
  const char *buyer[5];
  long buyer_id;
};
```

Further assume that these records are stored in a big array and that a frequently-run job adds up the expected payments of all the outstanding bills. In this scenario, the memory used for the `buyer` and `buyer_id` fields is unnecessarily loaded into the caches. Judging from the data in Figure 3.11 the program will perform up to 5 times worse than it could.

It is much better to split the `order` data structure in two, storing the first two fields in one structure and the other fields elsewhere. This change certainly increases the complexity of the program, but the performance gains might justify this cost.

Finally, let's consider another cache use optimization which, while also applying to the other caches, is primarily felt in the L1d access. As seen in Figure 3.8 an increased associativity of the cache benefits normal operation. The larger the cache, the higher the associativity usually is. The L1d cache is too large to be fully associative but not large enough to have the same associativity as L2 caches. This can be a problem if many of the objects in the working set fall into the same cache set. If this leads to evictions due to overuse of a set, the program can experience delays even though much of the cache is unused. These cache misses are sometimes called *conflict misses*. Since the L1d addressing uses virtual addresses, this is actually something the programmer can have control over. If variables which are used together are also stored together the likelihood of them falling into the same set is minimized. Figure 6.5 shows how quickly the problem can hit.

<p align="center">
<img src="https://static.lwn.net/images/cpumemory/cpumemory.69.png"/>
Figure 6.5: Cache Associativity Effects
</p>

In the figure, the now familiar Follow {*The test was performed on a 32-bit machine, hence `NPAD`=15 means one 64-byte cache line per list element.*} with `NPAD`=15 test is measured with a special setup. The X–axis is the distance between two list elements, measured in empty list elements. In other words, a distance of 2 means that the next element's address is 128 bytes after the previous one. All elements are laid out in the virtual address space with the same distance. The Y–axis shows the total length of the list. Only one to 16 elements are used, meaning that the total working set size is 64 to 1024 bytes. The z–axis shows the average number of cycles needed to traverse each list element.

The result shown in the figure should not be surprising. If few elements are used, all the data fits into L1d and the access time is only 3 cycles per list element. The same is true for almost all arrangements of the list elements: the virtual addresses are nicely mapped to L1d slots with almost no conflicts. There are two (in this graph) special distance values for which the situation is different. If the distance is a multiple of 4096 bytes (i.e., distance of 64 elements) and the length of the list is greater than eight, the average number of cycles per list element increases dramatically. In these situations all entries are in the same set and, once the list length is greater than the associativity, entries are flushed from L1d and have to be re-read from L2 the next round. This results in the cost of about 10 cycles per list element.

With this graph we can determine that the processor used has an L1d cache with associativity 8 and a total size of 32kB. That means that the test could, if necessary, be used to determine these values. The same effects can be measured for the L2 cache but, here, it is more complex since the L2 cache is indexed using physical addresses and it is much larger.

For programmers this means that associativity is something worth paying attention to. Laying out data at boundaries that are powers of two happens often enough in the real world, but this is exactly the situation which can easily lead to the above effects and degraded performance. Unaligned accesses can increase the probability of conflict misses since each access might require an additional cache line.

<p align="center">
<img src="https://static.lwn.net/images/cpumemory/cpumemory.58.png"/>
Figure 6.6: Bank Address of L1d on AMD
</p>

If this optimization is performed, another related optimization is possible, too. AMD's processors, at least, implement the L1d as several individual banks. The L1d can receive two data words per cycle but only if both words are stored in different banks or in a bank with the same index. The bank address is encoded in the low bits of the virtual address as shown in Figure 6.6. If variables which are used together are also stored together the likelihood that they are in different banks or the same bank with the same index is high.

### 6.2.2 Optimizing Level 1 Instruction Cache Access

Preparing code for good L1i use needs similar techniques as good L1d use. The problem is, though, that the programmer usually does not directly influence the way L1i is used unless s/he writes code in assembler. If compilers are used, programmers can indirectly determine the L1i use by guiding the compiler to create a better code layout.

Code has the advantage that it is linear between jumps. In these periods the processor can prefetch memory efficiently. Jumps disturb this nice picture because



- the jump target might not be statically determined;

- and even if it is static the memory fetch might take a long time if it misses all caches.


These problems create stalls in execution with a possibly severe impact on performance. This is why today's processors invest heavily in branch prediction (BP). Highly specialized BP units try to determine the target of a jump as far ahead of the jump as possible so that the processor can initiate loading the instructions at the new location into the cache. They use static and dynamic rules and are increasingly good at determining patterns in execution.

Getting data into the cache as soon as possible is even more important for the instruction cache. As mentioned in Section 3.1, instructions have to be decoded before they can be executed and, to speed this up (important on x86 and x86-64), instructions are actually cached in the decoded form, not in the byte/word form read from memory.

To achieve the best L1i use programmers should look out for at least the following aspects of code generation:

1. reduce the code footprint as much as possible. This has to be balanced with optimizations like loop unrolling and inlining.

3. code execution should be linear without bubbles. {Bubbles describe graphically the holes in the execution in the pipeline of a processor which appear when the execution has to wait for resources. For more details the reader is referred to literature on processor design.}

4. aligning code when it makes sense.


We will now look at some compiler techniques available to help with optimizing programs according to these aspects.

Compilers have options to enable levels of optimization; specific optimizations can also be individually enabled. Many of the optimizations enabled at high optimization levels (-O2 and -O3 for gcc) deal with loop optimizations and function inlining. In general, these are good optimizations. If the code which is optimized in these ways accounts for a significant part of the total execution time of the program, overall performance can be improved. Inlining of functions, in particular, allows the compiler to optimize larger chunks of code at a time which, in turn, enables the generation of machine code which better exploits the processor's pipeline architecture. The handling of both code and data (through dead code elimination or value range propagation, and others) works better when larger parts of the program can be considered as a single unit.

A larger code size means higher pressure on the L1i (and also L2 and higher level) caches. This *can* lead to less performance. Smaller code can be faster. Fortunately gcc has an optimization option to specify this. If -Os is used the compiler will optimize for code size. Optimizations which are known to increase the code size are disabled. Using this option often produces surprising results. Especially if the compiler cannot really take advantage of loop unrolling and inlining, this option is a big win.

Inlining can be controlled individually as well. The compiler has heuristics and limits which guide inlining; these limits can be controlled by the programmer. The -finline-limit option specifies how large a function must be to be considered too large for inlining. If a function is called in multiple places, inlining it in all of them would cause an explosion in the code size. But there is more. Assume a function `inlcand` is called in two functions `f1` and `f2`. The functions `f1` and `f2` are themselves called in sequence.

> | With inlining                                                |      | Without inlining                                             |
> | ------------------------------------------------------------ | ---- | ------------------------------------------------------------ |
> | `start f1  code f1  inlined inlcand  more code f1 end f1 start f2  code f2  inlined inlcand  more code f2 end f2 ` |      | `start inlcand  code inlcand end inlcand start f1  code f1 end f1 start f2  code f2 end f2 ` |
>
> **Table 6.3: Inlining Vs Not**

Table 6.3 shows how the generated code could look like in the cases of no inline and inlining in both functions. If the function `inlcand` is inlined in both `f1` and `f2` the total size of the generated code is:

> size `f1` + size `f2` + 2 × size `inlcand`

If no inlining happens, the total size is smaller by `size inlcand`. This is how much more L1i and L2 cache is needed if `f1` and `f2` are called shortly after one another. Plus: if `inlcand` is not inlined, the code might still be in L1i and it will not have to be decoded again. Plus: the branch prediction unit might do a better job of predicting jumps since it has already seen the code. If the compiler default for the upper limit on the size of inlined functions is not the best for the program, it should be lowered.

There are cases, though, when inlining always makes sense. If a function is only called once it might as well be inlined. This gives the compiler the opportunity to perform more optimizations (like value range propagation, which might significantly improve the code). That inlining might be thwarted by the selection limits. gcc has, for cases like this, an option to specify that a function is always inlined. Adding the `always_inline` function attribute instructs the compiler to do exactly what the name suggests.

In the same context, if a function should never be inlined despite being small enough, the `noinline` function attribute can be used. Using this attribute makes sense even for small functions if they are called often from different places. If the L1i content can be reused and the overall footprint is reduced this often makes up for the additional cost of the extra function call. Branch prediction units are pretty good these days. If inlining can lead to more aggressive optimizations things look different. This is something which must be decided on a case-by-case basis.

The `always_inline` attribute works well if the inline code is always used. But what if this is not the case? What if the inlined function is called only occasionally:

> ```
>   void fct(void) {
>     ... code block A ...
>    if (condition)
>      inlfct()
>    ... code block C ...
>   }
> ```

The code generated for such a code sequence in general matches the structure of the sources. That means first comes the code block A, then a conditional jump which, if the condition evaluates to false, jumps forward. The code generated for the inlined `inlfct` comes next, and finally the code block C. This looks all reasonable but it has a problem.

If the `condition` is frequently false, the execution is not linear. There is a big chunk of unused code in the middle which not only pollutes the L1i due to prefetching, it also can cause problems with branch prediction. If the branch prediction is wrong the conditional expression can be very inefficient.

This is a general problem and not specific to inlining functions. Whenever conditional execution is used and it is lopsided (i.e., the expression far more often leads to one result than the other) there is the potential for false static branch prediction and thus bubbles in the pipeline. This can be prevented by telling the compiler to move the less often executed code out of the main code path. In that case the conditional branch generated for an `if` statement would jump to a place out of the order as can be seen in the following figure.



> ![img](https://static.lwn.net/images/cpumemory/cpumemory.38.png)

The upper parts represents the simple code layout. If the area B, e.g. generated from the inlined function `inlfct` above, is often not executed because the conditional I jumps over it, the prefetching of the processor will pull in cache lines containing block B which are rarely used. Using block reordering this can be changed, with a result that can be seen in the lower part of the figure. The often-executed code is linear in memory while the rarely-executed code is moved somewhere where it does not hurt prefetching and L1i efficiency.

gcc provides two methods to achieve this. First, the compiler can take profiling output into account while recompiling code and lay out the code blocks according to the profile. We will see how this works in Section 7. The second method is through explicit branch prediction. gcc recognizes `__builtin_expect`:



> ```
>   long __builtin_expect(long EXP, long C);
> ```

This construct tells the compiler that the expression `EXP` most likely will have the value `C`. The return value is `EXP`. `__builtin_expect` is meant to be used in an conditional expression. In almost all cases will it be used in the context of boolean expressions in which case it is much more convenient to define two helper macros:



> ```
>   #define unlikely(expr) __builtin_expect(!!(expr), 0)
>   #define likely(expr) __builtin_expect(!!(expr), 1)
> ```

These macros can then be used as in



> ```
>   if (likely(a > 1))
> ```

If the programmer makes use of these macros and then uses the `-freorder-blocks` optimization option gcc will reorder blocks as in the figure above. This option is enabled with `-O2` but disabled for `-Os`. There is another option to reorder block (`-freorder-blocks-and-partition`) but it has limited usefulness because it does not work with exception handling.





There is another big advantage of small loops, at least on certain processors. The Intel Core 2 front end has a special feature called Loop Stream Detector (LSD). If a loop has no more than 18 instructions (none of which is a call to a subroutine), requires only up to 4 decoder fetches of 16 bytes, has at most 4 branch instructions, and is executed more than 64 times, than the loop is sometimes locked in the instruction queue and therefore more quickly available when the loop is used again. This applies, for instance, to small inner loops which are entered many times through an outer loop. Even without such specialized hardware compact loops have advantages.



Inlining is not the only aspect of optimization with respect to L1i. Another aspect is alignment, just as for data. There are obvious differences: code is a mostly linear blob which cannot be placed arbitrarily in the address space and it cannot be influenced directly by the programmer as the compiler generates the code. There are some aspects which the programmer can control, though.

Aligning each single instruction does not make any sense. The goal is to have the instruction stream be sequential. So alignment only makes sense in strategic places. To decide where to add alignments it is necessary to understand what the advantages can be. Having an instruction at the beginning of a cache line {*For some processors cache lines are not the atomic blocks for instructions. The Intel Core 2 front end issues 16 byte blocks to the decoder. They are appropriately aligned and so no issued block can span a cache line boundary. Aligning at the beginning of a cache line still has advantages since it optimizes the positive effects of prefetching.*} means that the prefetch of the cache line is maximized. For instructions this also means the decoder is more effective. It is easy to see that, if an instruction at the end of a cache line is executed, the processor has to get ready to read a new cache line and decode the instructions. There are things which can go wrong (such as cache line misses), meaning that an instruction at the end of the cache line is, on average, not as effectively executed as one at the beginning.

Combine this with the follow-up deduction that the problem is most severe if control was just transferred to the instruction in question (and hence prefetching is not effective) and we arrive at our final conclusion where alignment of code is most useful:



- 

- at the beginning of functions;

  

- at the beginning of basic blocks which are reached only through jumps;

  

- to some extent, at the beginning of loops.

  

In the first two cases the alignment comes at little cost. Execution proceeds at a new location and, if we choose it to be at the beginning of a cache line, we optimize prefetching and decoding. {*For instruction decoding processors often use a smaller unit than cache lines, 16 bytes in case of x86 and x86-64.*} The compiler accomplishes this alignment through the insertion of a series of no-op instructions to fill the gap created by aligning the code. This “dead code” takes a little space but does not normally hurt performance.

The third case is slightly different: aligning beginning of each loop might create performance problems. The problem is that beginning of a loop often follows other code sequentially. If the circumstances are not very lucky there will be a gap between the previous instruction and the aligned beginning of the loop. Unlike in the previous two cases, this gap cannot be completely dead. After execution of the previous instruction the first instruction in the loop must be executed. This means that, following the previous instruction, there either must be a number of no-op instructions to fill the gap or there must be an unconditional jump to the beginning of the loop. Neither possibility is free. Especially if the loop itself is not executed often, the no-ops or the jump might cost more than one saves by aligning the loop.

There are three ways the programmer can influence the alignment of code. Obviously, if the code is written in assembler the function and all instructions in it can be explicitly aligned. The assembler provides for all architectures the `.align` pseudo-op to do that. For high-level languages the compiler must be told about alignment requirements. Unlike for data types and variables this is not possible in the source code. Instead a compiler option is used:



> ```
>   -falign-functions=N
> ```

This option instructs the compiler to align all functions to the next power-of-two boundary greater than `N`. That means a gap of up to `N` bytes is created. For small functions using a large value for `N` is a waste. Equally for code which is executed only rarely. The latter can happen a lot in libraries which can contain both popular and not-so-popular interfaces. A wise choice of the option value can speed things up or save memory by avoiding alignment. All alignment is turned off by using one as the value of `N` or by using the `-fno-align-functions` option.

The alignment for the second case above—beginning of basic blocks which are not reached sequentially—can be controlled with a different option:



> ```
>   -falign-jumps=N
> ```

All the other details are equivalent, the same warning about waste of memory applies.

The third case also has its own option:



> ```
>   -falign-loops=N
> ```

Yet again, the same details and warnings apply. Except that here, as explained before, alignment comes at a runtime cost since either no-ops or a jump instruction has to be executed if the aligned address is reached sequentially.

gcc knows about one more option for controlling alignment which is mentioned here only for completeness. `-falign-labels` aligns every single label in the code (basically the beginning of each basic block). This, in all but a few exceptional cases, slows down the code and therefore should not be used.

### 6.2.3 Optimizing Level 2 and Higher Cache Access

Everything said about optimizations for using level 1 cache also applies to level 2 and higher cache accesses. There are two additional aspects of last level caches:



- 

- cache misses are always very expensive. While L1 misses (hopefully) frequently hit L2 and higher cache, thus limiting the penalties, there is obviously no fallback for the last level cache.

  

- L2 caches and higher are often shared by multiple cores and/or hyper-threads. The effective cache size available to each execution unit is therefore usually less than the total cache size.

  

To avoid the high costs of cache misses, the working set size should be matched to the cache size. If data is only needed once this obviously is not necessary since the cache would be ineffective anyway. We are talking about workloads where the data set is needed more than once. In such a case the use of a working set which is too large to fit into the cache will create large amounts of cache misses which, even with prefetching being performed successfully, will slow down the program.

A program has to perform its job even if the data set is too large. It is the programmer's job to do the work in a way which minimizes cache misses. For last-level caches this is possible—just as for L1 caches—by working on the job in smaller pieces. This is very similar to the optimized matrix multiplication on Table 6.2. One difference, though, is that, for last level caches, the data blocks which are be worked on can be bigger. The code becomes yet more complicated if L1 optimizations are needed, too. Imagine a matrix multiplication where the data sets—the two input matrices and the output matrix—do not fit into the last level cache together. In this case it might be appropriate to optimize the L1 and last level cache accesses at the same time.

The L1 cache line size is usually constant over many processor generations; even if it is not, the differences will be small. It is no big problem to just assume the larger size. On processors with smaller cache sizes two or more cache lines will then be used instead of one. In any case, it is reasonable to hardcode the cache line size and optimize the code for it.

For higher level caches this is not the case if the program is supposed to be generic. The sizes of those caches can vary widely. Factors of eight or more are not uncommon. It is not possible to assume the larger cache size as a default since this would mean the code performs poorly on all machines except those with the biggest cache. The opposite choice is bad too: assuming the smallest cache means throwing away 87% of the cache or more. This is bad; as we can see from Figure 3.14 using large caches can have a huge impact on the program's speed.

What this means is that the code must dynamically adjust itself to the cache line size. This is an optimization specific to the program. All we can say here is that the programmer should compute the program's requirements correctly. Not only are the data sets themselves needed, the higher level caches are also used for other purposes; for example, all the executed instructions are loaded from cache. If library functions are used this cache usage might add up to a significant amount. Those library functions might also need data of their own which further reduces the available memory.

Once we have a formula for the memory requirement we can compare it with the cache size. As mentioned before, the cache might be shared with multiple other cores. Currently {*There definitely will sometime soon be a better way!*} the only way to get correct information without hardcoding knowledge is through the `/sys` filesystem. In Table 5.2 we have seen the what the kernel publishes about the hardware. A program has to find the directory:



> ```
> /sys/devices/system/cpu/cpu*/cache
> ```

for the last level cache. This can be recognized by the highest numeric value in the `level` file in that directory. When the directory is identified the program should read the content of the `size` file in that directory and divide the numeric value by the number of bits set in the bitmask in the file `shared_cpu_map`.

The value which is computed this way is a safe lower limit. Sometimes a program knows a bit more about the behavior of other threads or processes. If those threads are scheduled on a core or hyper-thread sharing the cache, and the cache usage is known to not exhaust its fraction of the total cache size, then the computed limit might be too low to be optimal. Whether more than the fair share should be used really depends on the situation. The programmer has to make a choice or has to allow the user to make a decision.



### 6.2.4 Optimizing TLB Usage**

There are two kinds of optimization of TLB usage. The first optimization is to reduce the number of pages a program has to use. This automatically results in fewer TLB misses. The second optimization is to make the TLB lookup cheaper by reducing the number higher level directory tables which must be allocated. Fewer tables means less memory usage which can result is higher cache hit rates for the directory lookup.

The first optimization is closely related to the minimization of page faults. We will cover that topic in detail in Section 7.5. While page faults usually are a one-time cost, TLB misses are a perpetual penalty given that the TLB cache is usually small and it is flushed frequently. Page faults are orders of magnitude more expensive than TLB misses but, if a program is running long enough and certain parts of the program are executed frequently enough, TLB misses can outweigh even page fault costs. It is therefore important to regard page optimization not only from the perspective of page faults but also from the TLB miss perspective. The difference is that, while page fault optimizations only require page-wide grouping of the code and data, TLB optimization requires that, at any point in time, as few TLB entries are in use as possible.

The second TLB optimization is even harder to control. The number of page directories which have to be used depends on the distribution of the address ranges used in the virtual address space of the process. Widely varying locations in the address space mean more directories. A complication is that Address Space Layout Randomization (ASLR) leads to exactly these situations. The load addresses of stack, DSOs, heap, and possibly executable are randomized at runtime to prevent attackers of the machine from guessing the addresses of functions or variables.

For maximum performance ASLR certainly should be turned off. The costs of the extra directories is low enough, though, to make this step unnecessary in all but a few extreme cases. One possible optimization the kernel could at any time perform is to ensure that a single mapping does not cross the address space boundary between two directories. This would limit ASLR in a minimal fashion but not enough to substantially weaken it.

The only way a programmer is directly affected by this is when an address space region is explicitly requested. This happens when using `mmap` with `MAP_FIXED`. Allocating new a address space region this way is very dangerous and hardly ever done. It is possible, though, and, if it is used, the programmer should know about the boundaries of the last level page directory and select the requested address appropriately.


## 6.3 Prefetching

**预取的目的是隐藏内存访问的延迟**。当代处理器的命令管道和乱序 (OOO) 执行功能可以隐藏一些延迟，但充其量仅适用于命中缓存的访问。为了覆盖主内存访问的延迟，命令队列必须非常长。 一些没有 OOO 的处理器试图通过增加内核数量来弥补，但除非所有正在运行的代码都是并行的，否则这是一个糟糕的交易。

预取可以进一步帮助隐藏延迟。处理器自行执行预取，但由某些事件触发（硬件预取）或程序明确请求（软件预取）。

### 6.3.1 Hardware Prefetching

**硬件预取**通常是在特定模式下，由两个或多个缓存未命中的序列触发。这些缓存未命中可以是后续或前面的缓存行。在旧实现中，只识别相邻缓存行的缓存未命中。在现代硬件中，**跨步**（stride）也被认为是一种模式，这意味着跳过固定数量的缓存行被认为是一种模式，并得到适当的处理。

如果每个缓存未命中都触发了硬件预取，那么性能会很差。随机内存访问（例如对全局变量）非常普遍，由此产生的预取将主要浪费 **FSB** 带宽。这就是为什么至少需要两次缓存未命中，才启动预取。今天的处理器都希望有不止一个内存访问流。处理器尝试自动将每个缓存未命中分配给这样的流，如果达到阈值，则启动硬件预取。今天的 CPU 可以为更高级别的缓存跟踪 8 到 16 个单独的流。

负责模式识别的单元与相应的缓存相关联。 L1d 和 L1i 缓存可以有一个预取单元。L2 和更高层次的缓存很可能有一个预取单元。L2 和更高的预取单元与使用相同缓存的所有其他内核和超线程共享。 因此，八到十六个独立流的数量迅速减少。

**预取有一个很大的弱点：它不能跨越页面边界**。当我们们意识到 CPU 支持请求分页时，原因就很明显了。如果允许预取器跨越页面边界，则访问可能会触发 OS 事件，以使页面可用。这本身就很糟糕，尤其是对性能而言。更糟糕的是，预取器不知道程序或操作系统本身的语义。因此，它可能会预取在现实生活中永远不会被请求的页面。==这意味着预取器将在处理器之前以可识别的模式访问内存区域的末尾==。这不仅是可能的，而且非常有可能。如果处理器（作为预取的副作用）触发了对这样一个页面的请求，那么如果这样的请求永远不会发生，操作系统甚至可能完全脱离其轨道。

因此，重要的是要认识到，无论预取器在预测模式方面有多好，除非显式地从新页面预取或读取，否则程序都会在页面边界处遇到缓存丢失。这是优化数据布局的另一个原因，如第 6.2 节所述，通过将不相关的数据排除在外，以最大限度地减少缓存污染。

由于这个页面的限制，处理器没有非常复杂的逻辑来识别预取模式。4k 页面仍然占主导地位，有意义的只有这么多。这些年来，识别跨步的地址范围有所增加，但超出当前常用的 512 字节窗口可能没有多大意义。目前预取单元不能识别非线性访问模式。这些模式很有可能是真正随机的，或者至少是完全不重复的，因此尝试识别它们是没有意义的。

如果意外触发了硬件预取，我们只能做这么多。一种可能是尝试检测这个问题，并改变数据和/或代码布局。事实可能会证明这很难。可能有特殊的本地化解决方案，例如在 x86 和 x86-64 处理器上使用 `ud2` 指令^35^。该指令本身不能执行，在间接跳转指令后使用；它被用作指令提取器的信号，说明处理器不应该浪费精力解码后面的内存，因为执行将在不同的位置继续。不过，这是一个非常特殊的情况。在大多数情况下，人们不得不忍受这个问题。

> 35. 或 `non-instruction`。它是推荐的未定义操作码。

可以完全或部分禁用整个处理器的硬件预取。 Intel 处理器上，模型特定寄存器 (MSR) 用于此目的（IA32_MISC_ENABLE，大多数处理器上的位  9；位 19 仅禁用相邻高速缓存行预取）。 在大多数情况下，这必须发生在内核中，因为它是一项特权操作。如果分析显示系统上运行的重要应用程序由于硬件预取而遭受带宽耗尽和过早的缓存驱逐，则可以使用 MSR。

### 6.3.2 Software Prefetching

硬件预取的优点是不必调整程序。如前所述，缺点是访问模式必须很简单，并且不能跨页面边界进行预取。由于这些原因，我们现在有了更多的可能性，其中最重要的是软件预取。软件预取需要通过插入特殊指令修改源代码。一些编译器支持 `pragmas` 来或多或少地自动插入预取指令。Intel 对编译器内置函数的约定通常用于在 x86 和 x86-64 上插入这些特殊指令：

```c
#include <xmmintrin.h>
enum _mm_hint {
    _MM_HINT_T0 = 3,
    _MM_HINT_T1 = 2,
    _MM_HINT_T2 = 1,
    _MM_HINT_NTA = 0
};
void _mm_prefetch(void *p, enum _mm_hint h);
```

程序可以在程序中的任何指针上使用 `_mm_prefetch` 内置函数。大多数处理器（当然是所有 x86 和 x86-64 处理器）都会忽略无效指针导致的错误，这使程序员的生活变得更加轻松。但是，如果传递的指针引用了有效内存，则预取单元将被指示将数据加载到缓存中，并在必要时驱逐其他数据。绝对应该避免不必要的预取，因为这可能会降低缓存的有效性并消耗内存带宽（可能是两个缓存行，如果被驱逐的缓存行是脏的）。

与 `_mm_prefetch` 内置函数一起使用的不同提示是由实现定义的。这意味着每个处理器版本实现它们的方式（略有）不同。一般来说，对于`_MM_HINT_T0`，**包含性缓存**将数据提取到缓存的所有层级中；**独占缓存**则将数据加载到最底层的缓存中，如果数据在更高层级的缓存中，则将其加载到 L1d 中。`_MM_HINT_T1` 提示将数据拉入 L2 而不是 L1d。如果有 L3 缓存，`_MM_HINT_T2` 提示可以为它做类似的事情。但是，这些细节是弱指定的，需要针对实际使用的处理器进行验证。通常，如果要立即使用数据，使用 `_MM_HINT_T0` 是正确的做法。当然，这要求 L1d 缓存足够大，可以容纳所有预取的数据。如果立即使用的工作集太大，将所有内容预取到 L1d 中是一个坏主意，应该使用其他两个提示。

第四个提示 `_MM_HINT_NTA` 的特殊之处在于它告诉处理器特别对待预取的缓存行。NTA 代表我们在 6.1 节中已经解释过的**非时态**对齐。程序告诉处理器，应该尽可能避免使用这些数据污染缓存，因为这些数据只用于很短的时间。因此包含性缓存的实现在加载时，处理器可以避免将数据读入较低层级缓存。当数据从 L1d 被逐出时，数据不需要被推入 L2 或更高，而是可以直接写入内存。如果给出此提示，处理器设计人员可能会利用其他技巧。程序员在使用这个提示时必须小心：如果直接工作集太大，并强制驱逐**加载了 NTA 提示的**高速缓存行，则会从内存中重新加载。


> **Figure 6.7: Average with Prefetch, NPAD=31**![](https://static.lwn.net/images/cpumemory/cpumemory.56.png)

<u>==图 6.7 显示了使用现在熟悉的指针追踪框架的测试结果==</u>。列表是随机的。与之前的测试不同的是，程序实际上在每个列表节点上都花费了一些时间（大约 160 个周期）。正如我们从图 3.15 中的数据中了解到的，一旦工作集大小大于最后一级缓存，程序的性能就会受到严重影响。

我们现在可以尝试通过在计算之前发出预取请求来改善这种情况。即，在每一轮循环中，我们预取一个新元素。必须仔细选择列表中预取的节点与当前处理的节点之间的距离。假设每个节点在 160 个周期内处理，并且我们必须预取两个缓存行（`NPAD`=31），5 个列表元素的距离就足够了。

图 6.7 中的结果表明预取确实有帮助。只要工作集大小不超过最后一级缓存的大小（机器有 512 kB = 2^19^ B 的 L2），这些数字是相同的。预取指令不会增加可测量的额外负担。 一旦超过 L2 大小，预取就会节省 50 到 60 个周期，最多可节省 8%。 预取的使用不能隐藏所有的性能惩罚，但它确实有点帮助。

AMD 在其 Opteron 系列的 10h 家族中实现了另一条指令：`prefetchw`。到目前为止，该指令在 Intel 方面没有等效指令，并且不能通过内置函数获得。`prefetchw` 指令将缓存行预取到 L1 中，就像其他预取指令一样。不同之处在于高速缓存行立即进入 **M** 状态。如果随后没有对缓存行进行写入，这将是一个缺点。如果有一个或多个写入，它们将被加速，因为写入不必更改缓存状态——这在预取缓存行时已经发生。

与我们在这里实现的微不足道的 8% 相比，预取可以具有更大的优势。但是要正确地使用它是出了名的困难，特别是同一个二进制文件应该在各种机器上运行良好。CPU 提供的性能计数器可以帮助程序员分析预取。可以计数和采样的事件包括硬件预取、软件预取、有用的软件预取、各个层级的缓存未命中等。在 7.1 节中，我们将介绍其中的一些事件。 所有这些计数器都是特定于机器的。

在分析程序时，首先查看缓存未命中。 当找到大量缓存未命中的源头时，应尝试为有问题的内存访问添加预取指令。这应该一次在一个地方完成，通过测量**有用预取指令**的性能计数器来检查每次修改的结果。如果这些计数器不增加，预取可能是错误的，它没有足够的时间从内存中加载，或者预取从缓存中驱逐仍然需要的内存。

今天的 gcc 能够在一种情况下自动发出预取指令。 如果循环正在遍历数组，则可以使用以下选项：

```
-fprefetch-loop-arrays
```

编译器将确定预取是否有意义，如果是，它应该向前看多远。对于小型数组，这可能是一个缺点，如果在编译时不知道数组的大小，结果可能会更糟。 gcc 手册警告说，好处在很大程度上取决于代码的形式，并且在某些情况下，代码实际上可能运行得更慢。 程序员必须谨慎使用此选项。

### 6.3.3 Special Kind of Prefetch: Speculation

The OOO execution capability of a processor allows moving instructions around if they do not conflict with each other. For instance (using this time IA-64 for the example):

```
st8        [r4] = 12
add        r5 = r6, r7;;
st8        [r18] = r5
```

This code sequence stores 12 at the address specified by register `r4`, adds the content of registers `r6` and `r7` and stores it in register `r5`. Finally it stores the sum at the address specified by register `r18`. The point here is that the add instruction can be executed before—or at the same time as—the first `st8` instruction since there is no data dependency. But what happens if one of the addends has to be loaded?

```
st8        [r4] = 12
ld8        r6 = [r8];;
add        r5 = r6, r7;;
st8        [r18] = r5
```

The extra `ld8` instruction loads the value from the address specified by the register `r8`. There is an obvious data dependency between this load instruction and the following `add` instruction (this is the reason for the `;;` after the instruction, thanks for asking). What is critical here is that the new `ld8` instruction—unlike the `add` instruction—cannot be moved in front of the first `st8`. The processor cannot determine quickly enough during the instruction decoding whether the store and load conflict, i.e., whether `r4` and `r8` might have same value. If they do have the same value, the `st8` instruction would determine the value loaded into `r6`. What is worse, the `ld8` might also bring with it a large latency in case the load misses the caches. The IA-64 architecture supports speculative loads for this case:

```
ld8.a      r6 = [r8];;
[... other instructions ...]
st8        [r4] = 12
ld8.c.clr  r6 = [r8];;
add        r5 = r6, r7;;
st8        [r18] = r5
```

The new `ld8.a` and `ld8.c.clr` instructions belong together and replace the `ld8` instruction in the previous code sequence. The `ld8.a` instruction is the speculative load. The value cannot be used directly but the processor can start the work. At the time when the `ld8.c.clr` instruction is reached the content might have been loaded already (given there is a sufficient number of instructions in the gap). The arguments for this instruction must match that for the `ld8.a` instruction. If the preceding `st8` instruction does not overwrite the value (i.e., `r4` and `r8` are the same), nothing has to be done. The speculative load does its job and the latency of the load is hidden. If the store and load do conflict the `ld8.c.clr` reloads the value from memory and we end up with the semantics of a normal `ld8` instruction.

Speculative loads are not (yet?) widely used. But as the example shows it is a very simple yet effective way to hide latencies. Prefetching is basically equivalent and, for processors with fewer registers, speculative loads probably do not make much sense. Speculative loads have the (sometimes big) advantage of loading the value directly into the register and not into the cache line where it might be evicted again (for instance, when the thread is descheduled). If speculation is available it should be used.

### 6.3.4 Helper Threads

When one tries to use software prefetching one often runs into problems with the complexity of the code. If the code has to iterate over a data structure (a list in our case) one has to implement two independent iterations in the same loop: the normal iteration doing the work and the second iteration, which looks ahead, to use prefetching. This easily gets complex enough that mistakes are likely.

Furthermore, it is necessary to determine how far to look ahead. Too little and the memory will not be loaded in time. Too far and the just loaded data might have been evicted again. Another problem is that prefetch instructions, although they do not block and wait for the memory to be loaded, take time. The instruction has to be decoded, which might be noticeable if the decoder is too busy, for instance, due to well written/generated code. Finally, the code size of the loop is increased. This decreases the L1i efficiency. If one tries to avoid parts of this cost by issuing multiple prefetch requests in a row (in case the second load does not depend on the result of the first) one runs into problems with the number of outstanding prefetch requests.

An alternative approach is to perform the normal operation and the prefetch completely separately. This can happen using two normal threads. The threads must obviously be scheduled so that the prefetch thread is populating a cache accessed by both threads. There are two special solutions worth mentioning:

- Use hyper-threads (see Figure 3.22) on the same core. In this case the prefetch can go into L2 (or even L1d).

- Use “dumber” threads than SMT threads which can do nothing but prefetch and other simple operations. This is an option processor manufacturers might explore.


The use of hyper-threads is particularly intriguing. As we have seen on Figure 3.22, the sharing of caches is a problem if the hyper-threads execute independent code. If, instead, one thread is used as a prefetch helper thread this is not a problem. To the contrary, it is the desired effect since the lowest level cache is preloaded. Furthermore, since the prefetch thread is mostly idle or waiting for memory, the normal operation of the other hyper-thread is not disturbed much if it does not have to access main memory itself. The latter is exactly what the prefetch helper thread prevents.

The only tricky part is to ensure that the helper thread is not running too far ahead. It must not completely pollute the cache so that the oldest prefetched values are evicted again. On Linux, synchronization is easily done using the `futex` system call [futexes] or, at a little bit higher cost, using the POSIX thread synchronization primitives.

> **Figure 6.8: Average with Helper Thread, NPAD=31**![](https://static.lwn.net/images/cpumemory/cpumemory.53.png)

The benefits of the approach can be seen in Figure 6.8. This is the same test as in Figure 6.7 only with the additional result added. The new test creates an additional helper thread which runs about 100 list entries ahead and reads (not only prefetches) all the cache lines of each list element. In this case we have two cache lines per list element (`NPAD`=31 on a 32-bit machine with 64 byte cache line size).

The two threads are scheduled on two hyper-threads of the same core. The test machine has only one core but the results should be about the same if there is more than one core. The affinity functions, which we will introduce in Section 6.4.3, are used to tie the threads down to the appropriate hyper-thread.

To determine which two (or more) processors the OS knows are hyper-threads, the `NUMA_cpu_level_mask` interface from libNUMA can be used (see Section 12).

```c
#include <libNUMA.h>
ssize_t NUMA_cpu_level_mask(size_t destsize,
                         cpu_set_t *dest,
                         size_t srcsize,
                         const cpu_set_t*src,
                         unsigned int level);
```

This interface can be used to determine the hierarchy of CPUs as they are connected through caches and memory. Of interest here is level 1 which corresponds to hyper-threads. To schedule two threads on two hyper-threads the libNUMA functions can be used (error handling dropped for brevity):

```c
cpu_set_t self;
NUMA_cpu_self_current_mask(sizeof(self),
                          &self);
cpu_set_t hts;
NUMA_cpu_level_mask(sizeof(hts), &hts,
                   sizeof(self), &self, 1);
CPU_XOR(&hts, &hts, &self);
```

After this code is executed we have two CPU bit sets. `self` can be used to set the affinity of the current thread and the mask in `hts` can be used to set the affinity of the helper thread. This should ideally happen before the thread is created. In Section 6.4.3 we will introduce the interface to set the affinity. If there is no hyper-thread available the `NUMA_cpu_level_mask` function will return 1. This can be used as a sign to avoid this optimization.

The result of this benchmark might be surprising (or maybe not). If the working set fits into L2, the overhead of the helper thread reduces the performance by between 10% and 60% (ignore the smallest working set sizes again, the noise is too high). This should be expected since, if all the data is already in the L2 cache, the prefetch helper thread only uses system resources without contributing to the execution.

Once the L2 size is exhausted the picture changes, though. The prefetch helper thread helps to reduce the runtime by about 25%. We still see a rising curve simply because the prefetches cannot be processed fast enough. The arithmetic operations performed by the main thread and the memory load operations of the helper thread do complement each other, though. The resource collisions are minimal which causes this synergistic effect.

The results of this test should be transferable to many other situations. Hyper-threads, often not useful due to cache pollution, shine in these situations and should be taken advantage of. The `sys` file system allows a program to find the thread siblings (see the `thread_siblings` column in Table 5.3). Once this information is available the program just has to define the affinity of the threads and then run the loop in two modes: normal operation and prefetching. The amount of memory prefetched should depend on the size of the shared cache. In this example the L2 size is relevant and the program can query the size using

```c
 sysconf(_SC_LEVEL2_CACHE_SIZE)
```

Whether or not the progress of the helper thread must be restricted depends on the program. In general it is best to make sure there is some synchronization since scheduling details could otherwise cause significant performance degradations.

### 6.3.5 Direct Cache Access

One sources of cache misses in a modern OS is the handling of incoming data traffic. Modern hardware, like Network Interface Cards (NICs) and disk controllers, has the ability to write the received or read data directly into memory without involving the CPU. This is crucial for the performance of the devices we have today, but it also causes problems. Assume an incoming packet from a network: the OS has to decide how to handle it by looking at the header of the packet. The NIC places the packet into memory and then notifies the processor about the arrival. The processor has no chance to prefetch the data since it does not know when the data will arrive, and maybe not even where exactly it will be stored. The result is a cache miss when reading the header.

Intel has added technology in their chipsets and CPUs to alleviate this problem [directcacheaccess]. The idea is to populate the cache of the CPU which will be notified about the incoming packet with the packet's data. The payload of the packet is not critical here, this data will, in general, be handled by higher-level functions, either in the kernel or at user level. The packet header is used to make decisions about the way the packet has to be handled and so this data is needed immediately.

The network I/O hardware already has DMA to write the packet. That means it communicates directly with the memory controller which potentially is integrated in the Northbridge. Another side of the memory controller is the interface to the processors through the FSB (assuming the memory controller is not integrated into the CPU itself).

The idea behind Direct Cache Access (DCA) is to extend the protocol between the NIC and the memory controller. In Figure 6.9 the first figure shows the beginning of the DMA transfer in a regular machine with North- and Southbridge.

> | ![img](https://static.lwn.net/images/cpumemory/cpumemory.54.png) | ![img](https://static.lwn.net/images/cpumemory/cpumemory.55.png) |
> | ------------------------------------------------------------ | ------------------------------------------------------------ |
> | DMA Initiated                                                | DMA and DCA Executed                                         |
>
> **Figure 6.9: Direct Cache Access**

The NIC is connected to (or is part of) the Southbridge. It initiates the DMA access but provides the new information about the packet header which should be pushed into the processor's cache.

The traditional behavior would be, in step two, to simply complete the DMA transfer with the connection to the memory. For the DMA transfers with the DCA flag set the Northbridge additionally sends the data on the FSB with a special, new DCA flag. The processor always snoops the FSB and, if it recognizes the DCA flag, it tries to load the data directed to the processor into the lowest cache. The DCA flag is, in fact, a hint; the processor can freely ignore it. After the DMA transfer is finished the processor is signaled.

The OS, when processing the packet, first has to determine what kind of packet it is. If the DCA hint is not ignored, the loads the OS has to perform to identify the packet most likely hit the cache. Multiply this saving of hundreds of cycles per packet with tens of thousands of packets which can be processed per second, and the savings add up to very significant numbers, especially when it comes to latency.

Without the integration of I/O hardware (NIC in this case), chipset, and CPUs such an optimization is not possible. It is therefore necessary to make sure to select the platform wisely if this technology is needed.

## 6.4 Multi-Thread Optimizations

When it comes to multi-threading, there are three different aspects of cache use which are important:

- Concurrency
- Atomicity
- Bandwidth

These aspects also apply to multi-process situations but, because multiple processes are (mostly) independent, it is not so easy to optimize for them. The possible multi-process optimizations are a subset of those available for the multi-thread scenario. So we will deal exclusively with the latter here.

Concurrency in this context refers to the memory effects a process experiences when running more than one thread at a time. A property of threads is that they all share the same address space and, therefore, can all access the same memory. In the ideal case, the memory regions used by the threads are distinct, in which case those threads are coupled only lightly (common input and/or output, for instance). If more than one thread uses the same data, coordination is needed; this is when atomicity comes into play. Finally, depending on the machine architecture, the available memory and inter-processor bus bandwidth available to the processors is limited. We will handle these three aspects separately in the following sections—although they are, of course, closely linked.



**6.4.1 Concurrency Optimizations**

Initially, in this section, we will discuss two separate issues which actually require contradictory optimizations. A multi-threaded application uses common data in some of its threads. Normal cache optimization calls for keeping data together so that the footprint of the application is small, thus maximizing the amount of memory which fits into the caches at any one time.

There is a problem with this approach, though: if multiple threads write to a memory location, the cache line must be in ‘E’ (exclusive) state in the L1d of each respective core. This means that a lot of RFO requests are sent, in the worst case one for each write access. So a normal write will be suddenly very expensive. If the same memory location is used, synchronization is needed (maybe through the use of atomic operations, which is handled in the next section). The problem is also visible, though, when all the threads are using different memory locations and are supposedly independent.



> **Figure 6.10: Concurrent Cache Line Access Overhead**



Figure 6.10 shows the results of this “false sharing”. The test program (shown in Section 9.3) creates a number of threads which do nothing but increment a memory location (500 million times). The measured time is from the program start until the program finishes after waiting for the last thread. The threads are pinned to individual processors. The machine has four P4 processors. The blue values represent runs where the memory allocations assigned to each thread are on separate cache lines. The red part is the penalty occurred when the locations for the threads are moved to just one cache line.

The blue measurements (when using individual cache lines) match what one would expect. The program scales without penalty to many threads. Each processor keeps its cache line in its own L1d and there are no bandwidth issues since not much code or data has to be read (in fact, it is all cached). The measured slight increase is really system noise and probably some prefetching effects (the threads use sequential cache lines).

The measured overhead, computed by dividing the time needed when using one cache line versus a separate cache line for each thread, is 390%, 734%, and 1,147% respectively. These large numbers might be surprising at first sight but, when thinking about the cache interaction needed, it should be obvious. The cache line is pulled from one processor's cache just after it has finished writing to the cache line. All processors, except the one which has the cache line at any given moment, are delayed and cannot do anything. Each additional processor will just cause more delays.

It is clear from these measurements that this scenario must be avoided in programs. Given the huge penalty, this problem is, in many situations, obvious (profiling will show the code location, at least) but there is a pitfall with modern hardware. Figure 6.11 shows the equivalent measurements when running the code on a single processor, quad core machine (Intel Core 2 QX 6700). Even with this processor's two separate L2s the test case does not show any scalability issues. There is a slight overhead when using the same cache line more than once but it does not increase with the number of cores. {*I cannot explain the lower number when all four cores are used but it is reproducible.*} If more than one of these processors were used we would, of course, see results similar to those in Figure 6.10. Despite the increasing use of multi-core processors, many machines will continue to use multiple processors and, therefore, it is important to handle this scenario correctly, which might mean testing the code on real SMP machines.



> **Figure 6.11: Overhead, Quad Core**

There is a very simple “fix” for the problem: put every variable on its own cache line. This is where the conflict with the previously mentioned optimization comes into play, specifically, the footprint of the application would increase a lot. This is not acceptable; it is therefore necessary to come up with a more intelligent solution.

What is needed is to identify which variables are used by only one thread at a time, those used by only one thread ever, and maybe those which are contested at times. Different solutions for each of these scenarios are possible and useful. The most basic criterion for the differentiation of variables is: are they ever written to and how often does this happen.

Variables which are never written to and those which are only initialized once are basically constants. Since RFO requests are only needed for write operations, constants can be shared in the cache (‘S’ state). So, these variables do not have to be treated specially; grouping them together is fine. If the programmer marks the variables correctly with `const`, the tool chain will move the variables away from the normal variables into the `.rodata` (read-only data) or `.data.rel.ro` (read-only after relocation) section {*Sections, identified by their names are the atomic units containing code and data in an ELF file.*} No other special action is required. If, for some reason, variables cannot be marked correctly with `const`, the programmer can influence their placement by assigning them to a special section.

When the linker constructs the final binary, it first appends the sections with the same name from all input files; those sections are then arranged in an order determined by the linker script. This means that, by moving all variables which are basically constant but are not marked as such into a special section, the programmer can group all of those variables together. There will not be a variable which is often written to between them. By aligning the first variable in that section appropriately, it is possible to guarantee that no false sharing happens. Assume this little example:



```
  int foo = 1;
  int bar __attribute__((section(".data.ro"))) = 2;
  int baz = 3;
  int xyzzy __attribute__((section(".data.ro"))) = 4;
```

If compiled, this input file defines four variables. The interesting part is that the variables `foo` and `baz`, and `bar` and `xyzzy` are grouped together respectively. Without the attribute definitions the compiler would allocate all four variables in the sequence in which they are defined in the source code the a section named `.data`. {*This is not guaranteed by the ISO C standard but it is how gcc works.*} With the code as-is the variables `bar` and `xyzzy` are placed in a section named `.data.ro`. The section name `.data.ro` is more or less arbitrary. A prefix of `.data.` guarantees that the GNU linker will place the section together with the other data sections.

The same technique can be applied to separate out variables which are mostly read but occasionally written. Simply choose a different section name. This separation seems to make sense in some cases like the Linux kernel.

If a variable is only ever used by one thread, there is another way to specify the variable. In this case it is possible and useful to use thread-local variables (see [mytls]). The C and C++ language in gcc allow variables to be defined as per-thread using the `__thread` keyword.



```
  int foo = 1;
  __thread int bar = 2;
  int baz = 3;
  __thread int xyzzy = 4;
```

The variables `bar` and `xyzzy` are not allocated in the normal data segment; instead each thread has its own separate area where such variables are stored. The variables can have static initializers. All thread-local variables are addressable by all other threads but, unless a thread passes a pointer to a thread-local variable to those other threads, there is no way the other threads can find that variable. Due to the variable being thread-local, false sharing is not a problem—unless the program artificially creates a problem. This solution is easy to set up (the compiler and linker do all the work), but it has its cost. When a thread is created, it has to spend some time on setting up the thread-local variables, which requires time and memory. In addition, addressing thread-local variables is usually more expensive than using global or automatic variables (see [mytls] for explanations of how the costs are minimized automatically, if possible).



One drawback of using thread-local storage (TLS) is that, if the use of the variable shifts over to another thread, the current value of the variable in the old thread is not available to new thread. Each thread's copy of the variable is distinct. Often this is not a problem at all and, if it is, the shift over to the new thread needs coordination, at which time the current value can be copied.

A second, bigger problem is possible waste of resources. If only one thread ever uses the variable at any one time, all threads have to pay a price in terms of memory. If a thread does not use any TLS variables, the lazy allocation of the TLS memory area prevents this from being a problem (except for TLS in the application itself). If a thread uses just one TLS variable in a DSO, the memory for all the other TLS variables in this object will be allocated, too. This could potentially add up if TLS variables are used on a large scale.

In general the best advice which can be given is



1. 

2. Separate at least read-only (after initialization) and read-write variables. Maybe extend this separation to read-mostly variables as a third category.

   

3. Group read-write variables which are used together into a structure. Using a structure is the only way to ensure the memory locations for all of those variables are close together in a way which is translated consistently by all gcc versions..

   

4. Move read-write variables which are often written to by different threads onto their own cache line. This might mean adding padding at the end to fill a remainder of the cache line. If combined with step 2, this is often not really wasteful. Extending the example above, we might end up with code as follows (assuming

    

   bar

    

   and

    

   xyzzy

    

   are meant to be used together):

   

   ```
     int foo = 1;
     int baz = 3;
     struct {
       struct al1 {
         int bar;
         int xyzzy;
       };
       char pad[CLSIZE - sizeof(struct al1)];
     } rwstruct __attribute__((aligned(CLSIZE))) =
       { { .bar = 2, .xyzzy = 4 } };
   ```

   Some code changes are needed (references to `bar` have to be replaced with `rwstruct.bar`, likewise for `xyzzy`) but that is all. The compiler and linker do all the rest. {*This code has to be compiled with `-fms-extensions`} on the command line.*}

   

5. If a variable is used by multiple threads, but every use is independent, move the variable into TLS.

   

**6.4.2 Atomicity Optimizations**

If multiple threads modify the same memory location concurrently, processors do not guarantee any specific result. This is a deliberate decision made to avoid costs which are unnecessary in 99.999% of all cases. For instance, if a memory location is in the ‘S’ state and two threads concurrently have to increment its value, the execution pipeline does not have to wait for the cache line to be available in the ‘E’ state before reading the old value from the cache to perform the addition. Instead it reads the value currently in the cache and, once the cache line is available in state ‘E’, the new value is written back. The result is not as expected if the two cache reads in the two threads happen simultaneously; one addition will be lost.

To assure this does not happen, processors provide atomic operations. These atomic operations would, for instance, not read the old value until it is clear that the addition could be performed in a way that the addition to the memory location appears as atomic. In addition to waiting for other cores and processors, some processors even signal atomic operations for specific addresses to other devices on the motherboard. All this makes atomic operations slower.

Processor vendors decided to provide different sets of atomic operations. Early RISC processors, in line with the ‘R’ for reduced, provided very few atomic operations, sometimes only an atomic bit set and test. {*HP Parisc still does not provide more…*} At the other end of the spectrum, we have x86 and x86-64 which provide a large number of atomic operations. The generally available atomic operations can be categorized in four classes:



- **Bit Test**

  These operations set or clear a bit atomically and return a status indicating whether the bit was set before or not.

- **Load Lock/Store Conditional (LL/SC)**

  {*Some people use “linked” instead of “lock”, it is all the same.*}These operations work as a pair where the special load instruction is used to start an transaction and the final store will only succeed if the location has not been modified in the meantime. The store operation indicates success or failure, so the program can repeat its efforts if necessary.

- **Compare-and-Swap (CAS)**

  This is a ternary operation which writes a value provided as a parameter into an address (the second parameter) only if the current value is the same as the third parameter value;

- **Atomic Arithmetic**

  These operations are only available on x86 and x86-64, which can perform arithmetic and logic operations on memory locations. These processors have support for non-atomic versions of these operations but RISC architectures do not. So it is no wonder that their availability is limited.

An architecture supports either the LL/SC or the CAS instruction, not both. Both approaches are basically equivalent; they allow the implementation of atomic arithmetic operations equally well, but CAS seems to be the preferred method these days. All other operations can be indirectly implemented using it. For instance, an atomic addition:



```
  int curval;
  int newval;
  do {
    curval = var;
    newval = curval + addend;
  } while (CAS(&var, curval, newval));
```

The result of the `CAS` call indicates whether the operation succeeded or not. If it returns failure (non-zero value), the loop is run again, the addition is performed, and the `CAS` call is tried again. This repeats until it is successful. Noteworthy about the code is that the address of the memory location has to be computed in two separate instructions. {*The `CAS` opcode on x86 and x86-64 can avoid the load of the value in the second and later iterations but, on this platform, we can write the atomic addition in a simpler way, with a single addition opcode.*} For LL/SC the code looks about the same.



```
  int curval;
  int newval;
  do {
    curval = LL(var);
    newval = curval + addend;
  } while (SC(var, newval));
```

Here we have to use a special load instruction (`LL`) and we do not have to pass the current value of the memory location to `SC` since the processor knows if the memory location has been modified in the meantime.



The big differentiators are x86 and x86-64 where we have the atomic operations and, here, it is important to select the proper atomic operation to achieve the best result. Figure 6.12 shows three different ways to implement an atomic increment operation.



> | `    for (i = 0; i < N; ++i)      __sync_add_and_fetch(&var,1); `**1. Add and Read Result** |
> | ------------------------------------------------------------ |
> | `    for (i = 0; i < N; ++i)      __sync_fetch_and_add(&var,1); `**2. Add and Return Old Value** |
> | `    for (i = 0; i < N; ++i) {      long v, n;      do {        v = var;        n = v + 1;      } while (!__sync_bool_compare_and_swap(&var, v, n));    } `**3. Atomic Replace with New Value** |
>
> **Figure 6.12: Atomic Increment in a Loop**

All three produce different code on x86 and x86-64 while the code might be identical on other architectures. There are huge performance differences. The following table shows the execution time for 1 million increments by four concurrent threads. The code uses the built-in primitives of gcc (`__sync_`*).



> | 1. Exchange Add | 2. Add Fetch | 3. CAS |
> | --------------- | ------------ | ------ |
> | 0.23s           | 0.21s        | 0.73s  |

The first two numbers are similar; we see that returning the old value is a little bit faster. The important piece of information is the highlighted field, the cost when using CAS. It is, unsurprisingly, a lot more expensive. There are several reasons for this: 1. there are two memory operations, 2. the CAS operation by itself is more complicated and requires even conditional operation, and 3. the whole operation has to be done in a loop in case two concurrent accesses cause a CAS call to fail.

Now a reader might ask a question: why would somebody use the complicated and longer code which utilizes CAS? The answer to this is: the complexity is usually hidden. As mentioned before, CAS is currently the unifying atomic operation across all interesting architectures. So some people think it is sufficient to define all atomic operations in terms of CAS. This makes programs simpler. But as the numbers show, the results can be everything but optimal. The memory handling overhead of the CAS solution is huge. The following illustrates the execution of just two threads, each on its own core.



> | Thread #1   | Thread #2   | `var` Cache State |
> | ----------- | ----------- | ----------------- |
> | `v = var`   |             | ‘E’ on Proc 1     |
> | `n = v + 1` | `v = var`   | ‘S’ on Proc 1+2   |
> | CAS(`var`)  | `n = v + 1` | ‘E’ on Proc 1     |
> |             | CAS(`var`)  | ‘E’ on Proc 2     |

We see that, within this short period of execution, the cache line status changes at least three times; two of the changes are RFOs. Additionally, the second CAS will fail, so that thread has to repeat the whole operation. During that operation the same can happen again.

In contrast, when the atomic arithmetic operations are used, the processor can keep the load and store operations needed to perform the addition (or whatever) together. It can ensure that concurrently-issued cache line requests are blocked until the atomic operation is done. Each loop iteration in the example therefore results in, at most, one RFO cache request and nothing else.

What all this means is that it is crucial to define the machine abstraction at a level at which atomic arithmetic and logic operations can be utilized. CAS should not be universally used as the unification mechanism.



For most processors, the atomic operations are, by themselves, always atomic. One can avoid them only by providing completely separate code paths for the case when atomicity is not needed. This means more code, a conditional, and further jumps to direct execution appropriately.

For x86 and x86-64 the situation is different: the same instructions can be used in both atomic and non-atomic ways. To make them atomic, a special prefix for the instruction is used: the `lock` prefix. This opens the door for atomic operations to avoid the high costs if the atomicity requirement in a given situation is not needed. Generic code in libraries, for example, which always has to be thread-safe if needed, can benefit from this. No information is needed when writing the code, the decision can be made at runtime. The trick is to jump over the `lock` prefix. This trick applies to all the instructions which the x86 and x86-64 processor allow to prefix with `lock`.



```
      cmpl $0, multiple_threads
      je   1f
      lock
  1:  add  $1, some_var
```

If this assembler code appears cryptic, do not worry, it is simple. The first instruction checks whether a variable is zero or not. Nonzero in this case indicates that more than one thread is running. If the value is zero, the second instruction jumps to label `1`. Otherwise, the next instruction is executed. This is the tricky part. If the `je` instruction does not jump, the `add` instruction is executed with the `lock` prefix. Otherwise it is executed without the `lock` prefix.

Adding a relatively expensive operation like a conditional jump (expensive in case the branch prediction fails) seems to be counter productive. Indeed it can be: if multiple threads are running most of the time, the performance is further decreased, especially if the branch prediction is not correct. But if there are many situations where only one thread is in use, the code is significantly faster. The alternative of using an if-then-else construct introduces an additional unconditional jump in both cases which can be slower. Given that an atomic operation costs on the order of 200 cycles, the cross-over point for using the trick (or the if-then-else block) is pretty low. This is definitely a technique to be kept in mind. Unfortunately this means gcc's `__sync_`* primitives cannot be used.



**6.4.3 Bandwidth Considerations**

When many threads are used, and they do not cause cache contention by using the same cache lines on different cores, there still are potential problems. Each processor has a maximum bandwidth to the memory which is shared by all cores and hyper-threads on that processor. Depending on the machine architecture (e.g., the one in Figure 2.1), multiple processors might share the same bus to memory or the Northbridge.

The processor cores themselves run at frequencies where, at full speed, even in perfect conditions, the connection to the memory cannot fulfill all load and store requests without waiting. Now, further divide the available bandwidth by the number of cores, hyper-threads, and processors sharing a connection to the Northbridge and suddenly parallelism becomes a big problem. Programs which are, in theory, very efficient may be limited by the memory bandwidth.



In Figure 3.32 we have seen that increasing the FSB speed of a processor can help a lot. This is why, with growing numbers of cores on a processor, we will also see an increase in the FSB speed. Still, this will never be enough if the program uses large working sets and it is sufficiently optimized. Programmers have to be prepared to recognize problems due to limited bandwidth.

The performance measurement counters of modern processors allow the observation of FSB contention. On Core 2 processors the `NUS_BNR_DRV` event counts the number of cycles a core has to wait because the bus is not ready. This indicates that the bus is highly used and loads from or stores to main memory take even longer than usual. The Core 2 processors support more events which can count specific bus actions like RFOs or the general FSB utilization. The latter might come in handy when investigating the possibility of scalability of an application during development. If the bus utilization rate is already close to 1.0 then the scalability opportunities are minimal.

If a bandwidth problem is recognized, there are several things which can be done. They are sometimes contradictory so some experimentation might be necessary. One solution is to buy faster computers, if there are some available. Getting more FSB speed, faster RAM modules, and possibly memory local to the processor, can—and probably will—help. It can cost a lot, though. If the program in question is only needed on one (or a few machines) the one-time expense for the hardware might cost less than reworking the program. In general, though, it is better to work on the program.

After optimizing the program itself to avoid cache misses, the only option left to achieve better bandwidth utilization is to place the threads better on the available cores. By default, the scheduler in the kernel will assign a thread to a processor according to its own policy. Moving a thread from one core to another is avoided when possible. The scheduler does not really know anything about the workload, though. It can gather information from cache misses etc but this is not much help in many situations.



> **Figure 6.13: Inefficient Scheduling**

One situation which can cause big FSB usage is when two threads are scheduled on different processors (or cores which do not share a cache) and they use the same data set. Figure 6.13 shows such a situation. Core 1 and 3 access the same data (indicated by the same color for the access indicator and the memory area). Similarly core 2 and 4 access the same data. But the threads are scheduled on different processors. This means each data set has to be read twice from memory. This situation can be handled better.



> **Figure 6.14: Efficient Scheduling**

In Figure 6.14 we see how it should ideally look like. Now the total cache size in use is reduced since now core 1 and 2 and core 3 and 4 work on the same data. The data sets have to be read from memory only once.

This is a simple example but, by extension, it applies to many situations. As mentioned before, the scheduler in the kernel has no insight into the use of data, so the programmer has to ensure that scheduling is done efficiently. There are not many kernel interfaces available to communicate this requirement. In fact, there is only one: defining thread affinity.

Thread affinity means assigning a thread to one or more cores. The scheduler will then choose among those cores (only) when deciding where to run the thread. Even if other cores are idle they will not be considered. This might sound like a disadvantage, but it is the price one has to pay. If too many threads exclusively run on a set of cores the remaining cores might mostly be idle and there is nothing one can do except change the affinity. By default threads can run on any core.

There are a number of interfaces to query and change the affinity of a thread:



> ```
> #define _GNU_SOURCE
> #include <sched.h>
> 
> int sched_setaffinity(pid_t pid, size_t size, const cpu_set_t *cpuset);
> int sched_getaffinity(pid_t pid, size_t size, cpu_set_t *cpuset);
> ```

These two interfaces are meant to be used for single-threaded code. The `pid` argument specifies which process's affinity should be changed or determined. The caller obviously needs appropriate privileges to do this. The second and third parameter specify the bitmask for the cores. The first function requires the bitmask to be filled in so that it can set the affinity. The second fills in the bitmask with the scheduling information of the selected thread. The interfaces are declared in `<sched.h>`.

The `cpu_set_t` type is also defined in that header, along with a number of macros to manipulate and use objects of this type.



> ```
> #define _GNU_SOURCE
> #include <sched.h>
> 
> #define CPU_SETSIZE
> #define CPU_SET(cpu, cpusetp)
> #define CPU_CLR(cpu, cpusetp)
> #define CPU_ZERO(cpusetp)
> #define CPU_ISSET(cpu, cpusetp)
> #define CPU_COUNT(cpusetp)
> ```

`CPU_SETSIZE` specifies how many CPUs can be represented in the data structure. The other three macros manipulate `cpu_set_t` objects. To initialize an object `CPU_ZERO` should be used; the other two macros should be used to select or deselect individual cores. `CPU_ISSET` tests whether a specific processor is part of the set. `CPU_COUNT` returns the number of cores selected in the set. The `cpu_set_t` type provide a reasonable default value for the upper limit on the number of CPUs. Over time it certainly will prove too small; at that point the type will be adjusted. This means programs always have to keep the size in mind. The above convenience macros implicitly handle the size according to the definition of `cpu_set_t`. If more dynamic size handling is needed an extended set of macros should be used:



> ```
> #define _GNU_SOURCE
> #include <sched.h>
> 
> #define CPU_SET_S(cpu, setsize, cpusetp)
> #define CPU_CLR_S(cpu, setsize, cpusetp)
> #define CPU_ZERO_S(setsize, cpusetp)
> #define CPU_ISSET_S(cpu, setsize, cpusetp)
> #define CPU_COUNT_S(setsize, cpusetp)
> ```

These interfaces take an additional parameter with the size. To be able to allocate dynamically sized CPU sets three macros are provided:



> ```
> #define _GNU_SOURCE
> #include <sched.h>
> 
> #define CPU_ALLOC_SIZE(count)
> #define CPU_ALLOC(count)
> #define CPU_FREE(cpuset)
> ```

The `CPU_ALLOC_SIZE` macro returns the number of bytes which have to be allocated for a `cpu_set_t` structure which can handle `count` CPUs. To allocate such a block the `CPU_ALLOC` macro can be used. The memory allocated this way should be freed with `CPU_FREE`. The functions will likely use `malloc` and `free` behind the scenes but this does not necessarily have to remain this way.

Finally, a number of operations on CPU set objects are defined:



> ```
> #define _GNU_SOURCE
> #include <sched.h>
> 
> #define CPU_EQUAL(cpuset1, cpuset2)
> #define CPU_AND(destset, cpuset1, cpuset2)
> #define CPU_OR(destset, cpuset1, cpuset2)
> #define CPU_XOR(destset, cpuset1, cpuset2)
> #define CPU_EQUAL_S(setsize, cpuset1, cpuset2)
> #define CPU_AND_S(setsize, destset, cpuset1, cpuset2)
> #define CPU_OR_S(setsize, destset, cpuset1, cpuset2)
> #define CPU_XOR_S(setsize, destset, cpuset1, cpuset2)
> ```

These two sets of four macros can check two sets for equality and perform logical AND, OR, and XOR operations on sets. These operations come in handy when using some of the libNUMA functions (see Section 12).

A process can determine on which processor it is currently running using the `sched_getcpu` interface:



> ```
> #define _GNU_SOURCE
> #include <sched.h>
> int sched_getcpu(void);
> ```

The result is the index of the CPU in the CPU set. Due to the nature of scheduling this number cannot always be 100% correct. The thread might have been moved to a different CPU between the time the result was returned and when the thread returns to userlevel. Programs always have to take this possibility of inaccuracy into account. More important is, in any case, the set of CPUs the thread is allowed to run on. This set can be retrieved using `sched_getaffinity`. The set is inherited by child threads and processes. Threads cannot rely on the set to be stable over the lifetime. The affinity mask can be set from the outside (see the `pid` parameter in the prototypes above); Linux also supports CPU hot-plugging which means CPUs can vanish from the system—and, therefore, also from the affinity CPU set.

In multi-threaded programs, the individual threads officially have no process ID as defined by POSIX and, therefore, the two functions above cannot be used. Instead `<pthread.h>` declares four different interfaces:



> ```
> #define _GNU_SOURCE
> #include <pthread.h>
> 
> int pthread_setaffinity_np(pthread_t th, size_t size,
>                            const cpu_set_t *cpuset);
> int pthread_getaffinity_np(pthread_t th, size_t size, cpu_set_t *cpuset);
> int pthread_attr_setaffinity_np(pthread_attr_t *at,
>                                 size_t size, const cpu_set_t *cpuset);
> int pthread_attr_getaffinity_np(pthread_attr_t *at, size_t size, 
>                                 cpu_set_t *cpuset);
> ```

The first two interfaces are basically equivalent to the two we have already seen, except that they take a thread handle in the first parameter instead of a process ID. This allows addressing individual threads in a process. It also means that these interfaces cannot be used from another process, they are strictly for intra-process use. The third and fourth interfaces use a thread attribute. These attributes are used when creating a new thread. By setting the attribute, a thread can be scheduled from the start on a specific set of CPUs. Selecting the target processors this early—instead of after the thread already started—can be of advantage on many different levels, including (and especially) memory allocation (see NUMA in Section 6.5).

Speaking of NUMA, the affinity interfaces play a big role in NUMA programming, too. We will come back to that case shortly.

So far, we have talked about the case where the working set of two threads overlaps such that having both threads on the same core makes sense. The opposite can be true, too. If two threads work on separate data sets, having them scheduled on the same core can be a problem. Both threads fight for the same cache, thereby reducing each others effective use of the cache. Second, both data sets have to be loaded into the same cache; in effect this increases the amount of data that has to be loaded and, therefore, the available bandwidth is cut in half.

The solution in this case is to set the affinity of the threads so that they cannot be scheduled on the same core. This is the opposite from the previous situation, so it is important to understand the situation one tries to optimize before making any changes.

Optimizing for cache sharing to optimize bandwidth is in reality an aspect of NUMA programming which is covered in the next section. One only has to extend the notion of “memory” to the caches. This will become ever more important once the number of levels of cache increases. For this reason, the solution to multi-core scheduling is available in the NUMA support library. See the code samples in Section 12 for ways to determine the affinity masks without hardcoding system details or diving into the depth of the `/sys` filesystem.

## 6.5 NUMA Programming

For NUMA programming everything said so far about cache optimizations applies as well. The differences only start below that level. NUMA introduces different costs when accessing different parts of the address space. With uniform memory access we can optimize to minimize page faults (see Section 7.5) but that is about it. All pages are created equal.

NUMA changes this. Access costs can depend on the page which is accessed. Differing access costs also increase the importance of optimizing for memory page locality. NUMA is inevitable for most SMP machines since both Intel with CSI (for x86,x86-64, and IA-64) and AMD (for Opteron) use it. With an increasing number of cores per processor we are likely to see a sharp reduction of SMP systems being used (at least outside data centers and offices of people with terribly high CPU usage requirements). Most home machines will be fine with just one processor and hence no NUMA issues. But this a) does not mean programmers can ignore NUMA and b) it does not mean there are not related issues.

If one thinks about generalizations to NUMA one quickly realizes the concept extends to processor caches as well. Two threads on cores using the same cache will collaborate faster than threads on cores not sharing a cache. This is not a fabricated case:



- early dual-core processors had no L2 sharing.

- Intel's Core 2 QX 6700 and QX 6800 quad core chips, for instance, have two separate L2 caches.

- as speculated early, with more cores on a chip and the desire to unify caches, we will have more levels of caches.

  

Caches form their own hierarchy, and placement of threads on cores becomes important for sharing (or not) of caches. This is not very different from the problems NUMA is facing and, therefore, the two concepts can be unified. Even people only interested in non-SMP machines should therefore read this section.

In Section 5.3 we have seen that the Linux kernel provides a lot of information which is useful—and needed—in NUMA programming. Collecting this information is not that easy, though. The currently available NUMA library on Linux is wholly inadequate for this purpose. A much more suitable version is currently under construction by the author.

The existing NUMA library, `libnuma`, part of the numactl package, provides no access to system architecture information. It is only a wrapper around the available system calls together with some convenience interfaces for commonly used operations. The system calls available on Linux today are:



- **`mbind`**

  Select binding for specified memory pages to nodes.

- **`set_mempolicy`**

  Set the default memory binding policy.

- **`get_mempolicy`**

  Get the default memory binding policy.

- **`migrate_pages`**

  Migrate all pages of a process on a given set of nodes to a different set of nodes.

- **`move_pages`**

  Move selected pages to given node or request node information about pages.

These interfaces are declared in `<numaif.h>` which comes along with the `libnuma` library. Before we go into more details we have to understand the concept of memory policies.



**6.5.1 Memory Policy**

The idea behind defining a memory policy is to allow existing code to work reasonably well in a NUMA environment without major modifications. The policy is inherited by child processes, which makes it possible to use the numactl tool. This tool can be used to, among other things, start a program with a given policy.

The Linux kernel supports the following policies:



- **`MPOL_BIND`**

  Memory is allocated only from the given set of nodes. If this is not possible allocation fails.

- **`MPOL_PREFERRED`**

  Memory is preferably allocated from the given set of nodes. If this fails memory from other nodes is considered.

- **`MPOL_INTERLEAVE`**

  Memory is allocated equally from the specified nodes. The node is selected either by the offset in the virtual memory region for VMA-based policies, or through a free-running counter for task-based policies.

- **`MPOL_DEFAULT`**

  Choose the allocation based on the default for the region.

This list seems to recursively define policies. This is half true. In fact, memory policies form a hierarchy (see Figure 6.15).



> **Figure 6.15: Memory Policy Hierarchy**

If an address is covered by a VMA policy then this policy is used. A special kind of policy is used for shared memory segments. If no policy for the specific address is present, the task's policy is used. If this is also not present the system's default policy is used.

The system default is to allocate memory local to the thread requesting the memory. No task and VMA policies are provided by default. For a process with multiple threads the local node is the “home” node, the one which first ran the process. The system calls mentioned above can be used to select different policies.



**6.5.2 Specifying Policies**

The `set_mempolicy` call can be used to set the task policy for the current thread (task in kernel-speak). Only the current thread is affected, not the entire process.



> ```
> #include <numaif.h>
> 
> long set_mempolicy(int mode, 
>                    unsigned long *nodemask, 
> 		   unsigned long maxnode);
> ```

The `mode` parameter must be one of the `MPOL_*` constants introduced in the previous section. The `nodemask` parameter specifies the memory nodes to use and `maxnode` is the number of nodes (i.e., bits) in `nodemask`. If `MPOL_DEFAULT` is used the `nodemask` parameter is ignored. If a null pointer is passed as `nodemask` for `MPOL_PREFERRED` the local node is selected. Otherwise `MPOL_PREFERRED` uses the lowest node number with the corresponding bit set in `nodemask`.

Setting a policy does not have any effect on already-allocated memory. Pages are not automatically migrated; only future allocations are affected. Note the difference between memory allocation and address space reservation: an address space region established using `mmap` is usually not automatically allocated. The first read or write operation on the memory region will allocate the appropriate page. If the policy changes between accesses to different pages of the same address space region, or if the policy allows allocation of memory from different nodes, a seemingly uniform address space region might be scattered across many memory nodes.



**6.5.3 Swapping and Policies**

If physical memory runs out, the system has to drop clean pages and save dirty pages to swap. The Linux swap implementation discards node information when it writes pages to swap. That means when the page is reused and paged in the node which is used will be chosen from scratch. The policies for the thread will likely cause a node which is close to the executing processors to be chosen, but the node might be different from the one used before.

This changing association means that the node association cannot be stored by a program as a property of the page. The association can change over time. For pages which are shared with other processes this can also happen because a process asks for it (see the discussion of `mbind` below). The kernel by itself can migrate pages if one node runs out of space while other nodes still have free space.

Any node association the user-level code learns about can therefore be true for only a short time. It is more of a hint than absolute information. Whenever accurate knowledge is required the `get_mempolicy` interface should be used (see Section 6.5.5).



**6.5.4 VMA Policy**

To set the VMA policy for an address range a different interface has to be used:



> ```
> #include <numaif.h>
> 
> long mbind(void *start, unsigned long len,
>            int mode,
>            unsigned long *nodemask,
>            unsigned long maxnode,
>            unsigned flags);
> ```

This interface registers a new VMA policy for the address range [`start`, `start` + `len`). Since memory handling operates on pages the start address must be page-aligned. The `len` value is rounded up to the next page size.

The `mode` parameter specifies, again, the policy; the values must be chosen from the list in Section 6.5.1. As with `set_mempolicy`, the `nodemask` parameter is only used for some policies. Its handling is identical.

The semantics of the `mbind` interface depends on the value of the `flags` parameter. By default, if `flags` is zero, the system call sets the VMA policy for the address range. Existing mappings are not affected. If this is not sufficient there are currently three flags to modify this behavior; they can be selected individually or together:



- **`MPOL_MF_STRICT`**

  The call to `mbind` will fail if not all pages are on the nodes specified by `nodemask`. In case this flag is used together with `MPOL_MF_MOVE` and/or `MPOL_MF_MOVEALL` the call will fail if any page cannot be moved.

- **`MPOL_MF_MOVE`**

  The kernel will try to move any page in the address range allocated on a node not in the set specified by `nodemask`. By default, only pages used exclusively by the current process's page tables are moved.

- **`MPOL_MF_MOVEALL`**

  Like `MPOL_MF_MOVE` but the kernel will try to move all pages, not just those used by the current process's page tables alone. This has system-wide implications since it influences the memory access of other processes—which are possibly not owned by the same user—as well. Therefore `MPOL_MF_MOVEALL` is a privileged operation (`CAP_NICE` capability is needed).

Note that support for `MPOL_MF_MOVE` and `MPOL_MF_MOVEALL` was added only in the 2.6.16 Linux kernel.

Calling `mbind` without any flags is most useful when the policy for a newly reserved address range has to be specified before any pages are actually allocated.



> ```
> void *p = mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_ANON, -1, 0);
> if (p != MAP_FAILED)
>   mbind(p, len, mode, nodemask, maxnode, 0);
> ```

This code sequence reserve an address space range of `len` bytes and specifies that the policy `mode` referencing the memory nodes in `nodemask` should be used. Unless the `MAP_POPULATE` flag is used with `mmap`, no memory will have been allocated by the time of the `mbind` call and, therefore, the new policy applies to all pages in that address space region.

The `MPOL_MF_STRICT` flag alone can be used to determine whether any page in the address range described by the `start` and `len` parameters to `mbind` is allocated on nodes other than those specified by `nodemask`. No allocated pages are changed. If all pages are allocated on the specified nodes, the VMA policy for the address space region will be changed according to `mode`.

Sometimes the rebalancing of memory is needed, in which case it might be necessary to move pages allocated on one node to another node. Calling `mbind` with `MPOL_MF_MOVE` set makes a best effort to achieve that. Only pages which are solely referenced by the process's page table tree are considered for moving. There can be multiple users in the form of threads or other processes which share that part of the page table tree. It is not possible to affect other processes which happen to map the same data. These pages do not share the page table entries.

If both `MPOL_MF_STRICT` and `MPOL_MF_MOVE` are passed to `mbind` the kernel will try to move all pages which are not allocated on the specified nodes. If this is not possible the call will fail. Such a call might be useful to determine whether there is a node (or set of nodes) which can house all the pages. Several combinations can be tried in succession until a suitable node is found.

The use of `MPOL_MF_MOVEALL` is harder to justify unless running the current process is the main purpose of the computer. The reason is that even pages that appear in multiple page tables are moved. That can easily affect other processes in a negative way. This operation should thus be used with caution.



**6.5.5 Querying Node Information**

The `get_mempolicy` interface can be used to query a variety of facts about the state of NUMA for a given address.



> ```
> #include <numaif.h>
> long get_mempolicy(int *policy,
>              const unsigned long *nmask,
>              unsigned long maxnode,
>              void *addr, int flags);
> ```

When `get_mempolicy` is called without a flag set in `flags`, the information about the policy for address `addr` is stored in the word pointed to by `policy` and in the bitmask for the nodes pointed to by `nmask`. If `addr` falls into an address space region for which a VMA policy has been specified, information about that policy is returned. Otherwise information about the task policy or, if necessary, system default policy will be returned.

If the `MPOL_F_NODE` flag is set in `flags`, and the policy governing `addr` is `MPOL_INTERLEAVE`, the value stored in the word pointed to by `policy` is the index of the node on which the next allocation is going to happen. This information can potentially be used to set the affinity of a thread which is going to work on the newly-allocated memory. This might be a less costly way to achieve proximity, especially if the thread has yet to be created.

The `MPOL_F_ADDR` flag can be used to retrieve yet another completely different data item. If this flag is used, the value stored in the word pointed to by `policy` is the index of the memory node on which the memory for the page containing `addr` has been allocated. This information can be used to make decisions about possible page migration, to decide which thread could work on the memory location most efficiently, and many more things.

The CPU—and therefore memory node—a thread is using is much more volatile than its memory allocations. Memory pages are, without explicit requests, only moved in extreme circumstances. A thread can be assigned to another CPU as the result of rebalancing the CPU loads. Information about the current CPU and node might therefore be short-lived. The scheduler will try to keep the thread on the same CPU, and possibly even on the same core, to minimize performance losses due to cold caches. This means it is useful to look at the current CPU and node information; one only must avoid assuming the association will not change.

libNUMA provides two interfaces to query the node information for a given virtual address space range:



> ```
> #include <libNUMA.h>
> 
> int NUMA_mem_get_node_idx(void *addr);
> int NUMA_mem_get_node_mask(void *addr,
>                            size_t size,
>                            size_t __destsize,
>                            memnode_set_t *dest);
> ```

`NUMA_mem_get_node_mask` sets in `dest` the bits for all memory nodes on which the pages in the range [`addr`, `addr`+`size`) are (or would be) allocated, according to the governing policy. `NUMA_mem_get_node` only looks at the address `addr` and returns the index of the memory node on which this address is (or would be) allocated. These interfaces are simpler to use than `get_mempolicy` and probably should be preferred.

The CPU currently used by a thread can be queried using `sched_getcpu` (see Section 6.4.3). Using this information, a program can determine the memory node(s) which are local to the CPU using the `NUMA_cpu_to_memnode` interface from libNUMA:



> ```
> #include <libNUMA.h>
> 
> int NUMA_cpu_to_memnode(size_t cpusetsize,
>                         const cpu_set_t *cpuset,
>                         size_t memnodesize,
>                         memnode_set_t *
>                         memnodeset);
> ```

A call to this function will set (in the memory node set pointed to by the fourth parameter) all the bits corresponding to memory nodes which are local to any of the CPUs in the set pointed to by the second parameter. Just like CPU information itself, this information is only correct until the configuration of the machine changes (for instance, CPUs get removed and added).

The bits in the `memnode_set_t` objects can be used in calls to the low-level functions like `get_mempolicy`. It is more convenient to use the other functions in libNUMA. The reverse mapping is available through:



> ```
> #include <libNUMA.h>
> 
> int NUMA_memnode_to_cpu(size_t memnodesize,
>                         const memnode_set_t *
>                         memnodeset,
>                         size_t cpusetsize,
>                         cpu_set_t *cpuset);
> ```

The bits set in the resulting `cpuset` are those of the CPUs local to any of the memory nodes with corresponding bits set in `memnodeset`. For both interfaces, the programmer has to be aware that the information can change over time (especially with CPU hot-plugging). In many situations, a single bit is set in the input bit set, but it is also meaningful, for instance, to pass the entire set of CPUs retrieved by a call to `sched_getaffinity` to `NUMA_cpu_to_memnode` to determine which are the memory nodes the thread ever can have direct access to.



**6.5.6 CPU and Node Sets**

Adjusting code for SMP and NUMA environments by changing the code to use the interfaces described so far might be prohibitively expensive (or impossible) if the sources are not available. Additionally, the system administrator might want to impose restrictions on the resources a user and/or process can use. For these situations the Linux kernel supports so-called CPU sets. The name is a bit misleading since memory nodes are also covered. They also have nothing to do with the `cpu_set_t` data type.

The interface to CPU sets is, at the moment, a special filesystem. It is usually not mounted (so far at least). This can be changed with



```
     mount -t cpuset none /dev/cpuset
```

Of course the mount point `/dev/cpuset` must exist. The content of this directory is a description of the default (root) CPU set. It comprises initially all CPUs and all memory nodes. The `cpus` file in that directory shows the CPUs in the CPU set, the `mems` file the memory nodes, the `tasks` file the processes.

To create a new CPU set one simply creates a new directory somewhere in the hierarchy. The new CPU set will inherit all settings from the parent. Then the CPUs and memory nodes for new CPU set can be changed by writing the new values into the `cpus` and `mems` pseudo files in the new directory.

If a process belongs to a CPU set, the settings for the CPUs and memory nodes are used as masks for the affinity and memory policy bitmasks. That means the program cannot select any CPU in the affinity mask which is not in the `cpus` file for the CPU set the process is using (i.e., where it is listed in the `tasks` file). Similarly for the node masks for the memory policy and the `mems` file.

The program will not experience any errors unless the bitmasks are empty after the masking, so CPU sets are an almost-invisible means to control program execution. This method is especially efficient on large machines with lots of CPUs and/or memory nodes. Moving a process into a new CPU set is as simple as writing the process ID into the `tasks` file of the appropriate CPU set.

The directories for the CPU sets contain a number of other files which can be used to specify details like behavior under memory pressure and exclusive access to CPUs and memory nodes. The interested reader is referred to the file `Documentation/cpusets.txt` in the kernel source tree.



**6.5.7 Explicit NUMA Optimizations**

All the local memory and affinity rules cannot help out if all threads on all the nodes need access to the same memory regions. It is, of course, possible to simply restrict the number of threads to a number supportable by the processors which are directly connected to the memory node. This does not take advantage of SMP NUMA machines, though, and is therefore not a real option.

If the data in question is read-only there is a simple solution: replication. Each node can get its own copy of the data so that no inter-node accesses are necessary. Code to do this can look like this:



> ```
> void *local_data(void) {
>   static void *data[NNODES];
>   int node =
>     NUMA_memnode_self_current_idx();
>   if (node == -1)
>     /* Cannot get node, pick one.  */
>     node = 0;
>   if (data[node] == NULL)
>     data[node] = allocate_data();
>   return data[node];
> }
> void worker(void) {
>   void *data = local_data();
>   for (...)
>     compute using data
> }
> ```

In this code the function `worker` prepares by getting a pointer to the local copy of the data by a call to `local_data`. Then it proceeds with the loop, which uses this pointer. The `local_data` function keeps a list of the already allocated copies of the data around. Each system has a limited number of memory nodes, so the size of the array with the pointers to the per-node memory copies is limited in size. The `NUMA_memnode_system_count` function from libNUMA returns this number. If the pointer for the current node, as determined by the `NUMA_memnode_self_current_idx` call, is not yet known a new copy is allocated.

It is important to realize that nothing terrible happens if the threads get scheduled onto another CPU connected to a different memory node after the `sched_getcpu` system call. It just means that the accesses using the `data` variable in `worker` access memory on another memory node. This slows the program down until `data` is computed anew, but that is all. The kernel will always avoid gratuitous rebalancing of the per-CPU run queues. If such a transfer happens it is usually for a good reason and will not happen again for the near future.

Things are more complicated when the memory area in question is writable. Simple duplication will not work in this case. Depending on the exact situation there might a number of possible solutions.

For instance, if the writable memory region is used to accumulate results, it might be possible to first create a separate region for each memory node in which the results are accumulated. Then, when this work is done, all the per-node memory regions are combined to get the total result. This technique can work even if the work never really stops, but intermediate results are needed. The requirement for this approach is that the accumulation of a result is stateless, i.e., it does not depend on the previously collected results.

It will always be better, though, to have direct access to the writable memory region. If the number of accesses to the memory region is substantial, it might be a good idea to force the kernel to migrate the memory pages in question to the local node. If the number of accesses is really high, and the writes on different nodes do not happen concurrently, this could help. But be aware that the kernel cannot perform miracles: the page migration is a copy operation and as such it is not cheap. This cost has to be amortized.



**6.5.8 Utilizing All Bandwidth**



The numbers in Figure 5.4 show that access to remote memory when the caches are ineffective is not measurably slower than access to local memory. This means a program could possibly save bandwidth to the local memory by writing data it does not have to read again into memory attached to another processor. The bandwidth of the connection to the DRAM modules and the bandwidth of the interconnects are mostly independent, so parallel use could improve overall performance.



Whether this is really possible depends on many factors. One really has to be sure that caches are ineffective since otherwise the slowdown related to remote accesses is measurable. Another big problem is whether the remote node has any needs for its own memory bandwidth. This possibility must be examined in detail before the approach is taken. In theory, using all the bandwidth available to a processor can have positive effects. A family 10h Opteron processor can be directly connected to up to four other processors. Utilizing all that additional bandwidth, perhaps coupled with appropriate prefetches (especially `prefetchw`) could lead to improvements if the rest of the system plays along.