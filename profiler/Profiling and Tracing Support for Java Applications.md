# Profiling and Tracing Support for Java Applications

## 摘要

> We demonstrate the ==feasibility== of undertaking performance evaluations for JVMs using: (1) a hybrid JVM/OS tool, such as async-profiler, (2) OS centric profiling and tracing tools based on Linux perf, and (3) the *Extended Berkeley Packet Filter Tracing* (eBPF) framework where we demonstrate the rationale behind the standard offwaketime tool, for analysing the causes of blocking latencies, and our own eBPF-based tool bcc-java, that relates changes in microarchitecture performance counter values to the execution of individual JVM and application threads at low overhead.
>
> The relative execution time overheads of the performance tools are illustrated for the DaCapo-bach-9.12 benchmarks with OpenJDK9 on an Intel Xeon E5-2690, running Ubuntu 16.04. Whereas sampling based tools can have up to 25% slowdown using 4kHz frequency, our tool bcc-java has a geometric mean of less than 5%. Only for the avrora benchmark, bcc-java has a significant overhead (37%) due to an unusually high number of futex system calls. <u>Finally, we provide a discussion on the recommended approaches to solve specific performance use-case scenarios.</u>
>

我们证明了使用以下方法对 JVM 进行性能评估的==可行性==：(1) 混合的 JVM/OS 工具，例如 *async-profiler*，(2) 基于 Linux perf 的以操作系统为中心的分析和跟踪工具，以及 (3) 我们展示了 *eBPF* 框架中标准 `offwaketime` 工具背后的基本原理，用于分析为何会阻塞，以及我们自己基于 *eBPF* 的工具 **bcc-java**，该工具以较低的成本将微体系结构性能计数器值的变化与单个 JVM 和应用程序线程的执行联系起来。

性能工具相对执行时间的开销在运行 Ubuntu 16.04 的英特尔至强 E5-2690 上使用 OpenJDK9 的 DaCapo-bach-9.12 基准测试进行说明。使用 4kHz 频率时，基于采样的工具性能可能会降低 25%，而我们的工具 `bcc-java` 引入的开销的几何平均值小于 5%。只是对于 avrora 基准测试，由于 `futex` 系统调用数量特别多，**bcc-java** 具有显著的开销（37%）。<u>最后，我们讨论了解决特定性能用例场景的推荐方法</u>。

## 1   简介

> We briefly survey standard performance evaluation approaches and tools, and their limitations for Java Virtual Machines (JVMs) concerning *Garbage Collection* (GC) log files, heap analysis, lock contention, processor core sampling, and ==bytecode based instrumentation== for the measurement of application specific performance metrics (Table [1 ](#_bookmark5)summarises tool features and capabilities). Logging involves turning on JVM flags to monitor specific JVM subsystem behavior (e.g. JIT compilation or GC). The two main approaches to profiling are tracing and sampling. ==Tracing instruments code== to measure microarchitecture or system performance metrics. Sampling profilers repeatedly collect the stack trace of called functions/methods, that describe the code running on the processing core, *on-core*, at the sampled moments in time.
>
> The Unix OS perf tool is becoming widely used as it supports both **sampling** and **tracing**. In the Java community, sampling based profiling is widely implemented with *JVM Tools Interface* (JVMTI) agent support using GetCallTrace (e.g. JProfiler). This approach is reliant on restricting sampling to safe-points, that are inserted by the JIT compiler to support GC. Such restrictions generate bias, and can lead to incorrect performance information. DTrace was a key tool popularizing tracing on Solaris, and then ported to FreeBSD, NetBSD, Mac OS, and in 2018 to Windows. Since 2016, Linux integrated similar functionality via the eBPF framework (Kernel version 4.9). This paper demonstrate the feasibility of undertaking performance evaluations for unmodified JVMs using either, (1), a hybrid JVM/OS tool, such as async-profiler, (2), OS-centric profiling and tracing tools based on perf, or (3) our own eBPF-based tracing tool, bcc-java. The contributions of this paper are:
>
> - We measure the overheads of on-core sampling with profiling for 100Hz (1 sample every 10ms), 1kHz (every 1ms) and 4kHz (every 250*µ*s) for the DaCapo benchmarks. We find that 1kHz sampling can be used with the `async-profiler` and `perf`/`perf-map-agent` with a reasonable geomean overhead of less than 5.2% and 11.2% respectively compared to normal execution.
> - We show how flamegraphs produced using `perf` combined with `perf-map-agent` can be used to identify where JIT based compilation has failed to inline successfully.
> - We report that heap allocation based profiling using the `async-profiler` is low overhead, at less than 2.8%, for all benchmarks, and that flamegraphs enable developers to easily determine the main allocation sources in their programs.
> - We present a low overhead tracing tool `bcc-java` (see Section [5)](#_bookmark11) with a geometric mean overhead of 3.6% for characterizing the performance of all service and application threads created by a JVM. The `bcc-java` tool is developed on top of Linux *eBPF Compiler Collection* (BCC) [[12](#_bookmark21), [23](#_bookmark38)] for adding tracing support to operating system kernels and applications. Performance counter measurements such as instructions executed, processor cycles and cache misses can be directly related to application thread IDs, and to VM services. The monitoring techniques in the tool have generic applicability that could be deployed to characterize any multi-threaded application.
> - We demonstrate how the BCC `offwaketime` tracing tool can be used to produce flamegraph visualizations that describe ==important aspects== of thread blocking and wakeup execution behavior without requiring JVM modifications.
> - We summarize the main features of the tools async-profiler, perf, offwaketime, and our new bcc-java tool in Table [1.](#_bookmark5)
>
> Section [2 ](#_bookmark0)generally discusses the capabilities of log file analysis tools targeting GC and JVM related memory performance analysis. Section [3 ](#_bookmark1)presents Flamegraphs [[13\] ](#_bookmark22)and how to interpret them for JVMs. Section [4 ](#_bookmark4)explains the capabilities of traditional Java based profiling tools and their limitations. The benefits of `AsyncGetCallTrace` based profiling, and the *stack fragment sampling* approach of [[15\] ](#_bookmark25)are discussed. Section [5 ](#_bookmark11)explains the rationale behind the design of the eBPF/BCC tools and the tracing tool that we have built. Section [6 ](#_bookmark12)presents our experimental methodology and performance overhead analysis. Section [7 ](#_bookmark13)presents guidelines for the use of performance evaluation tools under different use-cases, and finally Section [8 ](#_bookmark23)discusses our conclusions.
>

我们简要调研了评估 Java 虚拟机 (JVM) 性能的标准方法和工具，以及它们对 JVM 的限制，涉及 **GC** 日志文件、堆分析、锁争用、处理器采样和==基于字节码的工具==，用于测量特定于应用程序的性能指标（表 [1](#_bookmark5) 总结了这些工具的特性和功能）。日志涉及打开  JVM 标志以监视特定的 JVM 子系统行为（例如 JIT 编译或 GC）。主要的两种分析方法是跟踪和抽样。==跟踪工具代码==以**测量微体系结构**或**系统性能指标**。采样分析器反复收集被调用函数/方法的堆栈，这些函数/方法描述了在采样时刻在处理核心 *on-core* 上运行的代码。

因为支持**采样**和**跟踪**，Unix OS perf 工具正在得到广泛使用。在 Java 社区中，基于采样的分析器（例如 JProfiler）主要通过 **JVM 工具接口代理 (JVMTI)** 利用 `GetCallTrace` 来实现。这种方法依赖于将采样限制在安全点，这些安全点由 JIT 编译器插入以支持 GC。此类限制会产生偏差，并可能导致不正确的性能信息。`DTrace` 是在 Solaris 上关键的主流跟踪工具，然后移植到 FreeBSD、NetBSD、Mac OS，并于 2018 年移植到 Windows。自 2016 年以来，Linux 通过 eBPF 框架（内核版本 4.9）集成了类似的功能。本文展示了使用以下任一方法对未修改的 JVM 进行性能评估的可行性：(1) 混合 JVM/OS 工具，例如 async-profiler，(2) 基于 perf 的以操作系统为中心的分析和跟踪工具，或 (3 ) 我们自己的基于 eBPF 的跟踪工具 bcc-java。 本文的贡献是：

- 对于 [DaCapo 基准测试](https://www.cs.utexas.edu/users/mckinley/papers/dacapo-oopsla-2006.pdf)，通过 100Hz（每 10ms 1 个样本）、1kHz（每 1ms）和 4kHz（每 250*µ*s）的分析来测量 CPU 采样的开销。发现 `async-profiler` 和 `perf/perf-map-agent` 可以按 1kHz 采样，与正常执行相比，合理的几何平均开销分别小于 5.2% 和 11.2%。
- 展示如何使用 `perf` 结合 `perf-map-agent` 生成的火焰图用于识别 JIT 编译器未能成功内联的代码。
- 使用 `async-profiler` 进行基于堆分配的分析开销很低，所有基准测试的开销都低于 2.8%，并且火焰图使开发人员能够轻松确定代码中分配内存的主要源头。
- 提供了一个低开销跟踪工具`bcc-java`（参见[第 5 节](#_bookmark11)），其几何平均开销为 3.6%，用于描述由 JVM 创建的所有服务和应用程序线程的性能。`bcc-java` 工具是在 Linux *eBPF Compiler Collection* (BCC) [[12](#_bookmark21), [23](#_bookmark38)] （它们支持向操作系统内核和应用程序添加跟踪）之上开发的。<u>执行的指令</u>、<u>处理器周期</u>和<u>缓存未命中</u>等性能计数器测量可以与应用程序线程 ID 和 VM 服务直接相关。该工具中的监控技术具有通用性，可用于描述任意的多线程应用程序。
- 演示了如何使用 BCC `offwaketime` 跟踪工具生成可视化的火焰图，用来描述线程阻塞和唤醒执行行为的==重要方面==，无需修改 JVM。
- 在表 [1](#_bookmark5) 中总结了工具 `async-profiler`、`perf`、`offwaketime` 和新的 `bcc-java` 工具的主要特性。

第 [2](#_bookmark0) 节简单描述了<u>**日志文件分析工具**</u>针对 GC 和 JVM 相关内存性能分析的功能。第 [3](#_bookmark1) 节介绍了火焰图 [[13](#_bookmark22)] 以及如何基于 JVM  解释它们。第 [4](#_bookmark4) 节解释了 Java 传统分析工具的功能及其局限性，讨论了基于 `AsyncGetCallTrace` 的好处，以及**堆栈片段采样**法[[15](#_bookmark25)] 。第 [5](#_bookmark11) 节解释了设计 eBPF/BCC 工具和我们构建的跟踪工具背后的基本原理。第 [6](#_bookmark12) 节介绍了我们的实验方法和性能开销分析。第 [7](#_bookmark13) 节介绍了在不同用例下使用性能评估工具的指南，最后 [8](#_bookmark23) 节是我们的结论。

## 2   相关的工作：分析 GC 和 JVM

> JVM implementations support the logging of GC statistics by statically or dynamically turning on JVM flags. The statistics typically concern object allocations, usage of heap memory spaces, *stop-the-world* (STW) pauses, and total execution times for each GC invocation. STW pauses occur when all application threads are blocked during GC. Post-processing or runtime analysis of logs can be performed by tools such as [[10, ](#_bookmark19)[9, ](#_bookmark18)[11, ](#_bookmark20)[20\]. ](#_bookmark31)Lengauer *et al*. [[17\]](#_bookmark27) presents an in-depth analysis of the GC behavior of Java benchmarks including DaCapo using G1 GC in the HotSpot JVM. Automated processing of logs can aid identification and solution of GC related performance problems, such as increasing the maximum heap size, changing the ratio of younger to older generation heap sizes, and changing heuristics for moving objects into different heap spaces. Dynamic dashboard visualizations are typically used to present information concerning the relative health and efficiency of applications in production environments.
>
> However, GC log analysis cannot help in identifying or resolving which application code is the cause of high memory allocation and/or leaks. The JVM can provide a heap dump snapshot of the current live objects at relatively high cost. Such dumps may help developers to determine why GC execution and pause times might be higher than expected. Unfortunately heap dumps lack information on the allocation site of an object, nor can a dump help determine which thread allocated an object. Object deallocations can only be detected by comparing two subsequent dumps and finding that an object was removed. This is difficult as objects often change their heap storage location during GC where objects move between memory generation regions, and in heap compaction. Object allocation profiling and recording of every object using standard JVMs are typically based on expensive [[24\] ](#_bookmark39)combinations of bytecode based instrumentation, and stack traces. Consequently, many tools only record allocations above minimum sizes, and/or every n-th allocation in order to reduce overhead.
>
> AntTracks [[16\] ](#_bookmark26)is a customized HotSpot based JVM that instruments object allocation, movement and deallocation with a low logging overhead of less than 4.68%. The recorded events are sufficiently detailed to enable the heap’s state to be reconstructed offline, for the beginning and end of every GC cycle by incrementally applying the effect of events described in trace files. *Thread local allocation buffers* (TLABs) are used to store event traces, with data compression to minimise trace collection and storage overheads.
>
> ==Cao *et al*. [[6](#_bookmark15)] evaluated the power and performance requirements of managed runtime workload execution. Their work considered single-ISA heterogeneous *asymmetric multicore processor* (AMP) hardware with processor cores that are optimised for different power-performance tradeoffs. The main findings of interest are that GC accounts for 10% of processor execution cycles on average, and nearly 40% for lusearch from the DaCapo benchmarks==. 
>
> ==Sartor *et al*. [[21](#_bookmark32)] presented techniques to analyze the scalability of managed language applications with speedup stacks. Lightweight OS kernel modules were used to monitor an application and its VM service threads scheduling behavior with an overhead of up to 1.15% but, typically under 1%. They perform experiments on one node of an Intel Xeon E5-2650L server consisting of 2 sockets, each with 20MB shared LLC, 8 physical cores, with hyper threading enabled giving 16 logical cores on each socket, and a 64-bit 3.2.37 Linux kernel. Sufficient logical cores are present to ensure that the OS does not need to schedule out a thread other than for synchronization or I/O. A problem with this approach is that it is not expected to be portable across processor ISA architectures, and it may require recompilation for each new kernel release. For their experiments, they used JikesRVM [[4](#_bookmark35)] with the DaCapo lusearch, pmd, sunflow and xalan benchmarks. JikesRVM was modified to record the JVM application, compilation and garbage collection thread type *process identifiers* (PIDs) to enable their kernel modules to attribute microarchitectural performance counter measurements to specific JVM thread types. The *speedup stack* visualization helps developers to determine if optimization efforts that focus on application, or JVM runtime services are likely to improve performance. Our bcc-java tool development was motivated by [[21](#_bookmark32)]==.
>
> Sartor 等人[[21](#_bookmark32)] 介绍了使用加速堆栈分析托管语言应用程序的可扩展性的技术。轻量级操作系统内核模块用于监控应用程序及其 VM 服务线程调度行为，开销高达 1.15%，但通常低于 1%。他们在英特尔至强 E5-2650L 服务器的一个节点上执行实验，该服务器由 2 个插槽组成，每个插槽具有 20MB 共享 LLC、8 个物理内核，启用超线程，每个插槽上有 16 个逻辑内核，以及一个 64 位 3.2.37 Linux核心。存在足够的逻辑内核以确保操作系统不需要调度除同步或 I/O 之外的线程。这种方法的一个问题是它不能跨处理器 ISA 体系结构移植，并且可能需要为每个新内核版本重新编译。在他们的实验中，他们使用 JikesRVM [[4](#_bookmark35)] 和 DaCapo lusearch、pmd、sunflow 和 xalan 基准测试。 JikesRVM 被修改为记录 JVM 应用程序、编译和垃圾收集线程类型 *进程标识符 * (PID)，使其内核模块能够将微架构性能计数器测量值归因于特定的 JVM 线程类型。 *加速堆栈* 可视化可帮助开发人员确定专注于应用程序或 JVM 运行时服务的优化工作是否有可能提高性能。我们的 bcc-java 工具开发的动机是 [[21](#_bookmark32)]。 
>
> Sartor*等人*[[21]（#u bookmark32）]介绍了使用加速堆栈分析托管语言应用程序可伸缩性的技术。轻量级操作系统内核模块用于监控应用程序及其VM服务线程调度行为，开销高达1.15%，但通常低于1%。他们在Intel Xeon E5-2650L服务器的一个节点上进行实验，该服务器由2个套接字组成，每个套接字具有20MB共享LLC、8个物理内核，启用超线程，每个套接字上有16个逻辑内核，以及一个64位3.2.37 Linux内核。存在足够的逻辑内核，以确保操作系统不需要为同步或I/O之外的线程调度。这种方法的一个问题是，它不能跨处理器ISA体系结构移植，并且可能需要为每个新的内核版本重新编译。在他们的实验中，他们使用JikesRVM[[4]（#u bookmark35）]和DaCapo lusearch、pmd、sunflow和xalan基准测试。JikesRVM被修改为记录JVM应用程序、编译和垃圾收集线程类型*进程标识符*（PID），以使其内核模块能够将微体系结构性能计数器测量值归因于特定的JVM线程类型。*加速堆栈*可视化帮助开发人员确定专注于应用程序或JVM运行时服务的优化工作是否有可能提高性能。我们的bcc java工具开发是由[[21]（#u bookmark32）]推动的。
>
> Hofer *et al*. [[14](#_bookmark24)] modified OpenJDK8u45 to observe and trace events related to lock contention with a mean runtime overhead of 7.8%. The call-stacks of a thread blocking on a lock, and the thread holding the lock causing the blocking were recorded. They also presented a tool for users to identify locking bottlenecks in their code by analysing the traced events. They identified that many call-stacks that use locks are actually identical, therefore it is only necessary to maintain a **hash set** of known call-stacks.
>
> The BCC `offwaketime` tool performs a similar task to [[14](#_bookmark24)], except that it uses eBPF’s OS level tracing of kernel events associated with thread scheduling to determine: (1) the call-stack of a blocked thread, (2) its blocked time duration, and (3) the call-stack of the waking thread, which causes the scheduling state of the blocked thread to change to runnable. The call-stack of the blocking thread does not change whilst it is *off-core* and blocked. Hence, its callstack need only be recorded when it meets some selection criteria, such as belonging to a process that we are interested in, and if its blocking duration is within a specified range. This enables efficient and selective tracing of call-stack blocking and wakeup behavior due to thread interactions.

JVM 支持通过静态或动态打开 JVM 标志来记录 GC 统计信息。统计信息通常涉及<u>对象分配</u>、<u>堆内存空间使用</u>、<u>*stop-the-world* (STW) 暂停</u>以及<u>每次 GC 调用的总执行时间</u>。当 GC 时所有应用程序线程都被阻塞时，就会发生 STW 暂停。**后处理或运行时分析日志**可以通过 [[10](#_bookmark19), [9](#_bookmark18), [11](#_bookmark20), [20](#_bookmark31)] 等工具进行。Lengauer 等人 [[17\]](#_bookmark27) 深入分析了 Java 基准测试的 GC 行为，包括在 HotSpot JVM 中使用 G1 GC 测试 DaCapo。日志的自动处理有助于识别和解决 GC 相关的性能问题，例如增加**最大堆**、修改<u>年轻代</u>与<u>老年代</u>堆大小的比率，以及修改将对象移动到不同堆空间的**启发式方法**。**可视化动态仪表板**显示的信息通常都是关于生产环境中应用程序是否健康和是否高效。

但是，分析 GC 日志无助于定位泄漏**或/和**分配太多内存的应用程序代码。JVM 以相对较高的成本提供当前活动对象的堆转储快照。这类转储可以帮助开发人员确定 GC 执行和暂停时间高于预期的可能原因。不幸的是，堆转储缺少关于对象分配位置的信息，转储也无法确定是哪个线程分配对象。只有通过比较两个连续的转储，并发现某个对象已删除，才能检测到该对象已被释放。但问题是对象经常在 GC 和堆整理期间更改它们的**堆存储位置**，此时对象将在不同年代的内存区域中移动。分析和记录每个对象的分配，JVM 的标准方法是基于字节码的检测和堆栈跟踪的组合，这种组合开销通常很大 [[24](#_bookmark39)]。因此，许多工具只记录超过最小大小的分配，或者每 n 次分配记录一次以减少开销。

AntTracks [[16](#_bookmark26)] 是一个基于 HotSpot 定制的 JVM，通过埋点记录对象的分配、移动和释放，其日志开销不到4.68%。因为记录的事件足够详细，因此可通过增量应用日志中描述的事件，离线重建每个 GC 周期开始和结束时堆的状态。使用**线程本地分配缓冲区** (TLAB) 存储事件，并通过数据压缩将事件收集和存储开销降至最低。

==曹某**等人** [[6](#_bookmark15)] 评估了托管运行时工作负载执行的功率和性能要求。他们主要研究单 ISA 异构**非对称多核处理器** (AMP) 硬件，其处理器内核针对不同的功率性能权衡进行了优化。 令人感兴趣的主要发现是，GC 平均占处理器执行周期的 10%，而来自 DaCapo 基准测试的 lusearch 占了近40%==。

> - [ ] Sartor 等人[[21](#_bookmark32)] 介绍

霍弗等人[[14](#_bookmark24)] 修改 OpenJDK8u45 以观察和跟踪与锁竞争相关的事件，平均运行时开销为 7.8%。记录<u>阻塞在锁上的线程的</u>调用堆栈，以及导致阻塞的持有锁的线程。他们还为用户提供了一种工具，可以通过分析日志中的事件来识别代码中的锁定瓶颈。 他们发现许多使用锁的调用栈实际上是相同的，因此只需要维护一组已知调用栈的**散列集**。 

BCC `offwaketime` 工具执行与 [[14](#_bookmark24)] 类似的任务，不同之处在于它使用 eBPF，在操作系统层记录与线程调度相关的内核事件以确定：（1）阻塞线程的调用堆栈 , (2) 阻塞持续时间，以及 (3) **唤醒线程**的调用堆栈（==使得阻塞线程的调度状态变为可运行==）。阻塞线程的调用堆栈在 *off-core* 和阻塞时不会改变。因此，只有在满足某些选择标准时才需要记录它的调用栈，例如属于我们感兴趣的进程，并且其阻塞持续时间是否在指定范围内。==由于线程交互，这可以实现对调用堆栈阻塞和唤醒行为的高效和选择性跟踪==。

## 3   可视化火焰图

> Brendan Gregg [[13\] ](#_bookmark22)has shown that *scalable vector graphic* (SVG) visualizations of flamegraphs, see Figure [1, ](#_bookmark2)can aid **identification** of performance critical code from sampled stack traces. Stack traces can be related to processing core time, or to the change in a quantity, such as last level cache misses, from a hardware counter. In this way it is possible to produce flamegraphs that characterize the **==microarchitectural behavior==** of an application’s execution. SVGs enables flamegraphs to be viewed, zoomed in/out, and searched for text inside web browsers. The percentage contribution of a method is easily determined by searching for its name, whereupon each instance of the method in the graph is highlighted and <u>its overall contribution to the total number of samples is displayed at the bottom right hand corner</u>. This can be especially important for determining the relative contribution of methods having many different call-sites in an application.
>
> In a flamegraph, any stack traces having identical callers are merged, then any non-identical child callee nodes in the collected traces appear as a new control flow path. Control flow paths are visualized by presenting the unique names of methods inside rectangular blocks that are typically organized lexicographically in order from left to right. Divergence in a control flow path is indicated by more than one rectangular block being stacked on top of another, such as for the blocks stacked on top of `GangWorker::loop` in our Figure [1.](#_bookmark2) The topmost frame in a flamegraph is the method that was executing when a call-stack was sampled. The topmost methods will be identified correctly as long as any sampling skid^1^ [[7](#_bookmark16)] does not cause an incorrect method to be attributed. Note how the use of different colours can be used to distinguish between inlined Java, kernel, JIT compiled Java, C++, and native/library code. Interpreted Java methods are only labelled as `Interpreter`, and cannot be identified with `perf`/`perf-map-agent`. However, call-stacks and flamegraphs produced with `async-profiler` can identify methods that undergo interpreted execution. It is a fairly trivial matter, using options for `async-profiler` and `perf` to generate flamegraphs where thread PIDs are also captured with callstacks, this can sometimes be beneficial, but it can quickly become confusing if an application contains many threads.
>
> >  ^1^Skid occurs when the reported program counter (PC) value is a number of instructions away from the actual PC when the thread was interrupted for stack sampling.
>
> > - [ ] 图 1，2，3
>
> *Identifying Inlined Methods.* In Figure [1, ](#_bookmark2)an inlined method appears as a teal/blue frame above a green Java JIT compiled method, further inlined methods appear as stacks of teal/blue frames. Inlining information accuracy is improved through the use of -XX:+DebugNonSafePoints. <u>Note that flamegraph columns where a teal/blue rectangle/method is directly below green, indicate a place where the JIT compiled and inlined code has called a method that was chosen not to be inlined</u>. Note that *on-core* flamegraphs, only demonstrate stacks for threads that were executing when sampling of call stacks occurred. They neglect the importance of *off-core* time when threads are blocked.
>
> *Heap/Allocation Flamegraphs.* Figure [2](#_bookmark6) is an heap allocation flamegraph created using `async-profiler`, here the width of a call stack is proportional to the amount of heap memory allocation. The topmost call-stack element represents the allocated data type, i.e. an array, such as `char []`, or an object such as `StandardFilter`. The profiler features TLAB-driven sampling that relies on callbacks (see `AllocTracer` in the OpenJDK/HotSpot sources) to receive notifications; 1), when an object is allocated inside a newly created TLAB (annotated as INSIDE or aqua in Figure [2](#_bookmark6)), and 2), when an object is allocated on a slow path outside a TLAB (annotated as OUTSIDE or in orange in Figure [2](#_bookmark6)), for example when its size exceeds that of the TLAB. This means not every allocation is counted, but only allocations every `N kB`, where `N` is the average size of TLAB. This makes heap sampling very cheap and suitable for production, as in practice it will often reflect the top allocation sources, but the collected data may be incomplete.
>
> *Offwaketime Flamegraphs.* The blocking latency associated with *off-cpu* time can be analysed with offwaketime flamegraphs. Figure [3 ](#_bookmark8)presents cropped illustrative output of the BCC/eBPF tool offwaketime for the avrora benchmark for blocking of thread node-0. The bottom stacks, colored blue, are off-core stacks that indicate the sequence of method calls leading to a thread blocking, and these are read from the bottom up leading towards the point where they blocked, that is separated from the waking thread stack by a grey block that is labelled `--`. The waking thread stacks, colored **==aqua==**, have their call-stack frames listed in reverse order, and are read from the top down, from application code, down to the method that caused the blocked thread’s scheduling state to change. The tool offwaketime achieves similar information to [[14](#_bookmark24)] for determining the blocking and wakeup behavior without requiring a modified JVM. A current limitation is that one wakeup stack may not be sufficient to explain all the sources of blocking latency. For example, when the "waker-thread" was itself blocked on another thread, potentially leading to a chain of blocking and wakeup stacks.

Brendan Gregg [[13](#_bookmark22)] 已经表明，可视化的 SVG（可扩展矢量图形）火焰图，参见图 [1](#_bookmark2)，有助于从采样的堆栈跟踪中**识别出**性能关键代码。堆栈跟踪可能与 CPU 处理时间有关，也可能与硬件计数器的数量变化有关，例如未命中最后一级缓存。通过这种方式，可以生成代表应用程序执行的**==微观行为==**的火焰图。 SVG 支持在 Web 浏览器中查看、放大/缩小和搜索。通过搜索方法的名称可以很容易确定方法的百分比贡献，因此图中方法的每个实例都会高亮显示，<u>其对样本总数的总体贡献显示在右下角</u>。这对于确定==<u>应用程序中具有许多不同调用点的方法的相对贡献</u>==尤其重要。

火焰图中相同的堆栈会被合并，不同的堆栈显示为新的控制流路径。通过矩形框来可视化控制流路径，<u>矩形框按方法名来唯一标志，并按字典序从左到右组织</u>。控制流路径中的分支<u>由多个矩形框堆叠在一个矩形框的顶部</u>来表示，例如图 [1](#_bookmark2) 中，堆叠在 `GangWorker::loop` 顶部的矩形框。火焰图中最顶层的矩形框是采样时，调用堆栈路径上正在执行的方法。只要任何采样偏移^1^ [[7](#_bookmark16)] 不会导致归因于不正确的方法，就可以正确识别最顶层的方法。注意，使用不同颜色来区分<u>内联的 Java 方法</u>、<u>内核函数</u>、<u>JIT 编译的 Java 方法</u>、C++ 原生函数。被解释执行的 Java 方法只标记为 `Interpreter`，无法被 `perf`/`perf-map-agent` 识别。不过使用 `async-profiler` 生成的调用堆栈和火焰图可以识别经过解释执行的方法。配置 `async-profiler` 和 `perf` 的选项可以在采样时收集线程的 PID，这个很简单，有事可能有用，但如果应用程序包含许多线程，很快就会变得混乱。

> ^1^当中断线程以采样堆栈时，如果报告的PC值（程序计数器）与实际 PC 相距许多指令时，就会发生**偏移**。

**识别内联方法**。图 [1](#_bookmark2) 中，**绿色**是 Java JIT 编译的方法，其上方**青色/蓝色**的矩形框是内联方法，连续内联的方法显示为<u>青色/蓝色框的堆栈</u>。可使用  `-XX:+DebugNonSafePoints` 提高内联信息的准确性。请注意，火焰图中绿色方法正下方的青色/蓝色方法，表示 JIT 编译并内联的代码调用了一个未内联的方法。请注意，*on-core* 火焰图只展示了==采样调用堆栈时==正在执行线程的堆栈，当线程被阻塞时，*off-core* 时间的重要性被忽略了。

**堆/分配火焰图**。图[2](#_bookmark6)是使用`async-profiler`创建的堆分配火焰图，这里调用堆栈的宽度与堆内存分配量成正比。最顶层的调用堆栈元素表示分配的数据类型，即一个数组，如 `char []`，或一个对象，如 `StandardFilter`。分析器依赖 TLAB  回调来接收通知（参见 OpenJDK/HotSpot 源中的 `AllocTracer`）：1）在新创建的 TLAB 内分配对象时（在图 [2](#_bookmark6) 中注释为 INSIDE 或 ==aqua==），以及 2）当在 TLAB 外的[慢速路径上分配对象时](https://dzone.com/articles/thread-local-allocation-buffers)（注释为 OUTSIDE或图 [2](#_bookmark6) 中的橙色），例如当其大小超过 TLAB 时。<u>这意味着不是每一次分配都被计算在内，而是每 `N kB`分配一次，其中 `N` 是 TLAB 的平均大小</u>。这使得堆采样非常便宜并且适合生产，因为在实践中它通常会反映顶部分配源，但收集的数据可能不完整。 

> 1. [JVM Anatomy Quark #4: TLAB allocation](https://shipilev.net/jvm/anatomy-quarks/4-tlab-allocation/)

**Offwaketime 火焰图**。与 *off-cpu* 时间相关的阻塞延迟可以用 `Offwaketime` 火焰图进行分析。图 [3](#_bookmark8) 展示了 `BCC/eBPF` 工具 `offwaketime` 对于阻塞线程 `node-0` 的 `avrora` 基准测试的输出。底部堆栈（蓝色）是表示导致线程阻塞的方法调用顺序的 `off-core` 堆栈，这些堆栈是从下往上读取的，指向它们阻塞的点，该点之上是标记为 `--` 的灰色块，用于分割唤醒线程堆栈。唤醒线程堆栈，颜色为**浅绿色**，它们的调用堆栈帧以相反的顺序列出，从**应用程序代码**到**改变阻塞线程调度状态**的方法，从上到下读取。工具 offwaketime 获得与 [[14](#_bookmark24)] 类似的信息，用于确定阻塞和唤醒行为，但无需修改 JVM。当前的一个限制是，单个唤醒堆栈可能不足以解释阻塞延迟的所有来源。例如，当**唤醒线程**本身被另一个线程阻塞时，可能会导致一系列阻塞和唤醒堆栈。

## 4    JAVA BASED PROFILING

Sampling profilers repeatedly collect snapshots of the call-stack of functions/methods, that describe the *on-core* execution context of a thread running on a processing core, at the sampled moments in time. The thread execution time of specific methods is statistically related to the number of times the method appears in all stack traces - as long as the samples are collected fairly, and that all points in a program run are selected randomly to remain statistically independent. If many samples are collected over time, and the code stack-traces are highly correlated, then we can safely assume that threads in a program are spending most of their time executing code described by the most heavily correlated call-stack. Unfortunately, profilers that directly rely on the *Java Virtual Machine Tool Interface* (JVMTI) suffer from the problem of safe-point^2^ bias [[18](#_bookmark28)\]. The sampled data is skewed towards the times when threads have voluntarily yielded the processing core at safe-points where the JVTMI’s `GetStackTrace` can directly obtain call-stacks. One might incorrectly come to the conclusion that the actual hot code should lie somewhere between the sampled call stack of the "hot" safe-point and the potential call-stack of the immediately previous safe-point. Yet, this is not the case because there are significant, variable and lengthy delays between when an JVMTI agent signals an JVM to sample a stack trace and the operation occurring. The JVMTI request is first added to the VM thread’s work queue, and the VM thread must process any prior pending tasks first. Figure [1 ](#_bookmark2)demonstrates significant `VM_Thread` activity (20% of call stacks) related to GC.

> ^2^The JVM inserts safe-points that define points where the state of an executing thread is well understood. Effectively this means that all object references stored on the stack are at known locations, and actual storage locations can change during an GC.

Mytkowicz et. al. demonstrated, in [[18\] ](#_bookmark28)that four different commercial and open-source profilers often produced demonstrably different results for the percentage overhead of, and the actual hottest method. The overhead of profiling varied between 1.1x and 1.5x for the different profilers on sequential benchmarks chosen from `DaCapo`, and different profilers caused significant perturbation to the placement of safe-points in compiled methods. Safe-point placement is an implementation decision of the JVM, therefore different JVM’s are likely to report hot-methods with different results and accuracy. Current popular and widely used profilers such as VisualVM, JProfile and YourKit all provide a sampling processingcore profiler that suffers from the problem of safe-point bias. Further problems concerning sampling profilers, and also for bytecode based instrumentation using JVMTI, is that any injected bytecode:

- Introduces overhead to actually perform the stack trace collection.

- Affects the accuracy of analysis and the applied JIT optimizations; Inlining decisions change if a method becomes too large to inline. GC and memory behavior in the JVM may be significantly altered if escape analysis [[8\] ](#_bookmark17)cannot decide if it is safe to allocate specific objects on the stack, and instead they are allocated on the heap.
- Changes in code size are likely to affect compiled code layout, and cache performance due to the need to respect machine alignment for specific data types.
- Will change safe-point placement because of injected bytecode.

A prototype safe-point bias free proof-of-concept profiler was first demonstrated in [[18\]. ](#_bookmark28)UNIX signals were used to pause Java application threads prior to sampling the call-stack of the currently executing method. A JVMTI agent was used to build a map of x86 code ranges to Java methods that enables code addresses in call-stacks to be mapped back to Java methods. Note, the profiler executes completely outside of the JVM, and cannot know the identity of an interpreted method.

The honest-profiler [[3](#_bookmark34)] and async-profiler [[2](#_bookmark33), [19](#_bookmark29)] are recent examples that have extended and improved the proof of concept ideas of [[18](#_bookmark28)\]. We do not discuss the honest-profiler due to space limitations. The async-profiler employs a hybrid approach for its stack trace collection based on i), `AsyncGetCallTrace` to obtain information on a thread’s Java frames, without the requirement to be at a safe-point, and ii), `perf` to provide information on native code and kernel frames. The `perf_event` API is used to configure stack samples to be placed into a memory buffer, and a **signal** is delivered when a sample has been taken. Its signal handler then calls `AsyncGetCallTrace` to capture the Java stack that it then merges with the perf stack trace of native and kernel information. The main benefits of this approach over a purely perf/eBPF approach is that it can be used to profile applications running on older JVM releases that do not support `-XX:+PreserveFramepointer`^3^ that is necessary for OS tool based stack walking of Java thread stacks. The main pros and cons of async-profiler are:

> ^3^Execution overhead of this flag was less than 4% in our experiments.

- `AsyncGetCallTrace` is an internal Oracle/OpenJDK API that is not guaranteed to be available in other JVM implementations.

- Optimised compiler inserted stub code that enables efficient implementation of compiler intrinsics, and performance critical JVM operations such as `crc32` and `System.arraycopy` will generally be invisible to `AsyncGetCallTrace` because the JVM does not record the necessary metadata for many such stubs.

- Off-core thread stacks that are blocked or sleeping are not collected by `AsyncGetCallTrace`.

- `AsyncGetCallTrace` will produce some failed stack traces where it is unable to safely walk a stack without causing an invalid memory access. There were very few failed stack traces in our experiments. `perf` also suffers from this unavoidable problem.

- Does not require generating a map file to map Java code addresses to method names, if one uses `perf_event` directly then one traditionally uses `perf-map-agent` for this purpose.

- Identifies the methods in interpreter frames, `perf-map-agent` merely identifies that an interpreter frame is present.

- Allocation profiling can be performed to identify the sites where the largest amount of heap memory is allocated. In our experiments the overhead of allocation profiling is always below 2.8%. Allocation profiling does not impact on escape analysis or JIT optimizations. 

The `async-profiler` tool can collect stack traces from Java methods, native calls, JVM code and kernel functions (only possible in processing core profiling mode). Both `async-profiler` and `perf` approaches support reading performance counters supported by `perf_events` such as: last-level cache/data TLB load misses, L1-dcache misses, branch misses, page faults, and instructions retired. The `perf` approach collects callstacks and has access to `perf_events`, and uses the `perf-map-agent` JVMTI agent to generate a mapping of code addresses to Java symbol names. The main benefits of the `perf`-based approach over `async-profiler` are that: (1) the JVM flag `-XX:+DebugNonSafepoints` enables inlining decisions to be accurately visualized in a flamegraph, and that (2) JVM generated stubs, and performance critical operations (e.g. JVM intrinsics), can be observed that are invisible to `async-profiler`.

No commercial features are required by async-profiler, as it is completely based on open-source technologies and works with OpenJDK from version `7u40` onwards, where the TLAB callbacks appeared. Note, the heap allocation functionality of async-profiler requires `libjvm.so` JVM debug symbols

 *Stack Fragment Sampling (SFS).* SFS, as described in [[15](#_bookmark25)], has less overhead than `AsyncGetCallTrace` and JVMTI based methods, this is achieved using a modified HotSpot JVM. SFS typically occurs only when a thread is active and on-core. A thread is paused to copy a fragment of its full stack to a buffer. Copied fragments are asynchronously retrieved from the buffer and decoded to full stack traces. Redundant samples are not captured giving reduced storage overheads.

Their work is of low overhead, but they encounter significant problems when enabling the collection of information on waiting periods, i.e. when a thread is blocked for I/O or to acquire a lock. The number of lost samples increases from a maximum of 2% for `pmd`, to more than 50% for the benchmarks `actors`, `avrora`, and `scalatest` in their experiments. Further, SFS operational overheads are likely to be lower on their quad-core experimental setup in comparison to our 8 core node. The performance slowdown of their techniques was approximately `1.15x` (i.e. 15% slower) for avrora when waiting time was not sampled for a 10kHz sampling rate. They found i) that their heuristic rules to enable stack walking work for more than 90% of sampled stacks, even when execution is in native or VM code, and ii), that a fragment size of 32KB can obtain complete stack traces.
