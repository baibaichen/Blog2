# Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to Ask

> The original implementations of VectorWise and HyPer use different approaches. VectorWise uses exchange operators [3]. This classic approach [13] keeps its query processing operators like aggregation and join largely unaware of parallelism. HyPer, on the other hand, uses morsel-driven parallelism, in which joins and aggregations use shared hash-tables and are explicitly aware of parallelism. This allows HyPer to achieve better locality, load-balancing, and thus scalability, than VectorWise [22]. Using the 20 hyperthreads on our 10-core CPU, we measured an average speedup on the five TPC-H queries of 11.7 in HyPer, but only 7.2 in VectorWise. The parallelization framework is, however, orthogonal to the query processing model and we implemented morsel-driven parallelization in both Tectorwise and Typer, as it has been shown to scale better than exchange operators [22].
>
> Morsel-driven parallelism was developed for HyPer [22] and can therefore be implemented quite straightforwardly in Typer: The table scan loop is replaced with a parallel loop and shared data structures like hash tables are appropriately synchronized similar to HyPer’s implementation [22, 23].
>
> For Tectorwise, it is less obvious how to use morsel-driven parallelism. The runtime system of Tectorwise creates an operator tree and exclusive resources for every worker. To achieve that the workers can work together on one query, every operator can have shared state. For each operator, a single instance of shared state is created. All workers have access to it and use it to communicate. For example, the shared state for a hash join contains the hash-table for the build side and all workers insert tuples into it. In general, the shared state of each operator is used to share results and coordinate work distribution. Additionally, pipeline breaking operators use a barrier to enforce a global order of sub-tasks. The hash join operator uses this barrier to enforce that first all workers consume the build side and insert results into a shared hash table. Only after that, the probe phase of the join can start. With shared state and a barrier, the Tectorwise implementation exhibits the same workload balancing parallelization behavior as Typer.

VectorWise 和 HyPer 的原始实现使用不同的方法。 VectorWise 使用 Exchange  运算符 [3]。 这种经典方法 [13] 使其查询处理运算符（如聚合和Join）在很大程度上不了解并行性。另一方面，HyPer 使用 **Morsel** 驱动的并行性，**其中 Join 和聚合使用共享哈希表并明确了解并行性**。与 VectorWise [22] 相比，**这允许 HyPer 实现更好的局部性、负载平衡和可扩展性**。在我们的 10 核 CPU 上使用 20 个超线程，我们测得在 HyPer 中五个 TPC-H 查询的平均加速为 11.7，但在 VectorWise 中仅为 7.2。 然而，**并行化框架与查询处理模型正交**，我们在 Tectorwise 和 Typer 中实现了 **Morsel** 驱动的并行化，因为它已被证明比Exchange  运算符 [22] 具有更好的扩展性。

**Morsel** 驱动的并行化是为 HyPer [22] 开发的，因此可以在 Typer 中非常直接地实现：表扫描循环被并行循环替换，并且共享数据结构（如哈希表）被适当地同步，类似于 HyPer 的实现 [22, 23] .

对于 Tectorwise，如何使用 **Morsel** 驱动的并行不太明显。 Tectorwise 的运行时系统为每个 worker 创建一个**运算符树**和独享的资源。 为了实现 worker 可以在一个查询上一起工作，每个运算符都可以共享状态。 对于每个运算符，都会创建一个共享状态实例。 所有 worker 都可以访问它并使用它进行通信。例如，hash join 的共享状态包含构建端的哈希表，所有 worker 都将元组插入其中。一般来说，每个算子的共享状态用于共享结果和协调工作分配。此外，**管道中断算子**使用**屏障**来保障子任务的全局顺序。Hash join 运算符使用此屏障来强制所有 worker 首先读取构建端的数据，并将结果插入共享哈希表。只有在那之后，Join 的探测阶段才能开始。通过共享状态和屏障，Tectorwise 实现展示了与 Typer 相同的工作负载平衡并行化行为。