**Good morning/afternoon, thank you for having me.**

My name is Chang Chen. I’m a software engineer with over 25 years of experience, specializing in database development, big data systems, and high-performance computing.

I have spent the past 14 years deeply involved in C++ and large-scale distributed systems. I’m an Initial Committer and PMC member of **Apache Gluten (Incubating)**, where I led the project from concept to Apache incubation, achieving **double the performance** of Vanilla Spark in key workloads.

Throughout my career, I’ve held senior technical roles at companies like **Kyligence**, **Yihaodian**, and **Rovi Corporation**, focusing on big data platforms, query optimization, and real-time analytics. I’ve led architecture design, performance optimization, and cross-functional teams—successfully delivering production-ready solutions at scale.

I’m passionate about open-source innovation, performance engineering, and building robust data systems. I’m excited about the opportunity to contribute my expertise in database and big data technologies to your team.

Thank you again for this opportunity. I’m happy to answer any questions.

## 背景

Kyligence 起源于解决 Hive 查询的性能问题，最开始采用 MR 做离线预计算，使用 Calcite 做单机查询引擎，Spark 崛起之后，离线和交互式查询都转向为 Spark，现在我们知道，Spark 不是一个好的交互式查询引擎（code gen 时间较长，shuffle 落盘，单线程调度）。 但当时采用 Spark 是比老方案有很大的进步，对于（scan 数据量对于 100GB）的大查询性能可接受。但是发现小查询（数据量 <= 10 GB），有一些固定开销，性能到了 1 s之后，就很难再提升了，对那种数据量只有几 MB 或者几 KB 的查询影响尤其大。

> 一个痛点是索引数据量越小越不好向用户解释性能为啥不如 Presto。

我 19 年加入 Kyligence 就开始解决小查询性能问题，中间做了很多尝试，比较重要的是引入了基于 JVM 堆的 Local Cache，特定场景下有价值，但没有本质的提升。

所以在 22 年和 Intel 合作搞 Gluten 提升 Spark 的性能，因为 Velox 不成熟，而我们相对比较了解 Clickhouse，双方各自选择了一个后端来支持，但前端都采用 Spark。

> Photon 的出现证明 Native Engine 是可以走通的


## 我在 Gluten 的两个角色

早期团队规模比较大（大于 5 ）的时候主要是  Manager 的角色，后期开始做研发的工作，主要集中在数据湖集成方面。

### Manager 的角色


#### 1. 技术可行性调研

> 评估 ClickHouse 作为后端的潜力

首先要回答，**是否可以用 Clickhouse 替代 Spark 原生的 JVM 执行层，从而突破性能瓶颈？**带领团队从以下几个维度进行了深入的技术可行性分析：

- **性能对比**：我们基于标准的 TPC-H，在 100 的规模上（预计算后的主要规模）对比了 Vanilla Spark、以及通过 JNI 集成 ClickHouse 后的初步性能。CH 本身不支持 Shuffle，Join 的功能不完善，且主要是对大宽表的聚合比较擅长，<u>所以这阶段主要放在 Shuffle 上，通过提高 Shuffle 的性能</u>，基本满足了 2 倍的性能指标

  > 技术细节：实现一个抽象的 `IColumn::insertRangeSelective` 接口，因为 Shuffle 是按 key 重新分配数据，每行数据都有一个确定的位置。因为不同的列有不同的内存布局，所以不同的列类型需要采用不同的实现，比如 `ColumnAggregateFunction` 需要针对不同的聚合函数的缓冲区调用不同的 merge
  >
  > - [ ] Debug 一下实现，这里有点忘了。

- **功能覆盖**：梳理了 Spark SQL 支持的关键算子（如 JOIN、Window Function、Aggregation），并与 ClickHouse 的 SQL 引擎能力进行映射，先排除不能支持的 TPCH 的查询。

  > 技术细节：主要各种特殊的 Join实现，比如 TPCH 16 的 `isNullAwareAntiJoin`，存在Join；标准的 no equal join 当时也没有实现。
  >
  > 还有一些 ANSI  SQL 兼容性的问题，主要是 Null 值的处理方面。

- **生态兼容性**：主要是对 HDFS、S3以及文件格式的支持摸底。

  > 技术细节，Parquet scan 的性能不好，不支持 page index。

- **可集成性**：CH 并没有提供一个 API 接口，但是它的执行引擎相对对立，可以单独利用，后来采用了其他工程化的方法，达到了可维护性。

**结论**：ClickHouse 在性能、功能和工程实现上具备可行性，但需要

1. 提升读 Parquet 的性能
2. Join 的兼容性
3. **解决 ClickHouse 缺乏标准 API 的集成难题**

#### 2. 工程化建设

> 规划整体架构：构建可扩展

我发现，现实中确实有既使用 **Spark** 进行分布式调度与数据处理，又依赖 **ClickHouse** 实现高性能查询的研发人员。他们是 Gluten Clickhouse 后端最天然的潜在贡献者和使用者。为了保证他们参与共建，重点解决了两个关键问题：

1. 解决 ClickHouse 缺乏标准 API 的集成难题，**实现与 ClickHouse 主干代码的无缝集成**，通过自动化脚本每日同步上游 ClickHouse 代码，确保新特性（如函数、优化器规则）能快速集成。CH 进 CH repo， gluten 进 gluten repo。
2. 降低参与门槛，吸引外部开发者共建
   1. 代码托管于 GitHub，CI/CD 全流程自动化；
   2. 撰写清晰的架构文档、帮助新人快速上手；

   3. 定期举办线上社区会议，。

和内部产品集成，让公司看见效果， 将 Gluten 集成到公司核心产品中，用于加速 OLAP 查询和预计算任务。通过真实业务场景验证其稳定性与性能收益，赢得持续投入。

1. 保证产品质量，推动自动化的双跑测试框架的建设，集成内存泄露的检测工具（没有做到自动化）
2. 制定开发 Scope，确保公司看见效果，逐步完善 TPCH 22 和 TPCDS 的支持。

#### 3 性能对比和分析

> 首先是安排团队成员做了一个自动化的测试框架

因为竞争对手是 Starrocks，所以主要是分析和它的差距，然后基于此做性能比较。Gluten 的主要优势是稳定，性能确实还是有差距，有两个问题：

1. **优化器有差距**，优化器本身是个大主题，想不明白如何基于 Spark 现有的架构实现基于 Cascades 的 Top Down 优化器。但这里确实是 Gluten 可以做优化的空间。

   1. CBO Based 的 Join Reorder：Spark 原生的 join reorder 是单独作为一个优化规则，理论上是做不到全局最优，和 StarRocks 相比确实有差距。
   2. Runtime Filter，这一块只是知道有问题，但是不知道问题在那。
   3. 小的优化规则，*Subquery Elimination Using Window Aggregation*

   

2. **执行层面的差距**，这里是基于交互查询的来讨论的，存算分离的架构在离线 ETL 这是有优势的。CH 本身的基础设施比如内存管理，执行引擎、缓存和算子比如分组，排序，函数，都还不错。除了 Join 算子稍微差点外，主要是 Right Join 和 non equal Join 性能不好。它主要的问题是在和 Spark 的架构有冲突，

   1. 首先 Spark 是基于 Task 做单线程调度的，基于 JVM 这个问题不严重，并发小查询这里是瓶颈。
   2. 最突出的是无法实现 work steal，Spark 是在 Driver 调度 task，只能在 Driver 层面做推测执行调度。MPP 是根据节点分配工作负载，可以在节点内部实现 Work steal，这一块的性能大约有 20% 左右的提升。


#### 4 运维

1. 解决和 Clickhouse merge 的冲突
2. 使用 HeapTrack 查内存泄漏
3. 基于 JVM 的 singal  handler 的机制，提取异常的 stack。

### 研发角色

前期研发的工做不多，主要支持了 Parquet 的 Page Index，在 Parquet 读取阶段引入 Page Index 跳过机制，结合谓词下推，显著减少无效数据扫描，提升 I/O 效率。后期，主要是数据湖的支持。

1. 重构了 Delta 写的 pipeline，原始实现是分为两块的，也没有做列统计，因此，对于 Delta 的查询其实不友好，特别是如果要支持 Liquid Cluster，必须要有列统计信息。也是为实时做准备。
2. 支持 Iceberg 的读

## Gluten 的未来

1. 实时写 Delta，社区有支持 Flink 的计划，但是我和我的团队除了没有实现 state store ，基本上走通了 Spark Structure Stream。这里的投入小，收入大
2. GPU - Spark RAPIDS，这里我的经验不多，但是我推测在硬件架构上可能存在瓶颈，因为数据需要在 CPU 内存和 GPU 内存之间传输，有些场景会有收益，有些没有，又或者收益的上限不高。

###  数据湖

各个云厂商都在做大量支持，我曾经做了一些研究，

1. Delta 查询的支持，主要是支持读取 Liquid Cluster，经我们验证，相比与分区表，可以显著避免小文件带来的性能恶化的场景，这里有一些功能需要开发，需要为 Liquid Cluster 开发一个类似 DPP 的规则
2. Delta Deletion vector 的支持，这一块目前读写都已经走通。
3. Spark Structure Stream，已经走通的
   1. 读取 kafka
   2. 所有不需要 state store 的算子。
   3. 自动

### 优化器

我阅读过一篇关于 Azure Synapse Spark 查询优化器的论文，其中提到的 **Exchange 放置优化**，核心思想是**最大化 shuffle 数据的复用**（exchange reuse），减少重复的数据分发。这一思路非常清晰，且具有很强的工程实用性，我认为可以**快速借鉴并应用到 Gluten 的优化器中**。

论文中另一个让我印象深刻的点是**部分聚合下推**（Partial Aggregation Pushdown）。这让我联想到 Kylin 的预计算场景：我们曾遇到两张事实表直接 join 导致性能较差的情况，解决方案是将其中一张事实表预先聚合，生成一个轻量化的索引表，再与另一张表进行 join。这种“大表转小表”的优化策略，本质上与论文中的部分聚合下推思想**高度一致**。

然而，关键区别在于：**Synapse Spark 引入了基于成本的决策模型**，能够判断下推是否真正有益，从而避免无效或负收益的优化。而 Gluten 曾尝试引入类似规则（由 eBay 团队贡献），但由于缺乏精准的成本估算机制，导致新性能回退比较多，稳定性不足。

不过我认为，当前更关键的优化方向是**基于统计信息的 Join Reorder**。在这方面，StarRocks 的实现给我留下了深刻印象，如果 Gluten 能在具备统计信息的前提下，实现类似的 join 重排序能力，将极大提升复杂查询的性能。



---

备选内容

提升 Join 的内存管理，Clickhouse 最初的 Join 输出物化比较简单，数据膨胀的时候容易 OOM，改用延迟输出