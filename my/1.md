**Good morning/afternoon, thank you for having me.**

My name is Chang Chen. I’m a software engineer with over 25 years of experience, specializing in database development, big data systems, and high-performance computing.

I have spent the past 14 years deeply involved in C++ and large-scale distributed systems. I’m an Initial Committer and PMC member of **Apache Gluten (Incubating)**, where I led the project from concept to Apache incubation, achieving **double the performance** of Vanilla Spark in key workloads.

Throughout my career, I’ve held senior technical roles at companies like **Kyligence**, **Yihaodian**, and **Rovi Corporation**, focusing on big data platforms, query optimization, and real-time analytics. I’ve led architecture design, performance optimization, and cross-functional teams—successfully delivering production-ready solutions at scale.

I’m passionate about open-source innovation, performance engineering, and building robust data systems. I’m excited about the opportunity to contribute my expertise in database and big data technologies to your team.

Thank you again for this opportunity. I’m happy to answer any questions.

## 背景

Kyligence 起源于解决 Hive 查询的性能问题，最开始采用 MR 做离线预计算，使用 Calcite 做单机查询引擎，因为不是分布式架构，（scan 数据量大于 100GB的）大查询无解，Spark 崛起之后，离线和交互式查询都转向为 Spark，现在我们知道，Spark 不是一个好的交互式查询引擎（code gen 时间较长，shuffle 落盘，单线程调度）。 但当时采用 Spark 相比老方案有很大的进步，（通过水平扩展）解决了大查询的性能问题。但是发现小查询（数据量 <= 10 GB），有一些固定开销，性能到了 1 s之后，就很难再提升了，对那种数据量只有几 MB 或者几 KB 的查询影响尤其大。

> 一个痛点是索引数据量越小越不好向用户解释性能为啥不如旧版本。

19 年加入 Kyligence 就开始解决小查询性能问题，中间做了很多尝试，比较重要的是引入了基于 JVM 堆的 Local Cache，特定场景下有价值，但没有本质的提升。

> [!IMPORTANT]
>
> 1. local cache + soft affinity
> 2. 缓存击穿之后，性能变慢

所以在 22 年和 Intel 合作搞 Gluten 提升 Spark 的性能，因为 Velox 不成熟，而我们相对比较了解 Clickhouse，双方各自选择了一个后端来支持，但前端都采用 Spark。

> Photon 的出现证明 Native Engine 是可以走通的


## 我在 Gluten 的两个角色

早期团队规模比较大（大于 5 ）的时候主要是  Manager 的角色，后期开始做研发的工作，主要集中在数据湖集成方面。

### Manager 的角色


#### 1. 技术可行性调研

> 评估 ClickHouse 作为后端的潜力

首先要回答，**是否可以用 Clickhouse 替代 Spark 原生的 JVM 执行层，从而突破性能瓶颈？**带领团队从以下几个维度进行了深入的技术可行性分析：

- **性能对比**：我们基于标准的 TPC-H，在 100 的规模上（预计算后的主要规模）对比了 Vanilla Spark、以及通过 JNI 集成 ClickHouse 后的初步性能。CH 本身不支持 Shuffle，Join 的功能不完善，且主要是对大宽表的聚合比较擅长，<u>所以这阶段主要放在 Shuffle 上，通过提高 Shuffle 的性能</u>，基本满足了 2 倍的性能指标

  > [!IMPORTANT]
  >
  > **技术细节**：
  >
  > 1. 实现一个抽象的 `IColumn::insertRangeSelective` 接口，因为 Shuffle 是按 key 重新分配数据，每行数据都有一个确定的位置。不同的列有不同的内存布局，不同的列类型需要采用不同的实现，比如 `ColumnAggregateFunction` 需要针对不同的聚合函数的缓冲区调用不同的 merge。
  >
  > 2. Map 和 reduce 比较多时，还是要参考 Spark 的方案，要Sort shuffle。使用 hash shuffle 有 OOM。
  >
  > | 方面          | Hash Shuffle (无排序)             | Sort Shuffle (有排序)   |
  > | :------------ | :-------------------------------- | :---------------------- |
  > | 文件数量      | **M * R** (巨大，导致瓶颈)        | **2 * M** (可控)        |
  > | 写入模式      | 随机写 (效率低)                   | 顺序写 (效率高)         |
  > | 读取模式      | 随机读 (效率低)                   | 顺序读 (效率高)         |
  > | 压缩效率      | 低                                | **高** (数据有序)       |
  > | CPU 开销      | 低                                | **较高** (需要排序)     |
  > | 内存开销      | 高 (同时维护多个文件句柄和缓冲区) | 相对较低 (可溢出到磁盘) |
  > | Reduce 端合并 | 复杂，可能需要全排序              | 简单，高效的 K 路归并   |

- **功能覆盖**：梳理了 Spark SQL 支持的关键算子（如 JOIN、Window Function、Aggregation），并与 ClickHouse 的 SQL 引擎能力进行映射，先排除不能支持的 TPCH 的查询。

  > [!IMPORTANT]
  >
  > **技术细节**：
  >
  > 1. 主要各种特殊的 Join实现，比如 TPCH 16 的 `isNullAwareAntiJoin`，TPCDS 的 Exist-Join；标准的 no equal join 当时也没有实现。
  > 2. 和 spark 的兼容性的问题，细节忘记了。
  >    1. 比较深的印象是 decimal AVG 函数，中间结果是 float 导致结果不准
  >    2. 日期

- **生态兼容性**：主要是对 HDFS、S3以及文件格式的支持摸底。

  > [!IMPORTANT]
  >
  > **技术细节**
  >
  > Parquet scan 的性能不好，采用 Arrow，不支持 page index。

- **可集成性**：CH 并没有提供一个 API 接口，但是它的执行引擎相对对立，可以单独利用，后来采用了其他工程化的方法，达到了可维护性。

**结论**：ClickHouse 在性能、功能和工程实现上具备可行性，但需要

1. **解决 ClickHouse 缺乏标准 API 的集成难题**
2. 提升读 Parquet 的性能
3. Join 的兼容性

#### 2. 工程化建设

> 规划整体架构：构建可扩展

我发现，现实中确实有既使用 **Spark** 进行分布式调度与数据处理，又依赖 **ClickHouse** 实现高性能查询的研发人员。他们是 Gluten Clickhouse 后端最天然的潜在贡献者和使用者。为了保证他们参与共建，重点解决了两个关键问题：

1. 解决 ClickHouse 缺乏标准 API 的集成难题，**实现与 ClickHouse 主干代码的无缝集成**，通过自动化脚本每日同步上游 ClickHouse 代码，确保新特性（如函数、优化器规则）能快速集成。CH 进 CH repo， gluten 进 gluten repo。
2. 降低参与门槛，吸引外部开发者共建
   1. 代码托管于 GitHub，CI/CD 全流程自动化；
   2. 撰写清晰的架构文档、帮助新人快速上手；

   3. 定期举办线上社区会议，。

和内部产品集成，让公司看见效果， 将 Gluten 集成到公司核心产品中，用于加速 OLAP 查询和预计算任务。通过真实业务场景验证其稳定性与性能收益，赢得持续投入。

1. 保证产品质量，推动自动化的双跑测试框架的建设，集成内存泄露的检测工具（没有做到自动化）
2. 制定开发 Scope，确保公司看见效果，逐步完善 TPCH 22 和 TPCDS 的支持。

#### 3 竞品分析

> 首先是安排团队成员做了一个自动化的压测框架，然后基于此做性能比较

因为竞争对手是 Starrocks，所以主要是分析和它的差距。Gluten 的主要优势是稳定，性能确实有差距，主要在两个方面：

1. **优化器有差距**
   1. Join Reorder & Runtime Filter： Apache Spark 社区在这两块投入不多。一方面 Spark 优化器采用 System R 风格， Join Reorder 与物理实现分离，确实可能会错过全局最优解，另一方面，社区希望用 Runtime Filter 和 AQE 去减缓这个缺陷。另外我们发现 AWS 的 Runtime filter 也有一些不错的地方，细节忘记了。
   2. 优化规则
      1. *Subquery Elimination Using Window Aggregation*，TPCH 17 收益明显。
      2. 部分聚合下推

2. **执行层面的差距**，这里是基于交互查询的来讨论的，存算分离的架构对离线 ETL 有优势。CH 本身的基础设施比如内存管理，执行引擎、缓存和算子比如分组，排序，函数，都还不错。除了 Join 算子稍微差点外，主要是 Right Join 和 non equal Join 性能不好。它主要的问题是在和 Spark 的架构有冲突，
   1. 首先，Spark 是基于 Task 做单线程调度的，基于 JVM 的执行引擎这个问题不严重，Native Engine 并发小查询会导致这里是瓶颈。
   2. 其次，最突出的是无法实现 work steal，Spark 是在 Driver 调度 task，只能在 Driver 层面做推测执行调度。MPP 是根据节点分配工作负载，可以在节点内部实现 Work steal，这一块的性能大约有 20% 左右的提升。这也是我们发现，像 TPCH Q1 这种只有分组的查询总是比 StarRocks 慢 20%。
   3. 最后，因为 Spark 不重视交互式，它的 sql parser 和 analyzer 在复杂 sql 上表现不佳。


#### 4 日常运维

1. 解决和 Clickhouse merge 的冲突

### 研发角色

1. 使用 HeapTrack 查内存泄漏

   > [!IMPORTANT]
   >
   > 因为是拦截了 `malloc`，不能使用 jemalloc（没有 export），一致没有加入 CI，人工周期性的检测。
   >
   > 其他的没有跑成功，比如 jemalloc 自带的 profiler。

2. 基于 `libjsig.so`  提供的**信号链（Signal Chaining）机制**获取 JNI 异常信号

   > [!IMPORTANT]
   >
   > **异步安全的信号处理**  信号处理程序在极其受限的环境中运行，大多数 C/C++ 函数不能安全调用。该使用管道来安全地将信号信息从信号处理上下文转移到正常线程，在那里可以安全地执行完整的处理。
   >
   > **堆栈跟踪捕获** 当接收到信号时，采用 clickhouse 的功能捕获堆栈跟踪
   > **线程分离处理信号**  在独立线程中处理每个捕获的信号，避免在处理一个信号时阻塞其他信号的接收。

3. 支持了 Parquet 的 Page Index，在 Parquet 读取阶段引入 Page Index 跳过机制，结合谓词下推，显著减少无效数据扫描，提升 I/O 效率。

   > [!IMPORTANT]
   >
   > 某些情况下比 spark jvm 的 Parquet reader 还慢
   >
   > 1. Parquet 编码大量利用字典格式，Spark 是延迟物化，CH 是直接解码出来
   > 2. 不支持 Page Index
   > 3. 不支持真正的 filter 下推
   >
   > 
   >
   > Parquet 是按行而不是页对齐，因此利用 Clickhouse 提供的功能将表达式树转换为逆波兰表示法，然后求出合法的、基于行的范围，然后过滤掉不需要的 Page。提供给 Parquet reader 的数据是合法的、过滤后的基于 Page 的数据。
   >
   > Skip IO => 收益最大
   >
   > Skip Decompress => 收益适中
   >
   > Skip Decode => 过滤的时候还是要先解码，某些 bit packing 编码的可以做到不用解码就过滤，但效果如何要评估
   >
   > Skip Output => 减少内存 copy

4. 重构了写 Parquet 的 pipeline，和 Delta 的写更加兼容，

   > [!IMPORTANT]
   >
   > 1. Spark 3.3 的实现的机制，导致写 Parquet 必须得分为两块，需要修改源码。Spark 3.5 之后读和写可以合并成一条 Pipeline 不用再修改 Spark 源码。
   > 2. 初版实现没有做列统计，对于 Delta 的查询不友好，特别是如果要支持 Liquid Cluster，必须要有列统计信息。
   > 3. 具体策略，数据处理全部走 Native engine 提高性能，返回 Delta 提交事务需要的数据，比如统计信息；利用 Delta 的功能实现事务和文件优化机制。
   > 4. 实现 Optimized Write，本质就是 adaptive shuffle（由我指导，团队成员实现） 机制。
   >
   > 收益，可以使用 Delta Lake 文件优化（Optimize）机制

5. 支持 Iceberg 的 Parquet 读，equality read 和 Positional Deletes。

   > [!IMPORTANT]
   >
   > 1. 首先支持 Row index，前面做 Page index 过滤的时候其实已经获得了row index
   > 2. Equality Deletes：构建过滤表达式，单列就是 `c1 not in ..`，多列就是 `c1 <> xx or c2 <> xx`
   > 3. Positional Deletes：构建 Deletion Vector，基于 `RoaringBitmap`，读 Delta 也需要用到。
   >
   > Iceberg 这块的设计没有 Delta 好，后续也在改成 DV。

## Gluten 半年规划

1. 实时写 Delta，社区有支持 Flink 的计划，但是我和我的团队除了没有实现 state store ，基本上走通了 Spark Structure Stream。这里的投入小，收入大
2. GPU - Spark RAPIDS，这里我的经验不多，但是我推测在硬件架构上可能存在瓶颈，因为数据需要在 CPU 内存和 GPU 内存之间传输，有些场景会有收益，有些没有，又或者收益的上限不高。

###  选项 1 功能：数据湖

Iceberg 被 DB 收购，所以选择优先支持 Delta。

####  Delta

已经实现了 Deletion Vector 读写；使用 Spark Structure Stream 打通 CDC 实时入湖（支持无状态的算子）；可以利用 Delta 自动文件优化机制优化小文件。下一步：

1. 支持窗口聚合和流式 SQL，构建完整的流批一体能力。
2. Delta Liquid Cluster 支持，经我们验证，相比与分区表，Liquid Cluster 可以显著避免小文件带来的性能恶化的场景，有小量功能的开发，需要为 Liquid Cluster 开发一个类似 DPP 的规则，写则需要支持 Hilbert Clustering。注，如果要支持 Spark 3.5，需要 Cherry Pick 4.0 的一些 commit。
3. 更深入的技术调研
   1. Delta 的 CDC 功能
   2. 如果支持 Delta 查询中广泛使用的匿名 UDF 如何支持？
   3. 流式 SQL，以 SQL 的形式创建流式作业。
   4. 支持读取 MySql Binlog 或者 PostgreSQL WAL。

### 选项 2 性能：优化器

我曾经过一篇关于 Azure Synapse Spark 查询优化器的论文，其中提到的 **Exchange 放置优化**，核心思想是**最大化 shuffle 数据的复用**（exchange reuse），减少重复的数据分发。这没有增加物理算子，可以**快速应用到 Gluten**中，没有开发成本。

论文中另一个让我印象深刻的点是**部分聚合下推**（Partial Aggregation Pushdown）。Gluten 曾尝试引入类似规则（由 eBay 团队贡献），但由于缺乏精准的成本估算机制，导致新性能回退比较多，稳定性不足。**Synapse Spark 引入了基于成本的决策模型**，能够判断下推是否真正有益，从而避免无效或负收益的优化。因为引入了新的逻辑算子，不确定是否有开发工作量。 

> 这让我联想到 Kylin 的预计算场景。我们曾遇到两张事实表直接 join 导致性能较差的情况，解决方案是将其中一张事实表预先聚合，生成一个轻量化的索引表，再与另一张表进行 join。这种“大表转小表”的优化策略，本质上与论文中的部分聚合下推思想**高度一致**。

总之，Native Engine 和优化器是两个几乎正交的领域。一些在 Fabric spark 证明有价值的优化，应该可以快速用上。

不过，长期来看我认为，当前更关键的优化方向是**基于统计信息的 Join Reorder**。在这方面，StarRocks 的实现给我留下了深刻印象，如果 Gluten 能在具备统计信息的前提下，实现类似的 join 重排序能力，将极大提升复杂查询的性能。

###  选项 3 性能：执行引擎

> 短期来说是文件系统的 Local cache，要扩展 Clickhouse 文件系统的 local cache，目前它不支持跨进程的共享。

长期可探索 Work Steal 机制，在分布式 Gluten 集群中实现较复杂。随着云上 CPU 能力提升，用户选择 scale-up（如 128 core 单机）执行离线 ETL 更具性价比——相比 8×16c，计算力相当但管理成本更低。若缺乏相应优化，单节点大负载 ETL 将无法发挥性能优势。
